{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMQkYfjd+7ko00gmeYygLD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainvo/DeepLearning_NER/blob/master/DL_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTBsYI1tLeVk",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5MwAmUALZ4V",
        "colab_type": "text"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q3HiGQgMU5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "7c1a78ed-e8f2-433b-c143-74e0998866cd"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
        "\n",
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 13:35:30--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17252156 (16M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "train.tsv           100%[===================>]  16.45M  26.6MB/s    in 0.6s    \n",
            "\n",
            "2020-05-06 13:35:33 (26.6 MB/s) - ‘train.tsv’ saved [17252156/17252156]\n",
            "\n",
            "--2020-05-06 13:35:35--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2419425 (2.3M) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>]   2.31M  10.1MB/s    in 0.2s    \n",
            "\n",
            "2020-05-06 13:35:37 (10.1 MB/s) - ‘dev.tsv’ saved [2419425/2419425]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOOHEYpiMzFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "19760df4-57c3-4d8b-caf1-5e3a06e4d7ed"
      },
      "source": [
        "from collections import namedtuple\n",
        "OneWord=namedtuple(\"OneWord\",[\"word\",\"entity_label\"])\n",
        "\n",
        "def read_ontonotes(tsv_file):\n",
        "  #\"\"\"Yield complete sentences\"\"\"\n",
        "    current_sentence=[] # list of (word,label) tuples\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\t')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line: #sentence break\n",
        "                if current_sentence: #if we gathered a sentence, we should yield it, because a new starts\n",
        "                    yield current_sentence #much like return, but continues past this line once the element has been consumed\n",
        "                    current_sentence=[] #...and start a new one\n",
        "                continue\n",
        "            #if we made it here, we are on a normal line\n",
        "            columns=[line[0], line[1]] #an actual word line\n",
        "            assert len(columns)==2 #we should have four columns, looking at the data\n",
        "            current_sentence.append(OneWord(*columns)) #shorthand for looping over columns\n",
        "        else: #for ... else -> the else part is executed once, when \"for\" runs out of elements\n",
        "            if current_sentence: #yield also the last one!\n",
        "                yield current_sentence\n",
        "\n",
        "#read the data in as sentences\n",
        "sentences_train=list(read_ontonotes(\"train.tsv\"))\n",
        "sentences_dev=list(read_ontonotes(\"dev.tsv\"))\n",
        "\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_train[:3]:\n",
        "    print(sent)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First three sentences\n",
            "[OneWord(word='Big', entity_label='O'), OneWord(word='Managers', entity_label='O'), OneWord(word='on', entity_label='O'), OneWord(word='Campus', entity_label='O')]\n",
            "\n",
            "[OneWord(word='In', entity_label='O'), OneWord(word='recent', entity_label='B-DATE'), OneWord(word='years', entity_label='I-DATE'), OneWord(word=',', entity_label='O'), OneWord(word='advanced', entity_label='O'), OneWord(word='education', entity_label='O'), OneWord(word='for', entity_label='O'), OneWord(word='professionals', entity_label='O'), OneWord(word='has', entity_label='O'), OneWord(word='become', entity_label='O'), OneWord(word='a', entity_label='O'), OneWord(word='hot', entity_label='O'), OneWord(word='topic', entity_label='O'), OneWord(word='in', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='business', entity_label='O'), OneWord(word='community', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "\n",
            "[OneWord(word='With', entity_label='O'), OneWord(word='this', entity_label='O'), OneWord(word='trend', entity_label='O'), OneWord(word=',', entity_label='O'), OneWord(word='suddenly', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='mature', entity_label='O'), OneWord(word='faces', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='managers', entity_label='O'), OneWord(word='boasting', entity_label='O'), OneWord(word='an', entity_label='O'), OneWord(word='average', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='over', entity_label='O'), OneWord(word='ten', entity_label='B-DATE'), OneWord(word='years', entity_label='I-DATE'), OneWord(word='of', entity_label='O'), OneWord(word='professional', entity_label='O'), OneWord(word='experience', entity_label='O'), OneWord(word='have', entity_label='O'), OneWord(word='flooded', entity_label='O'), OneWord(word='in', entity_label='O'), OneWord(word='among', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='young', entity_label='O'), OneWord(word='people', entity_label='O'), OneWord(word='populating', entity_label='O'), OneWord(word='university', entity_label='O'), OneWord(word='campuses', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZHhXzVTQA_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8101d511-54c6-4c57-f481-5bbc10fac447"
      },
      "source": [
        "# shape into dicts per sentence\n",
        "data_dict_train = []\n",
        "for line in sentences_train:\n",
        "    sent_text= []\n",
        "    sent_tags = []\n",
        "    for OneWord in line:\n",
        "        #print(OneWord)\n",
        "        sent_text.append(OneWord.word)\n",
        "        sent_tags.append(OneWord.entity_label)\n",
        "    sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "    #print(sent_dict)\n",
        "    data_dict_train.append(sent_dict)\n",
        "print(data_dict_train[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Big', 'Managers', 'on', 'Campus'], 'tags': ['O', 'O', 'O', 'O']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpYBQMbcQGBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b30bb991-d303-444a-9aa1-22b2fb556c27"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "data = data_dict_train\n",
        "random.seed(124)\n",
        "random.shuffle(data)\n",
        "\n",
        "texts=[example[\"text\"] for example in data]\n",
        "labels=[example[\"tags\"] for example in data]\n",
        "\n",
        "print('Text: ', texts[0])\n",
        "print('Label: ', labels[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  ['Nonetheless', ',', 'when', 'taken', 'as', 'a', 'whole', 'it', 'presents', 'the', 'ideas', 'that', 'there', 'are', 'spirits', 'in', 'the', 'earth', 'and', 'sky', ',', 'and', 'that', 'the', 'universe', 'is', 'an', 'organic', 'and', 'connected', 'whole', '.']\n",
            "Label:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_gY3Z_1QLLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "f2e5e7f0-5d93-4f8c-866f-30fd97b4ea58"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizer vocab size: 39078\n",
            "Train shape (1029876, 39078)\n",
            "Dev shape (250420, 39078)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4OZN6QAQQCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "3f6aae01-d0cf-4746-a711-9c1d2f505f38"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.05, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WlnO4jEQUgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "734c58af-b9d9-4305-a213-e6f55e2808ae"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8895415701621276"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueZwJCfpQ3Vg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae9a0ef4-15db-4391-ea6c-6136b0252fd2"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8925045922849613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwMwwo9-MPv4",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCo0xF5kMMbH",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgSYNcerMI9R",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRDxKgLSL_uf",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlG6ZWkIL-HY",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}