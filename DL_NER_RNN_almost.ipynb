{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTBsYI1tLeVk"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2GevEzfPP2",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/api/annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O5MwAmUALZ4V"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q3HiGQgMU5L",
        "outputId": "37b0b6b3-795b-43f1-f712-c3a62cf0a8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# Training data: Used for training the model\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
        "\n",
        "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
        "\n",
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
        "\n",
        "#saved model\n",
        "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-19 22:21:11--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17252156 (16M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "\rtrain.tsv             0%[                    ]       0  --.-KB/s               \rtrain.tsv            27%[====>               ]   4.56M  22.8MB/s               \rtrain.tsv           100%[===================>]  16.45M  45.6MB/s    in 0.4s    \n",
            "\n",
            "2020-05-19 22:21:12 (45.6 MB/s) - ‘train.tsv’ saved [17252156/17252156]\n",
            "\n",
            "--2020-05-19 22:21:14--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2419425 (2.3M) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>]   2.31M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-19 22:21:14 (17.5 MB/s) - ‘dev.tsv’ saved [2419425/2419425]\n",
            "\n",
            "--2020-05-19 22:21:17--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1788466 (1.7M) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>]   1.71M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-05-19 22:21:17 (27.7 MB/s) - ‘test.tsv’ saved [1788466/1788466]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeoOlUGTwijU",
        "colab_type": "code",
        "outputId": "105fb692-f9de-4382-d793-8dbf28a969bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOOHEYpiMzFp",
        "colab": {}
      },
      "source": [
        "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
        "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
        "\n",
        "def read_ontonotes(tsv_file): # \n",
        "    current_sent = [] # list of (word,label) lists\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line:\n",
        "                if current_sent:\n",
        "                    yield current_sent\n",
        "                    current_sent=[]\n",
        "                continue\n",
        "            current_sent.append(line[0]) \n",
        "        else:\n",
        "            if current_sent:\n",
        "                yield current_sent\n",
        "\n",
        "train_data_full = list(read_ontonotes('train.tsv'))\n",
        "dev_data_full = list(read_ontonotes('dev.tsv'))\n",
        "test_data_full = list(read_ontonotes('test.tsv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qm0zpFpw2Rt",
        "colab_type": "code",
        "outputId": "b8e76142-961d-42ba-a118-ed496f8993b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import re\n",
        "from pprint import pprint\n",
        "#regex for empty space chars, \\t \\n\n",
        "#tab = re.compile('[\\t]')\n",
        "#line = re.compile('[\\n]')\n",
        "punct = re.compile('[.?!:;]')\n",
        "\n",
        "def splitter(sent):\n",
        "    #print('----------------------------------------')\n",
        "    #print(\"one sentence in raw data:\", sent)\n",
        "    split_list = []\n",
        "    # loop over tokens items inside sentence, supposedly item= token+ \\t +tag\n",
        "    for item in sent: \n",
        "        #print(\"Item in sentence: \", item)\n",
        "        if item != None:\n",
        "            match1 = item.count('\\n')\n",
        "            #print(match1)\n",
        "            match2 = item.count('\\t')\n",
        "            #print(match2)\n",
        "            if match1 ==0: # no new lines nested\n",
        "                if match2 == 1: #just one tab inside token\n",
        "                    item_pair = item.split('\\t')\n",
        "                if item_pair[0] =='': # replacing empty string with missing quote marks\n",
        "                    item_pair[0] = '\\\"'\n",
        "                split_list.append(item_pair) \n",
        "            else:\n",
        "                subitems_list = item.split('\\n') ## check if token has \\n -> bundled, quotes\n",
        "                if len(subitems_list) > 1:  ## item string has more than one sentence nested in it\n",
        "                    #print(\"Found nested sentences: \", subitems_list)\n",
        "                    #print(\"subseq start\")\n",
        "                    for j in range(len(subitems_list)):  \n",
        "                        token = subitems_list[j]  \n",
        "                        #print(token)\n",
        "                        subtoken_listed_again = token.split('\\n') \n",
        "                        for token in subtoken_listed_again:\n",
        "                            match1=token.count('\\n')\n",
        "                            match2=token.count('\\t')\n",
        "                            if  match1 == 0: # no new lines nested\n",
        "                               if  match2 == 1: #just one tab inside token\n",
        "                                    token = token.split('\\t')\n",
        "                            if token =='': # replacing empty string with missing quote marks\n",
        "                                token = '\\\"'\n",
        "                            if token == '.':\n",
        "                                split_list.append(token)\n",
        "                                continue\n",
        "                                split_list=[]\n",
        "                            else:\n",
        "                                split_list.append(token)\n",
        "                    #print(\"subseq end\")\n",
        "    for item in split_list:\n",
        "        #print(\"Item in split list: \",item)\n",
        "        if type(item) != list:\n",
        "            split_list.remove(item)\n",
        "        if item[0] =='': # replacing empty string with missing quote marks\n",
        "            item[0] = '\\\"'\n",
        "    #print(\"Resplitted sentence :\", split_list)\n",
        "    return split_list\n",
        "\n",
        "def clean(raw_data): ## input list is list of lists of strings \n",
        "    clean_data =[]  #list of lists that have one clean sentence per list\n",
        "    for sent in raw_data: # split by [] lines, supposedly a sentence line\n",
        "        one_sentence = [] #collects the new sentence if there has been need to resplit items\n",
        "        splitted= splitter(sent)\n",
        "        for item in splitted:\n",
        "            #print(item)\n",
        "            matchi = re.match(punct, item[0])\n",
        "            if matchi:\n",
        "                #print(\"collected sentence\")\n",
        "                one_sentence.append(item)\n",
        "                clean_data.append(one_sentence)\n",
        "                one_sentence=[]\n",
        "                break\n",
        "            else:\n",
        "                one_sentence.append(item)\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "train_data_clean = clean(train_data_full)\n",
        "print(len(train_data_clean))\n",
        "for item in train_data_clean[:3]:\n",
        "    print(item)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50252\n",
            "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O'], ['.', 'O']]\n",
            "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O'], ['.', 'O']]\n",
            "[['In', 'O'], ['order', 'O'], ['to', 'O'], ['attract', 'O'], ['this', 'O'], ['group', 'O'], ['of', 'O'], ['seasoned', 'O'], ['adults', 'O'], ['pulling', 'O'], ['in', 'O'], ['over', 'O'], ['NT$', 'B-MONEY'], ['1', 'I-MONEY'], ['million', 'I-MONEY'], ['a', 'O'], ['year', 'O'], ['back', 'O'], ['to', 'O'], ['the', 'O'], ['ivory', 'O'], ['tower', 'O'], [',', 'O'], ['universities', 'O'], ['have', 'O'], ['begun', 'O'], ['to', 'O'], ['establish', 'O'], ['executive', 'O'], ['MBA', 'B-WORK_OF_ART'], ['(', 'O'], ['EMBA', 'B-WORK_OF_ART'], [')', 'O'], ['programs', 'O'], ['.', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJOw362Aw7Gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1bb7a9dc-d47f-49fe-e5f7-f2b461b2f0ea"
      },
      "source": [
        "# final check on the sentences\n",
        "item_lengths = []\n",
        "max_text = 0\n",
        "max_ind=0\n",
        "for item in train_data_clean:\n",
        "    item_lengths.append(len(item))\n",
        "    if len(item) > max_text:\n",
        "        max_text = len(item)\n",
        "        max_ind = train_data_clean.index(item)\n",
        "print(\"Longest sentence:\", max_text, \"index: \",max_ind)\n",
        "\n",
        "lengths_sorted = sorted(item_lengths, reverse=True)\n",
        "max = item_lengths.index(max_text)\n",
        "\n",
        "#pprint(train_data_clean[max])\n",
        "print(lengths_sorted[:300]) # longest sentences\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest sentence: 168 index:  8041\n",
            "[168, 145, 123, 121, 120, 108, 106, 106, 102, 101, 97, 96, 94, 94, 93, 93, 93, 92, 91, 91, 91, 90, 90, 90, 90, 90, 90, 89, 88, 88, 88, 87, 87, 86, 85, 85, 85, 85, 84, 84, 84, 84, 83, 83, 83, 83, 82, 82, 82, 82, 81, 81, 80, 80, 80, 79, 79, 79, 79, 79, 79, 78, 78, 78, 78, 78, 77, 77, 77, 77, 77, 77, 77, 77, 76, 76, 76, 76, 76, 76, 76, 76, 76, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-1cxTQd1t2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78fae35c-4505-4a9d-ed5a-9ce86dd62a26"
      },
      "source": [
        "# pop the longest\n",
        "train_data_clean.pop(max_ind)\n",
        "print(len(train_data_clean))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1B44CdNw-64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3f0ddd4e-88b3-42f4-d88c-fcbec131afd1"
      },
      "source": [
        "\n",
        "print('------------------------------------------')\n",
        "dev_data_clean = clean(dev_data_full)\n",
        "print(len(dev_data_clean))\n",
        "for item in dev_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')\n",
        "test_data_clean = clean(test_data_full)\n",
        "print(len(test_data_clean))\n",
        "for item in test_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------\n",
            "9954\n",
            "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON'], ['.', 'O']]\n",
            "[['On', 'O'], ['August', 'B-DATE'], ['25', 'I-DATE'], ['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['wrapped', 'O'], ['up', 'O'], ['his', 'O'], ['first', 'B-ORDINAL'], ['overseas', 'O'], ['trip', 'O'], ['since', 'O'], ['taking', 'O'], ['office', 'O'], [',', 'O'], ['swinging', 'O'], ['through', 'O'], ['three', 'B-CARDINAL'], ['countries', 'O'], ['in', 'O'], ['Latin', 'B-LOC'], ['America', 'I-LOC'], ['and', 'O'], ['another', 'O'], ['three', 'B-CARDINAL'], ['in', 'O'], ['Africa', 'B-LOC'], ['.', 'O']]\n",
            "[['While', 'O'], ['in', 'O'], ['the', 'B-GPE'], ['Dominican', 'I-GPE'], ['Republic', 'I-GPE'], ['to', 'O'], ['attend', 'O'], ['the', 'O'], ['inauguration', 'O'], ['of', 'O'], ['President', 'O'], ['Hipolito', 'B-PERSON'], ['Mejia', 'I-PERSON'], [',', 'O'], ['Chen', 'B-PERSON'], ['had', 'O'], ['a', 'O'], ['chance', 'O'], ['to', 'O'], ['meet', 'O'], ['with', 'O'], ['the', 'O'], ['leaders', 'O'], ['of', 'O'], ['many', 'O'], ['different', 'O'], ['nations', 'O'], ['.', 'O']]\n",
            "------------------------------------------\n",
            "7920\n",
            "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O'], ['.', 'O']]\n",
            "[['Worldwide', 'O'], [',', 'O'], ['biotechnology', 'O'], ['is', 'O'], ['a', 'O'], ['rising', 'O'], ['star', 'O'], ['of', 'O'], ['the', 'O'], ['industrial', 'O'], ['stage', 'O'], ['.', 'O']]\n",
            "[['In', 'O'], ['Taiwan', 'B-GPE'], [',', 'O'], ['in', 'O'], ['recent', 'O'], ['years', 'O'], ['the', 'B-ORG'], ['Ministry', 'I-ORG'], ['of', 'I-ORG'], ['Economic', 'I-ORG'], ['Affairs', 'I-ORG'], [',', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Science', 'I-ORG'], ['Council', 'I-ORG'], ['and', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Health', 'I-ORG'], ['Research', 'I-ORG'], ['Institutes', 'I-ORG'], ['have', 'O'], ['been', 'O'], ['strongly', 'O'], ['pursuing', 'O'], ['\"', 'O'], ['biochip', 'O'], ['\"', 'O'], ['research', 'O'], ['programs', 'O'], ['.', 'O']]\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZHhXzVTQA_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ba5694db-43b1-4b7c-dab9-4050c5906070"
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for item in f: # list of lists (tokens)\n",
        "        #print(item)\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for token in item:\n",
        "            if len(token) ==2:\n",
        "                sent_text.append(token[0])\n",
        "                sent_tags.append(token[1])\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict['text'])\n",
        "        #print(sent_dict['tags'])\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "train_data_sent = list(reshape_sent2dicts(train_data_clean))\n",
        "samp = train_data_sent[:2]\n",
        "print(samp)\n",
        "print()\n",
        "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
        "samp2 = dev_data_sent[:3]\n",
        "print(samp2)\n",
        "test_data_sent = list(reshape_sent2dicts(test_data_clean))\n",
        "samp3 = test_data_sent[:3]\n",
        "print(samp3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "\n",
            "[{'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']}, {'text': ['On', 'August', '25', 'President', 'Chen', 'Shui', '-', 'bian', 'wrapped', 'up', 'his', 'first', 'overseas', 'trip', 'since', 'taking', 'office', ',', 'swinging', 'through', 'three', 'countries', 'in', 'Latin', 'America', 'and', 'another', 'three', 'in', 'Africa', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-CARDINAL', 'O', 'B-LOC', 'O']}, {'text': ['While', 'in', 'the', 'Dominican', 'Republic', 'to', 'attend', 'the', 'inauguration', 'of', 'President', 'Hipolito', 'Mejia', ',', 'Chen', 'had', 'a', 'chance', 'to', 'meet', 'with', 'the', 'leaders', 'of', 'many', 'different', 'nations', '.'], 'tags': ['O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "[{'text': ['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['Worldwide', ',', 'biotechnology', 'is', 'a', 'rising', 'star', 'of', 'the', 'industrial', 'stage', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['In', 'Taiwan', ',', 'in', 'recent', 'years', 'the', 'Ministry', 'of', 'Economic', 'Affairs', ',', 'the', 'National', 'Science', 'Council', 'and', 'the', 'National', 'Health', 'Research', 'Institutes', 'have', 'been', 'strongly', 'pursuing', '\"', 'biochip', '\"', 'research', 'programs', '.'], 'tags': ['O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpYBQMbcQGBi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "d148a0b9-1d53-49ec-c14e-43c432f121b3"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "random.seed(125)\n",
        "random.shuffle(train_data_sent)\n",
        "#max_sent = [max(len(i[\"text\"])) for i in train_data_sent]\n",
        "#print(max_sent)\n",
        "print(type(train_data_sent))\n",
        "print(train_data_sent[0]) ##one dict\n",
        "print()\n",
        "print(train_data_sent[0][\"text\"])\n",
        "print()\n",
        "print(train_data_sent[0][\"tags\"])\n",
        "print('------------')\n",
        "\n",
        "def typed_listing(data, key):\n",
        "    listed = []\n",
        "    max_length = 0\n",
        "    for item in data: # dictionary {text:\"\", tags:\"\"}\n",
        "        #print('Item: ', item)\n",
        "        #print('Key: ', key, ' content: ', item[key], 'length: ',len(item[key]))\n",
        "        if len(item[key]) > max_length:\n",
        "            max = len(item[key])\n",
        "        listed.append(item[key])\n",
        "    return listed, max_length\n",
        "\n",
        "listed_texts= typed_listing(train_data_sent, \"text\")\n",
        "train_texts = listed_texts[0]\n",
        "train_txt_max = listed_texts[1]\n",
        "listed_labels = typed_listing(train_data_sent, \"tags\")\n",
        "train_labels= listed_labels[0]\n",
        "train_lbl_max = listed_labels[1]\n",
        "print(train_txt_max)\n",
        "print(train_texts[0])\n",
        "print(train_labels[0])\n",
        "\n",
        "\n",
        "print('-----------------------------')\n",
        "print(len(train_texts))\n",
        "print('-----------------------')\n",
        "print('Text: ', train_texts[0])\n",
        "print(' Texts length: ',len(train_texts))\n",
        "print('Label: ', train_labels[0])\n",
        "print(' Labels length: ',len(train_labels))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "{'text': ['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.'], 'tags': ['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']}\n",
            "\n",
            "['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.']\n",
            "\n",
            "['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']\n",
            "------------\n",
            "0\n",
            "['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.']\n",
            "['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']\n",
            "-----------------------------\n",
            "50251\n",
            "-----------------------\n",
            "Text:  ['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.']\n",
            " Texts length:  50251\n",
            "Label:  ['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']\n",
            " Labels length:  50251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQQRw0YO-Ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "1ec74ab2-cf64-4101-e631-c545c3869fbd"
      },
      "source": [
        "## same for validation/dev data\n",
        "listed_texts_dev = typed_listing(dev_data_sent, \"text\")\n",
        "dev_texts = listed_texts_dev[0]\n",
        "dev_txt_max = listed_texts_dev[1]\n",
        "\n",
        "listed_labels_dev = typed_listing(dev_data_sent, \"tags\")\n",
        "dev_labels= listed_labels_dev[0]\n",
        "dev_lbl_max = listed_labels_dev[1]\n",
        "\n",
        "print('Text: ', dev_texts[0])\n",
        "print(' Texts length: ',len(dev_texts))\n",
        "print('Label: ', dev_labels[0])\n",
        "print(' Labels length: ',len(dev_labels))\n",
        "\n",
        "##and test data\n",
        "print(\"length of test data: \",len(test_data_sent))\n",
        "listed_texts_test = typed_listing(test_data_sent, \"text\")\n",
        "test_texts = listed_texts_test[0]\n",
        "test_txt_max = listed_texts_test[1]\n",
        "print(\"Length of test texts: \",test_txt_max) # for some reason colab gives zero?\n",
        "print(\"first item of text texts: \",test_texts[0])\n",
        "listed_labels_test = typed_listing(test_data_sent, \"tags\")\n",
        "test_labels= listed_labels_test[0]\n",
        "test_lbl_max = listed_labels_test[1]\n",
        "print(\"Length of test labels: \",test_lbl_max) # for some reason colab gives zero?\n",
        "print(test_texts[:2])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.']\n",
            " Texts length:  9954\n",
            "Label:  ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']\n",
            " Labels length:  9954\n",
            "length of test data:  7920\n",
            "Length of test texts:  0\n",
            "first item of text texts:  ['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71', '.']\n",
            "Length of test labels:  0\n",
            "[['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71', '.'], ['Worldwide', ',', 'biotechnology', 'is', 'a', 'rising', 'star', 'of', 'the', 'industrial', 'stage', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICn08fOgbyXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6d3029f7-0b6d-43ef-f6ca-e6ef8e519cfb"
      },
      "source": [
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-19 22:39:20--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  52.1MB/s    in 12s     \n",
            "\n",
            "2020-05-19 22:39:32 (55.5 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFv2qclTbyXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "de807ffc-7d36-4230-a8d2-a69fef0b6c03"
      },
      "source": [
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_C5ii8byXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "318f0420-fc25-4ba5-c494-431f828ba615"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdgjgOlbyXx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8786f095-0e4c-4a23-dac1-62ff0f2b7ead"
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGaojUBhbyX2",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Ox5_ObyX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "6ec8702d-8485-474d-a495-d72b71972b11"
      },
      "source": [
        "#Labels\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in train_labels: #loops over sentences \n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 25,\n",
            " 'B-DATE': 14,\n",
            " 'B-EVENT': 7,\n",
            " 'B-FAC': 22,\n",
            " 'B-GPE': 26,\n",
            " 'B-LANGUAGE': 23,\n",
            " 'B-LAW': 0,\n",
            " 'B-LOC': 21,\n",
            " 'B-MONEY': 4,\n",
            " 'B-NORP': 1,\n",
            " 'B-ORDINAL': 11,\n",
            " 'B-ORG': 30,\n",
            " 'B-PERCENT': 2,\n",
            " 'B-PERSON': 18,\n",
            " 'B-PRODUCT': 8,\n",
            " 'B-QUANTITY': 12,\n",
            " 'B-TIME': 6,\n",
            " 'B-WORK_OF_ART': 32,\n",
            " 'I-CARDINAL': 5,\n",
            " 'I-DATE': 16,\n",
            " 'I-EVENT': 28,\n",
            " 'I-FAC': 24,\n",
            " 'I-GPE': 13,\n",
            " 'I-LANGUAGE': 10,\n",
            " 'I-LAW': 15,\n",
            " 'I-LOC': 19,\n",
            " 'I-MONEY': 27,\n",
            " 'I-NORP': 33,\n",
            " 'I-ORDINAL': 34,\n",
            " 'I-ORG': 31,\n",
            " 'I-PERCENT': 9,\n",
            " 'I-PERSON': 35,\n",
            " 'I-PRODUCT': 20,\n",
            " 'I-QUANTITY': 3,\n",
            " 'I-TIME': 17,\n",
            " 'I-WORK_OF_ART': 29,\n",
            " 'O': 36}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k8DshceEaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd22e9d3-95cb-4615-d5bc-00e02607bf17"
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in train_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
        "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "pprint(vectorized_labels[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36, 30, 31, 31, 36, 36, 36, 36, 36, 36, 36, 36, 18, 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtqLdCMPf3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "5e5b632e-dc94-457d-9250-e0d3101fbf67"
      },
      "source": [
        "## vectorization of the texts\n",
        "def text_vectorizer(vocab, train_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(train_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
        "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "test_vectorized_data, test_lengths = text_vectorizer(vocabulary,test_texts)\n",
        "\n",
        "pprint(train_texts[0])\n",
        "pprint(vectorized_data[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['But',\n",
            " 'the',\n",
            " 'Bush',\n",
            " 'administration',\n",
            " '``',\n",
            " 'has',\n",
            " 'abdicated',\n",
            " 'that',\n",
            " 'obligation',\n",
            " ',',\n",
            " \"''\",\n",
            " 'says',\n",
            " 'Frelick',\n",
            " '.']\n",
            "[129, 3, 1417, 1132, 1, 45, 1, 13, 7820, 2, 1, 208, 1, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6FH5F1QGrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc469181-9328-489e-9b5d-addaff454126"
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "arr_lengths = numpy.array(lengths)\n",
        "max_len = numpy.max(arr_lengths)\n",
        "print(max_len)\n",
        "\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max_len)\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max_len), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])\n",
        "\n",
        "# Same stuff for the validation data\n",
        "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max_len)\n",
        "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post',maxlen=max_len), -1)\n",
        "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
        "validation_weights[validation_weights > 0] = 1\n",
        "\n",
        "#and for test data\n",
        "test_vectorized_data_padded=pad_sequences(test_vectorized_data, padding='post', maxlen=max_len)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "145\n",
            "Old shape: (50251,)\n",
            "New shape: (50251, 145)\n",
            "First example:\n",
            "[ 129    3 1417 1132    1   45    1   13 7820    2    1  208    1    4\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "Padded labels shape: (50251, 145, 1)\n",
            "{'B-CARDINAL': 25,\n",
            " 'B-DATE': 14,\n",
            " 'B-EVENT': 7,\n",
            " 'B-FAC': 22,\n",
            " 'B-GPE': 26,\n",
            " 'B-LANGUAGE': 23,\n",
            " 'B-LAW': 0,\n",
            " 'B-LOC': 21,\n",
            " 'B-MONEY': 4,\n",
            " 'B-NORP': 1,\n",
            " 'B-ORDINAL': 11,\n",
            " 'B-ORG': 30,\n",
            " 'B-PERCENT': 2,\n",
            " 'B-PERSON': 18,\n",
            " 'B-PRODUCT': 8,\n",
            " 'B-QUANTITY': 12,\n",
            " 'B-TIME': 6,\n",
            " 'B-WORK_OF_ART': 32,\n",
            " 'I-CARDINAL': 5,\n",
            " 'I-DATE': 16,\n",
            " 'I-EVENT': 28,\n",
            " 'I-FAC': 24,\n",
            " 'I-GPE': 13,\n",
            " 'I-LANGUAGE': 10,\n",
            " 'I-LAW': 15,\n",
            " 'I-LOC': 19,\n",
            " 'I-MONEY': 27,\n",
            " 'I-NORP': 33,\n",
            " 'I-ORDINAL': 34,\n",
            " 'I-ORG': 31,\n",
            " 'I-PERCENT': 9,\n",
            " 'I-PERSON': 35,\n",
            " 'I-PRODUCT': 20,\n",
            " 'I-QUANTITY': 3,\n",
            " 'I-TIME': 17,\n",
            " 'I-WORK_OF_ART': 29,\n",
            " 'O': 36}\n",
            "First example labels:\n",
            "array([[36],\n",
            "       [30],\n",
            "       [31],\n",
            "       [31],\n",
            "       [36],\n",
            "       [36],\n",
            "       [36],\n",
            "       [36],\n",
            "       [36],\n",
            "       [36],\n",
            "       [36],\n",
            "       [36],\n",
            "       [18],\n",
            "       [36],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0]], dtype=int32)\n",
            "First weight vector:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmC32aOqM7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function 1, token level\n",
        "import keras\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def accuracy(predictions, gold, lengths):\n",
        "    pred_tags = numpy.concatenate([labels[:lengths[i]] for i, labels in enumerate(predictions)]).ravel()\n",
        "    \n",
        "    gold_tags = numpy.concatenate([labels[:lengths[i], 0] for i, labels in enumerate(gold)]).ravel()\n",
        "    accuracy = accuracy_score(gold_tags, pred_tags)\n",
        "    print('Accuracy:', accuracy )\n",
        "    return accuracy\n",
        "#local variables\n",
        "valid_VD_pad = validation_vectorized_data_padded\n",
        "valid_VL_pad = validation_vectorized_labels_padded\n",
        "valid_lengths = validation_lengths\n",
        "\n",
        "class EvaluateTags(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.accuracy = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        pred = numpy.argmax(self.model.predict(valid_VD_pad), axis=-1)\n",
        "        eval_parameter = accuracy(pred, valid_VL_pad, valid_lengths)\n",
        "        self.accuracy.append(eval_parameter)\n",
        "        return  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAOVAbBTRAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function 2, entities\n",
        "\n",
        "def _convert_to_entities(input_sequence):\n",
        "    \"\"\"\n",
        "    Reads a sequence of tags and converts them into a set of entities.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    previous_tag = label_map['O']\n",
        "    for i, tag in enumerate(input_sequence):\n",
        "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "            current_entity.append((tag, i))\n",
        "        elif tag == label_map['O']: # Entity has ended\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "        elif tag == previous_tag: # Current entity continues\n",
        "            current_entity.append((tag, i))\n",
        "        previous_tag = tag\n",
        "    \n",
        "    # Add the last entity to our entity list if the sentences ends with an entity\n",
        "    if len(current_entity) > 0:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    entity_offsets = set()\n",
        "    \n",
        "    for e in entities:\n",
        "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
        "    return entity_offsets\n",
        "\n",
        "def _entity_level_PRF(predictions, gold, lengths):\n",
        "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
        "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
        "    \n",
        "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
        "    pred_count = sum([len(e) for e in pred_entities])\n",
        "    \n",
        "    try:\n",
        "        precision = tp / pred_count # tp / (tp+np)\n",
        "        recall = tp / sum([len(e) for e in gold_entities])\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    except Exception as e:\n",
        "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
        "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
        "    return precision, recall, fscore             \n",
        "\n",
        "def evaluate(predictions, gold, lengths):\n",
        "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
        "    return precision, recall, fscore\n",
        "\n",
        "class EvaluateEntities(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.precision = []\n",
        "        self.recall = []\n",
        "        self.fscore = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
        "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
        "        self.precision.append(evaluation_parameters[0])\n",
        "        self.recall.append(evaluation_parameters[1])\n",
        "        self.fscore.append(evaluation_parameters[2])\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ydCexfTg5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b11d3ee1-08f5-4197-af6b-a3c21c314cc6"
      },
      "source": [
        "# model, basic with relu and Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model1 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)\n",
        "\n",
        "print(model1.summary())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 145)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 145, 300)          15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 145, 100)          30100     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 145, 37)           3737      \n",
            "=================================================================\n",
            "Total params: 15,034,437\n",
            "Trainable params: 33,837\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIPQrLXUVrWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "0f7eae57-6f6c-4352-e90a-70febc4e599b"
      },
      "source": [
        "# train the model\n",
        "optimizer=Adam(lr=0.007) # define the learning rate\n",
        "model1.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function1=EvaluateEntities()\n",
        "evaluation_function2=EvaluateTags()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model1.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function1, evaluation_function2])\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 3s - loss: 0.0291\n",
            "\n",
            "Precision/Recall/F-score: 0.5289235199457949 / 0.25843161597351544 / 0.34721450016679645\n",
            "Accuracy: 0.8961273353890804\n",
            "Epoch 2/10\n",
            " - 2s - loss: 0.0189\n",
            "\n",
            "Precision/Recall/F-score: 0.502910882221227 / 0.32530519346161807 / 0.39506483063624487\n",
            "Accuracy: 0.9039225467077816\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.0182\n",
            "\n",
            "Precision/Recall/F-score: 0.5187881634570221 / 0.27423960273122283 / 0.35880776415170956\n",
            "Accuracy: 0.8975920551423907\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.0178\n",
            "\n",
            "Precision/Recall/F-score: 0.5055973813420622 / 0.3195944547899855 / 0.39163286004056796\n",
            "Accuracy: 0.9037275530564122\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.0176\n",
            "\n",
            "Precision/Recall/F-score: 0.5130817775440442 / 0.3229877922615353 / 0.396424308606547\n",
            "Accuracy: 0.9031561763105387\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.0174\n",
            "\n",
            "Precision/Recall/F-score: 0.5413807358412567 / 0.27097041175253467 / 0.361169332597904\n",
            "Accuracy: 0.8964946490114275\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.0172\n",
            "\n",
            "Precision/Recall/F-score: 0.5113504669711637 / 0.31040761431822883 / 0.3863109646186332\n",
            "Accuracy: 0.9017594775984038\n",
            "Epoch 8/10\n",
            " - 2s - loss: 0.0171\n",
            "\n",
            "Precision/Recall/F-score: 0.5055261893320518 / 0.3047382578108835 / 0.38025405349581737\n",
            "Accuracy: 0.9015418102666425\n",
            "Epoch 9/10\n",
            " - 2s - loss: 0.0170\n",
            "\n",
            "Precision/Recall/F-score: 0.5039227895392279 / 0.33490585557624664 / 0.4023865755127409\n",
            "Accuracy: 0.90478414656267\n",
            "Epoch 10/10\n",
            " - 2s - loss: 0.0169\n",
            "\n",
            "Precision/Recall/F-score: 0.5094818376068376 / 0.3157459135112766 / 0.38987251219948393\n",
            "Accuracy: 0.9009658987846907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PovjRyU3AWP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "#save model and init weights\n",
        "model1.save('model1.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bah0URSVV5GP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "e3a0b6fe-9aff-4e12-c369-46acf704d7f3"
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    f_arr = numpy.array(fscores)\n",
        "    p_arr = numpy.array(precision)\n",
        "    a_arr = numpy.array(accuracy)\n",
        "    print(\"HISTORY:\")\n",
        "    print(\"Entity level\")\n",
        "    print(\"F-scores, entity level: \",fscores)\n",
        "    print(\"Highest f-score:\", numpy.max(f_arr))\n",
        "    print(\"Precision: \",precision)\n",
        "    print(\"Highest precision:\", numpy.max(p_arr))\n",
        "    print('-----------------------------')\n",
        "    print(\"Token level\")\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"Highest precision:\", numpy.max(a_arr))\n",
        "    \n",
        "    line1 = plt.plot(fscores)\n",
        "    line1.set_label(\"F-scores\")\n",
        "    line2 = plt.plot(precision)\n",
        "    line2.set_label(\"Precision\")\n",
        "    line3= plt.plot(accuracy)\n",
        "    line3.set_label(\"Token level, accuracy\")\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function1.fscore, evaluation_function1.precision)\n",
        "plot_history(evaluation_function2.accuracy)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d2daa38df81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_function1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_function1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_function2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-d2daa38df81d>\u001b[0m in \u001b[0;36mplot_history\u001b[0;34m(fscores)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mf_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mp_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ma_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HISTORY:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'precision' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BimErOSrMhqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "0e04482c-a8e8-4271-b425-669f9474e0e3"
      },
      "source": [
        "#prediction on test data\n",
        "m1_predictions = model1.predict(test_vectorized_data_padded)\n",
        "\n",
        "labels = numpy.argmax(m1_predictions, axis=-1)    \n",
        "pprint(labels[0])\n"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([ 4,  4,  4,  4,  4,  4,  4, 22, 35,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 15,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
            "        4,  4,  4,  4,  4,  4,  4,  4,  4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZsm9armCG4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 2 with sigmoid activation and Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model2 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)\n",
        "\n",
        "print(model2.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csFaCX7eCDGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train another model\n",
        "optimizer=Adam(lr=0.005) # define the learning rate\n",
        "model2.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "evaluation_function2=EvaluateTags()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model2.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function1, evaluation_function2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwI5fOALIuwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model and init weights\n",
        "model2.save('model2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg-s7U7eCQXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    f_arr = numpy.array(fscores)\n",
        "    p_arr = numpy.array(precision)\n",
        "    a_arr = numpy.array(accuracy)\n",
        "    print(\"HISTORY:\")\n",
        "    print(\"Entity level\")\n",
        "    print(\"F-scores, entity level: \",fscores)\n",
        "    print(\"Highest f-score:\", numpy.max(f_arr))\n",
        "    print(\"Precision: \",precision)\n",
        "    print(\"Highest precision:\", numpy.max(p_arr))\n",
        "    print('-----------------------------')\n",
        "    print(\"Token level\")\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"Highest precision:\", numpy.max(a_arr))\n",
        "    \n",
        "    line1 = plt.plot(fscores)\n",
        "    line1.set_label(\"F-scores\")\n",
        "    line2 = plt.plot(precision)\n",
        "    line2.set_label(\"Precision\")\n",
        "    line3= plt.plot(accuracy)\n",
        "    line3.set_label(\"Token level, accuracy\")\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function1.fscore, evaluation_function1.precision, evaluation_function2.accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMB5SeuXC2j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 3 with relu activation and Adamax\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, Adamax, Adadelta\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"exponential\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model3 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)\n",
        "\n",
        "print(model3.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "321odNcHGPWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train third model\n",
        "optimizer=Adamax(0.009) # define the learning rate, if applicable\n",
        "model3.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function1=EvaluateEntities()\n",
        "evaluation_function2=EvaluateTags()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model3.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function1, evaluation_function2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1G8Ikm4Ix_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model and init weights\n",
        "model3.save('model3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y22OKBRGR5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    f_arr = numpy.array(fscores)\n",
        "    p_arr = numpy.array(precision)\n",
        "    a_arr = numpy.array(accuracy)\n",
        "    print(\"HISTORY:\")\n",
        "    print(\"Entity level\")\n",
        "    print(\"F-scores, entity level: \",fscores)\n",
        "    print(\"Highest f-score:\", numpy.max(f_arr))\n",
        "    print(\"Precision: \",precision)\n",
        "    print(\"Highest precision:\", numpy.max(p_arr))\n",
        "    print('-----------------------------')\n",
        "    print(\"Token level\")\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"Highest precision:\", numpy.max(a_arr))\n",
        "    \n",
        "    line1 = plt.plot(fscores)\n",
        "    line1.set_label(\"F-scores\")\n",
        "    line2 = plt.plot(precision)\n",
        "    line2.set_label(\"Precision\")\n",
        "    line3= plt.plot(accuracy)\n",
        "    line3.set_label(\"Token level, accuracy\")\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function1.fscore, evaluation_function1.precision, evaluation_function2.accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs1AYMUCLa2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 4 with relu activation and SGD\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, Adamax\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model4 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)\n",
        "\n",
        "print(model4.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwMwwo9-MPv4"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9pXRewbyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#expanding to RNN model with context\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "rnn_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
        "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
        "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP-kwoNXUMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIal9meVXnN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer=Adam(lr=0.01) # define the learning rate\n",
        "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKNs4t8X3Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCo0xF5kMMbH"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgSYNcerMI9R"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRDxKgLSL_uf"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlG6ZWkIL-HY"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}