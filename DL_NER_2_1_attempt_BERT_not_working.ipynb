{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER_2.1.attempt_BERT_not_working.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainvo/DeepLearning_NER/blob/master/DL_NER_2_1_attempt_BERT_not_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTBsYI1tLeVk"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2GevEzfPP2",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/api/annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O5MwAmUALZ4V"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q3HiGQgMU5L",
        "outputId": "e426fcb1-850f-4abb-e7cc-d0061fef18d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Training data: Used for training the model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
        "\n",
        "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
        "\n",
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
        "\n",
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-10 00:18:37--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17252156 (16M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "train.tsv           100%[===================>]  16.45M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-10 00:18:38 (161 MB/s) - ‘train.tsv’ saved [17252156/17252156]\n",
            "\n",
            "--2020-05-10 00:18:41--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2419425 (2.3M) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>]   2.31M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-10 00:18:42 (21.0 MB/s) - ‘dev.tsv’ saved [2419425/2419425]\n",
            "\n",
            "--2020-05-10 00:18:44--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1788466 (1.7M) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>]   1.71M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-05-10 00:18:44 (19.1 MB/s) - ‘test.tsv’ saved [1788466/1788466]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOOHEYpiMzFp",
        "outputId": "2a02e763-42e4-449d-8952-ff4fe1c0aeec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "from collections import namedtuple\n",
        "OneWord=namedtuple(\"OneWord\",[\"word\",\"entity_label\"])\n",
        "\n",
        "def read_ontonotes(tsv_file):\n",
        "  #\"\"\"Yield complete sentences\"\"\"\n",
        "    current_sentence=[] # list of (word,label) tuples\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\t')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line: #sentence break\n",
        "                if current_sentence: #if we gathered a sentence, we should yield it, because a new starts\n",
        "                    yield current_sentence #much like return, but continues past this line once the element has been consumed\n",
        "                    current_sentence=[] #...and start a new one\n",
        "                continue\n",
        "            #if we made it here, we are on a normal line\n",
        "            columns=[line[0], line[1]] #an actual word line\n",
        "            assert len(columns)==2 #we should have four columns, looking at the data\n",
        "            current_sentence.append(OneWord(*columns)) #shorthand for looping over columns\n",
        "        else: #for ... else -> the else part is executed once, when \"for\" runs out of elements\n",
        "            if current_sentence: #yield also the last one!\n",
        "                yield current_sentence\n",
        "\n",
        "#read the data in as sentences\n",
        "sentences_train=list(read_ontonotes(\"train.tsv\"))\n",
        "sentences_dev=list(read_ontonotes(\"dev.tsv\"))\n",
        "sentences_test = list(read_ontonotes(\"test.tsv\"))\n",
        "\n",
        "print(type(sentences_test))\n",
        "\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_train[:3]:\n",
        "    print(sent)\n",
        "print(len(sentences_train))\n",
        "print('---------------------------------------------')\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_dev[:3]:\n",
        "    print(sent)\n",
        "print(len(sentences_dev))\n",
        "print('---------------------------------------------')\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_test[:3]:\n",
        "    print(sent)\n",
        "print(len(sentences_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "First three sentences\n",
            "[OneWord(word='Big', entity_label='O'), OneWord(word='Managers', entity_label='O'), OneWord(word='on', entity_label='O'), OneWord(word='Campus', entity_label='O')]\n",
            "[OneWord(word='In', entity_label='O'), OneWord(word='recent', entity_label='B-DATE'), OneWord(word='years', entity_label='I-DATE'), OneWord(word=',', entity_label='O'), OneWord(word='advanced', entity_label='O'), OneWord(word='education', entity_label='O'), OneWord(word='for', entity_label='O'), OneWord(word='professionals', entity_label='O'), OneWord(word='has', entity_label='O'), OneWord(word='become', entity_label='O'), OneWord(word='a', entity_label='O'), OneWord(word='hot', entity_label='O'), OneWord(word='topic', entity_label='O'), OneWord(word='in', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='business', entity_label='O'), OneWord(word='community', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "[OneWord(word='With', entity_label='O'), OneWord(word='this', entity_label='O'), OneWord(word='trend', entity_label='O'), OneWord(word=',', entity_label='O'), OneWord(word='suddenly', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='mature', entity_label='O'), OneWord(word='faces', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='managers', entity_label='O'), OneWord(word='boasting', entity_label='O'), OneWord(word='an', entity_label='O'), OneWord(word='average', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='over', entity_label='O'), OneWord(word='ten', entity_label='B-DATE'), OneWord(word='years', entity_label='I-DATE'), OneWord(word='of', entity_label='O'), OneWord(word='professional', entity_label='O'), OneWord(word='experience', entity_label='O'), OneWord(word='have', entity_label='O'), OneWord(word='flooded', entity_label='O'), OneWord(word='in', entity_label='O'), OneWord(word='among', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='young', entity_label='O'), OneWord(word='people', entity_label='O'), OneWord(word='populating', entity_label='O'), OneWord(word='university', entity_label='O'), OneWord(word='campuses', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "66818\n",
            "---------------------------------------------\n",
            "First three sentences\n",
            "[OneWord(word='President', entity_label='B-WORK_OF_ART'), OneWord(word='Chen', entity_label='I-WORK_OF_ART'), OneWord(word='Travels', entity_label='I-WORK_OF_ART'), OneWord(word='Abroad', entity_label='I-WORK_OF_ART')]\n",
            "[OneWord(word='(', entity_label='O'), OneWord(word='Chang', entity_label='B-PERSON'), OneWord(word='Chiung', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='fang', entity_label='I-PERSON'), OneWord(word='/', entity_label='O'), OneWord(word='tr.', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='David', entity_label='B-PERSON'), OneWord(word='Mayer', entity_label='I-PERSON'), OneWord(word=')', entity_label='O')]\n",
            "[OneWord(word='President', entity_label='O'), OneWord(word='Chen', entity_label='B-PERSON'), OneWord(word='Shui', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='bian', entity_label='I-PERSON'), OneWord(word='visited', entity_label='O'), OneWord(word='the', entity_label='B-FAC'), OneWord(word='Nicaraguan', entity_label='I-FAC'), OneWord(word='National', entity_label='I-FAC'), OneWord(word='Assembly', entity_label='I-FAC'), OneWord(word='on', entity_label='O'), OneWord(word='August', entity_label='B-DATE'), OneWord(word='17', entity_label='I-DATE'), OneWord(word=',', entity_label='O'), OneWord(word='where', entity_label='O'), OneWord(word='he', entity_label='O'), OneWord(word='received', entity_label='O'), OneWord(word='a', entity_label='O'), OneWord(word='medal', entity_label='O'), OneWord(word='from', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='president', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='assembly', entity_label='O'), OneWord(word=',', entity_label='O'), OneWord(word='Ivan', entity_label='B-PERSON'), OneWord(word='Escobar', entity_label='I-PERSON'), OneWord(word='Fornos', entity_label='I-PERSON'), OneWord(word='.', entity_label='O')]\n",
            "11612\n",
            "---------------------------------------------\n",
            "First three sentences\n",
            "[OneWord(word='Powerful', entity_label='B-WORK_OF_ART'), OneWord(word='Tools', entity_label='I-WORK_OF_ART'), OneWord(word='for', entity_label='I-WORK_OF_ART'), OneWord(word='Biotechnology', entity_label='I-WORK_OF_ART'), OneWord(word='-', entity_label='I-WORK_OF_ART'), OneWord(word='Biochips', entity_label='I-WORK_OF_ART')]\n",
            "[OneWord(word='(', entity_label='O'), OneWord(word='Chang', entity_label='B-PERSON'), OneWord(word='Chiung', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='fang', entity_label='I-PERSON'), OneWord(word='/', entity_label='O'), OneWord(word='photos', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='Hsueh', entity_label='B-PERSON'), OneWord(word='Chi', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='kuang', entity_label='I-PERSON'), OneWord(word='/', entity_label='O'), OneWord(word='tr.', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='Robert', entity_label='B-PERSON'), OneWord(word='Taylor', entity_label='I-PERSON'), OneWord(word=')', entity_label='O')]\n",
            "[OneWord(word='The', entity_label='O'), OneWord(word='enterovirus', entity_label='O'), OneWord(word='detection', entity_label='O'), OneWord(word='biochip', entity_label='O'), OneWord(word='developed', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='DR.', entity_label='B-ORG'), OneWord(word='Chip', entity_label='I-ORG'), OneWord(word='Biotechnology', entity_label='I-ORG'), OneWord(word='takes', entity_label='O'), OneWord(word='only', entity_label='B-TIME'), OneWord(word='six', entity_label='I-TIME'), OneWord(word='hours', entity_label='I-TIME'), OneWord(word='to', entity_label='O'), OneWord(word='give', entity_label='O'), OneWord(word='hospitals', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='answer', entity_label='O'), OneWord(word='to', entity_label='O'), OneWord(word='whether', entity_label='O'), OneWord(word='a', entity_label='O'), OneWord(word='sample', entity_label='O'), OneWord(word='contains', entity_label='O'), OneWord(word='enterovirus', entity_label='O'), OneWord(word=',', entity_label='O'), OneWord(word='and', entity_label='O'), OneWord(word='if', entity_label='O'), OneWord(word='it', entity_label='O'), OneWord(word='is', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='deadly', entity_label='O'), OneWord(word='strain', entity_label='O'), OneWord(word='Entero', entity_label='O'), OneWord(word='71', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "9751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZHhXzVTQA_P",
        "colab": {}
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for line in f:\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for OneWord in line:\n",
        "            #print(OneWord)\n",
        "            sent_text.append(OneWord.word)\n",
        "            sent_tags.append(OneWord.entity_label)\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict)\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "train_data = list(reshape_sent2dicts(sentences_train[:30000]))\n",
        "\n",
        "dev_data = list(reshape_sent2dicts(sentences_dev))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpYBQMbcQGBi",
        "outputId": "868bae0c-326e-4deb-ee71-61785379968a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "random.seed(123)\n",
        "random.shuffle(train_data)\n",
        "print(type(train_data))\n",
        "print(type(train_data[0]))\n",
        "\n",
        "train_texts=[i[\"text\"] for i in train_data]\n",
        "train_labels=[i[\"tags\"] for i in train_data]\n",
        "\n",
        "print(type(train_texts))\n",
        "print(type(train_texts[0]))\n",
        "\n",
        "print('Text: ', train_texts[0])\n",
        "print('Label: ', train_labels[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Text:  ['Sharon', 'Repudiates', 'the', 'Road', 'Map', '.']\n",
            "Label:  ['O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQQRw0YO-Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## same for validation/dev data\n",
        "dev_texts=[i[\"text\"] for i in dev_data]\n",
        "dev_labels=[i[\"tags\"] for i in dev_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICn08fOgbyXl",
        "colab_type": "code",
        "outputId": "4a5f56b9-6f2b-4032-e702-db5898cbf7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-10 00:18:52--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  10.7MB/s    in 62s     \n",
            "\n",
            "2020-05-10 00:19:56 (10.5 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFv2qclTbyXp",
        "colab_type": "code",
        "outputId": "eb2da43b-1a6d-456a-b976-b9705ac82c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_C5ii8byXt",
        "colab_type": "code",
        "outputId": "4d92965e-dff2-4a1c-e68b-9b96d7089b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdgjgOlbyXx",
        "colab_type": "code",
        "outputId": "2eb37e6c-0669-4c09-88a9-480fc6d6a2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGaojUBhbyX2",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Ox5_ObyX3",
        "colab_type": "code",
        "outputId": "4c5ec0c3-e70b-4ae1-a75c-046dc5dbc66e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#Labels\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in train_labels: #loops over sentences \n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 27,\n",
            " 'B-DATE': 25,\n",
            " 'B-EVENT': 17,\n",
            " 'B-FAC': 36,\n",
            " 'B-GPE': 31,\n",
            " 'B-LANGUAGE': 15,\n",
            " 'B-LAW': 9,\n",
            " 'B-LOC': 34,\n",
            " 'B-MONEY': 16,\n",
            " 'B-NORP': 30,\n",
            " 'B-ORDINAL': 5,\n",
            " 'B-ORG': 28,\n",
            " 'B-PERCENT': 7,\n",
            " 'B-PERSON': 22,\n",
            " 'B-PRODUCT': 1,\n",
            " 'B-QUANTITY': 33,\n",
            " 'B-TIME': 26,\n",
            " 'B-WORK_OF_ART': 11,\n",
            " 'I-CARDINAL': 24,\n",
            " 'I-DATE': 13,\n",
            " 'I-EVENT': 35,\n",
            " 'I-FAC': 14,\n",
            " 'I-GPE': 12,\n",
            " 'I-LANGUAGE': 0,\n",
            " 'I-LAW': 3,\n",
            " 'I-LOC': 2,\n",
            " 'I-MONEY': 20,\n",
            " 'I-NORP': 19,\n",
            " 'I-ORDINAL': 10,\n",
            " 'I-ORG': 29,\n",
            " 'I-PERCENT': 32,\n",
            " 'I-PERSON': 6,\n",
            " 'I-PRODUCT': 4,\n",
            " 'I-QUANTITY': 21,\n",
            " 'I-TIME': 18,\n",
            " 'I-WORK_OF_ART': 23,\n",
            " 'O': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k8DshceEaI",
        "colab_type": "code",
        "outputId": "9c62b90a-0713-4f54-d537-a02f1deb89c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in train_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
        "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "pprint(vectorized_labels[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8, 8, 8, 8, 8, 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtqLdCMPf3X",
        "colab_type": "code",
        "outputId": "ab96c7bb-b74f-4687-d626-d5f79bfa93d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## vectorization of the texts\n",
        "def text_vectorizer(vocab, train_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(train_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
        "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "\n",
        "pprint(train_texts[0])\n",
        "pprint(vectorized_data[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sharon', 'Repudiates', 'the', 'Road', 'Map', '.']\n",
            "[8346, 1, 3, 1685, 8936, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6FH5F1QGrq",
        "colab_type": "code",
        "outputId": "a3d7f046-a050-4f32-df3c-950a48ebe83b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max(lengths))\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max(lengths)), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])\n",
        "\n",
        "# Same stuff for the validation data\n",
        "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max(lengths))\n",
        "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post',maxlen=max(lengths)), -1)\n",
        "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
        "validation_weights[validation_weights > 0] = 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Old shape: (30000,)\n",
            "New shape: (30000, 561)\n",
            "First example:\n",
            "[8346    1    3 1685 8936    4    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n",
            "Padded labels shape: (30000, 561, 1)\n",
            "{'B-CARDINAL': 27,\n",
            " 'B-DATE': 25,\n",
            " 'B-EVENT': 17,\n",
            " 'B-FAC': 36,\n",
            " 'B-GPE': 31,\n",
            " 'B-LANGUAGE': 15,\n",
            " 'B-LAW': 9,\n",
            " 'B-LOC': 34,\n",
            " 'B-MONEY': 16,\n",
            " 'B-NORP': 30,\n",
            " 'B-ORDINAL': 5,\n",
            " 'B-ORG': 28,\n",
            " 'B-PERCENT': 7,\n",
            " 'B-PERSON': 22,\n",
            " 'B-PRODUCT': 1,\n",
            " 'B-QUANTITY': 33,\n",
            " 'B-TIME': 26,\n",
            " 'B-WORK_OF_ART': 11,\n",
            " 'I-CARDINAL': 24,\n",
            " 'I-DATE': 13,\n",
            " 'I-EVENT': 35,\n",
            " 'I-FAC': 14,\n",
            " 'I-GPE': 12,\n",
            " 'I-LANGUAGE': 0,\n",
            " 'I-LAW': 3,\n",
            " 'I-LOC': 2,\n",
            " 'I-MONEY': 20,\n",
            " 'I-NORP': 19,\n",
            " 'I-ORDINAL': 10,\n",
            " 'I-ORG': 29,\n",
            " 'I-PERCENT': 32,\n",
            " 'I-PERSON': 6,\n",
            " 'I-PRODUCT': 4,\n",
            " 'I-QUANTITY': 21,\n",
            " 'I-TIME': 18,\n",
            " 'I-WORK_OF_ART': 23,\n",
            " 'O': 8}\n",
            "First example labels:\n",
            "array([[8],\n",
            "       [8],\n",
            "       [8],\n",
            "       [8],\n",
            "       [8],\n",
            "       [8],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0]], dtype=int32)\n",
            "First weight vector:\n",
            "[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAOVAbBTRAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function\n",
        "import keras\n",
        "\n",
        "def _convert_to_entities(input_sequence):\n",
        "    \"\"\"\n",
        "    Reads a sequence of tags and converts them into a set of entities.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    previous_tag = label_map['O']\n",
        "    for i, tag in enumerate(input_sequence):\n",
        "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "            current_entity.append((tag, i))\n",
        "        elif tag == label_map['O']: # Entity has ended\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "        elif tag == previous_tag: # Current entity continues\n",
        "            current_entity.append((tag, i))\n",
        "        previous_tag = tag\n",
        "    \n",
        "    # Add the last entity to our entity list if the sentences ends with an entity\n",
        "    if len(current_entity) > 0:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    entity_offsets = set()\n",
        "    \n",
        "    for e in entities:\n",
        "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
        "    return entity_offsets\n",
        "\n",
        "def _entity_level_PRF(predictions, gold, lengths):\n",
        "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
        "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
        "    \n",
        "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
        "    pred_count = sum([len(e) for e in pred_entities])\n",
        "    \n",
        "    try:\n",
        "        precision = tp / pred_count # tp / (tp+np)\n",
        "        recall = tp / sum([len(e) for e in gold_entities])\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    except Exception as e:\n",
        "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
        "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
        "    return precision, recall, fscore             \n",
        "\n",
        "def evaluate(predictions, gold, lengths):\n",
        "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
        "    return precision, recall, fscore\n",
        "\n",
        "class EvaluateEntities(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.precision = []\n",
        "        self.recall = []\n",
        "        self.fscore = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
        "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
        "        self.precision.append(evaluation_parameters[0])\n",
        "        self.recall.append(evaluation_parameters[1])\n",
        "        self.fscore.append(evaluation_parameters[2])\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ydCexfTg5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oUO3GLfTrl3",
        "colab_type": "code",
        "outputId": "f5aa9b9f-08b2-4485-e037-696e08e831b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 561, 300)          15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 561, 100)          30100     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 561, 37)           3737      \n",
            "=================================================================\n",
            "Total params: 15,034,437\n",
            "Trainable params: 33,837\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIPQrLXUVrWr",
        "colab_type": "code",
        "outputId": "882d91fe-dcb6-4fcf-899d-5921127286b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train the model\n",
        "optimizer=Adam(lr=0.007) # define the learning rate\n",
        "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=20, callbacks=[evaluation_function])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " - 4s - loss: 0.0125\n",
            "\n",
            "Precision/Recall/F-score: 0.4911147011308562 / 0.16484112352239452 / 0.24683338746346214\n",
            "Epoch 2/20\n",
            " - 2s - loss: 0.0081\n",
            "\n",
            "Precision/Recall/F-score: 0.5111327018055218 / 0.22821096771861332 / 0.31553956115359627\n",
            "Epoch 3/20\n",
            " - 2s - loss: 0.0075\n",
            "\n",
            "Precision/Recall/F-score: 0.49011198475101264 / 0.2974370097241803 / 0.3702053947042811\n",
            "Epoch 4/20\n",
            " - 2s - loss: 0.0072\n",
            "\n",
            "Precision/Recall/F-score: 0.4943537534374817 / 0.3054260203159455 / 0.3775751888099388\n",
            "Epoch 5/20\n",
            " - 2s - loss: 0.0071\n",
            "\n",
            "Precision/Recall/F-score: 0.5026918298526889 / 0.30716119003723386 / 0.3813220840999865\n",
            "Epoch 6/20\n",
            " - 2s - loss: 0.0070\n",
            "\n",
            "Precision/Recall/F-score: 0.49126321647189763 / 0.31912663124028484 / 0.3869129796419258\n",
            "Epoch 7/20\n",
            " - 2s - loss: 0.0069\n",
            "\n",
            "Precision/Recall/F-score: 0.5068852459016393 / 0.3073780862523949 / 0.3826904901210675\n",
            "Epoch 8/20\n",
            " - 2s - loss: 0.0068\n",
            "\n",
            "Precision/Recall/F-score: 0.47409872434830835 / 0.30900480786610274 / 0.374149213227409\n",
            "Epoch 9/20\n",
            " - 2s - loss: 0.0068\n",
            "\n",
            "Precision/Recall/F-score: 0.4995276892195064 / 0.30585981274626756 / 0.37940853344095427\n",
            "Epoch 10/20\n",
            " - 2s - loss: 0.0067\n",
            "\n",
            "Precision/Recall/F-score: 0.4837497958517067 / 0.3212232946535083 / 0.38607924921793535\n",
            "Epoch 11/20\n",
            " - 2s - loss: 0.0067\n",
            "\n",
            "Precision/Recall/F-score: 0.49566419504838505 / 0.28514622419838775 / 0.36202583931890675\n",
            "Epoch 12/20\n",
            " - 2s - loss: 0.0067\n",
            "\n",
            "Precision/Recall/F-score: 0.48734573167263834 / 0.3097639446191664 / 0.3787738142598241\n",
            "Epoch 13/20\n",
            " - 2s - loss: 0.0066\n",
            "\n",
            "Precision/Recall/F-score: 0.4889332667665169 / 0.31862054007157575 / 0.38581746552856205\n",
            "Epoch 14/20\n",
            " - 2s - loss: 0.0066\n",
            "\n",
            "Precision/Recall/F-score: 0.4799325463743676 / 0.3086433141741677 / 0.37568477328229155\n",
            "Epoch 15/20\n",
            " - 2s - loss: 0.0066\n",
            "\n",
            "Precision/Recall/F-score: 0.4870537241929965 / 0.30871561291255467 / 0.3779011881319556\n",
            "Epoch 16/20\n",
            " - 2s - loss: 0.0066\n",
            "\n",
            "Precision/Recall/F-score: 0.48601452634832326 / 0.34106929834074395 / 0.4008411929645679\n",
            "Epoch 17/20\n",
            " - 2s - loss: 0.0066\n",
            "\n",
            "Precision/Recall/F-score: 0.4817807179264722 / 0.3211871452843148 / 0.3854245743411777\n",
            "Epoch 18/20\n",
            " - 2s - loss: 0.0065\n",
            "\n",
            "Precision/Recall/F-score: 0.4951096121416526 / 0.31840364385641473 / 0.38756517721602535\n",
            "Epoch 19/20\n",
            " - 2s - loss: 0.0065\n",
            "\n",
            "Precision/Recall/F-score: 0.4909763479430557 / 0.31666847413512633 / 0.3850126359740688\n",
            "Epoch 20/20\n",
            " - 2s - loss: 0.0065\n",
            "\n",
            "Precision/Recall/F-score: 0.49835104433858557 / 0.2949788526190218 / 0.3705974521425165\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bah0URSVV5GP",
        "colab_type": "code",
        "outputId": "7e72b8ce-00a0-4f80-a0e8-d6de7ba4b069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    print(\"History:\", fscores)\n",
        "    print(\"Highest f-score:\", max(fscores))\n",
        "    plt.plot(fscores)\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "History: [0.24683338746346214, 0.31553956115359627, 0.3702053947042811, 0.3775751888099388, 0.3813220840999865, 0.3869129796419258, 0.3826904901210675, 0.374149213227409, 0.37940853344095427, 0.38607924921793535, 0.36202583931890675, 0.3787738142598241, 0.38581746552856205, 0.37568477328229155, 0.3779011881319556, 0.4008411929645679, 0.3854245743411777, 0.38756517721602535, 0.3850126359740688, 0.3705974521425165]\n",
            "Highest f-score: 0.4008411929645679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwMwwo9-MPv4"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9pXRewbyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#expanding to RNN model with context\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "rnn_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
        "    rnn = LSTM(rnn_size, activation='sigmoid', return_sequences=True)(embeddings)\n",
        "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP-kwoNXUMb",
        "colab_type": "code",
        "outputId": "bc0c84f0-68a9-44eb-9a52-4a1c0b1292e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 561, 300)          15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 561, 100)          30100     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 561, 37)           3737      \n",
            "=================================================================\n",
            "Total params: 15,034,437\n",
            "Trainable params: 33,837\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIal9meVXnN_",
        "colab_type": "code",
        "outputId": "9b7d055f-33dd-4f9e-a543-3a68a5ec77cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "\n",
        "optimizer=Adam(lr=0.007) # define the learning rate\n",
        "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=5, callbacks=[evaluation_function])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " - 362s - loss: 0.0138\n",
            "\n",
            "Precision/Recall/F-score: 0.47042667974497304 / 0.17337237465206232 / 0.2533678482751334\n",
            "Epoch 2/5\n",
            " - 358s - loss: 0.0073\n",
            "\n",
            "Precision/Recall/F-score: 0.5696515654816595 / 0.291363915699671 / 0.38553525303740555\n",
            "Epoch 3/5\n",
            " - 358s - loss: 0.0063\n",
            "\n",
            "Precision/Recall/F-score: 0.5764580369843528 / 0.35158876477605466 / 0.43678006062647357\n",
            "Epoch 4/5\n",
            " - 357s - loss: 0.0058\n",
            "\n",
            "Precision/Recall/F-score: 0.5948531226857566 / 0.3484437696562195 / 0.4394647457086192\n",
            "Epoch 5/5\n",
            " - 359s - loss: 0.0056\n",
            "\n",
            "Precision/Recall/F-score: 0.5865805604203153 / 0.3874489390160142 / 0.4666594100359204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKNs4t8X3Ed",
        "colab_type": "code",
        "outputId": "8e711e3b-fa73-419e-daf8-89c7d1fad457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "History: [0.2533678482751334, 0.38553525303740555, 0.43678006062647357, 0.4394647457086192, 0.4666594100359204]\n",
            "Highest f-score: 0.4666594100359204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxU9b3/8dcnIRD2JQQJS0iAoECiiBHcpa4sXpDa9treLnrbUqu23mvrbQWsdcFae+vPttfWequ3t7e11lpBFFBbFZe6EdySsCZhC4YtkS2QkGQ+vz8y0DEGGGSSM5l5Px8PH5mZc07mzcF553C+35xj7o6IiCSulKADiIhI21LRi4gkOBW9iEiCU9GLiCQ4Fb2ISILrFHSAlvr37+85OTlBxxAR6VCWL1++w90zW1sWd0Wfk5NDUVFR0DFERDoUM9twuGU6dSMikuBU9CIiCU5FLyKS4FT0IiIJTkUvIpLgVPQiIglORS8ikuDibh69iEiy2bWvgb+u3MqBxhBfmJgd8++vohcRCcDOfQd4bsVWFhdX8feyHTQ0Oadm91HRi4h0ZB/WHuC5FVtYVLyF18p20BhyhvTtyr+encvUgixOHtK7Td5XRS8i0oZqag/wbOkWFhdX8Vp5NU0hJ7tfN7527nCmFgykYHBvzKxNM6joRURirHpvPc+WNp+Web2iudyHZXRj1nnDmVaQxdhBvdq83COp6EVEYmDH3nqeKWk+cn+jopqQQ27/7lxz/nCm5Ld/uUdS0YuIfELb9tTxbMkWFhdv4c11zeU+vH93rp00kqkFWYzO6hlYuUdS0YuIHINtu+t4pnQLi96v4q31NbjDiMzuXP+pkUw9OYsTT4iPco+kohcROYqtu+tYUlzF4uItLNvQXO55A3rw7QvymFqQxagTesRduUdS0YuItGLLrjoWF1expKSKog0f4g6jTujBDRfmMa0gi7wTegYdMWoqehGRsA927mdJeEB1+YYPAThpYE/+/aJRTC0YyMgBHafcI6noRSSpbd65nyXFVSwqruKdjTuB5nL/zsWjmHpyFiMyewSc8Pip6EUk6Wyq2cczJVtYVFzFu5uay31MVi9uuvREpuQPZHgClHskFb2IJIVNNftYXFzF4uIq3qvcBUD+4F78x+QTmZqfRU7/7gEnbDtRFb2ZTQZ+BqQCv3H3uw+z3hXA48Dp7l5kZjnASmB1eJU33P2a4w0tIhKNjdX7WFzSXO7vh8u9YHBvvjf5JKYWDGRYRuKWe6SjFr2ZpQL3AxcDlcAyM1vo7itarNcTuAF4s8W3KHf3cTHKKyJyRBuqa1kUPnIv2bwbgFOG9ObmKScxtSCLof26BZyw/UVzRD8BKHP3CgAzexSYAaxosd4dwI+Bm2KaUETkKNbtqGVxcRWL3q9iRVW43If2YfbUk5iSn5zlHimaoh8MbIp4XglMjFzBzMYDQ919kZm1LPpcM3sH2A3MdfdXWr6Bmc0CZgFkZ8f+WswiknjKt+8Nz5bZwspwuZ+a3Ye500YzOX8gQ/omd7lHOu7BWDNLAe4FrmplcRWQ7e7VZnYasMDMxrr77siV3P1B4EGAwsJCP95MIpKYyrbtPTSgumrLHgBOG9aXWy4bw5T8gQzq0zXghPEpmqLfDAyNeD4k/NpBPYF8YGn4V4AHAgvNbLq7FwH1AO6+3MzKgVFAUQyyi0gSWLt1z6Fz7mu27gWgcFhffnDZGKYUDCSrt8r9aKIp+mVAnpnl0lzwVwJfOLjQ3XcB/Q8+N7OlwHfDs24ygRp3bzKz4UAeUBHD/CKSYNydNVv/ceS+dttezOD0Yf344T+NYXJ+FgN7pwcds0M5atG7e6OZXQ88S/P0yofdvdTMbgeK3H3hETY/D7jdzBqAEHCNu9fEIriIJA53Z/XWPSx+v/k3VMu312IGE3L6cdv0sUzOH8gJvVTun5S5x9cp8cLCQi8q0pkdkUTn7qys2tN85F5SRcX2WlIMJuT2Y1pBFpfmD2RAT5V7tMxsubsXtrZMvxkrIu3G3Sn9YDdLSpov+btuR3O5nzE8g6vPzmXy2IFk9uwSdMyEo6IXkTZ1sNwXFVexpLiK9dX7SDE4c0QGXzs3l0vHDqR/D5V7W1LRi8gxc3fqG0PUN4Sob2yivjFEXUPz1/rGJuoaQuw/0MSyDTUsKd7Cxpp9pKYYZ43I4Bvnj+CSMSeQoXJvNyp6kQ6soSkULtwm6g5+bbV8Ix5HfK2LeN7aui3Luz5inWh0SjHOGtmfayeN4JKxA+nXvXMb7xFpjYpe5DiFQn6oDD9aqCHqGpuav7YszCOtG/5aH/m11fIN0RQ6vskU6WkpdOmUSpdOKaSntfyaQp+uaXQJr3No3SNs85HHaankZHSjTzeVe9BU9CJRWLejlh8tXkn59r3hov5HER9oiu7o9nDSUu2jRRouyUNl261ziwJNIT1cuOkRxdtaaUd+n4MlfXBZ59SUuL7PqcSOil7kCOobm/j1SxX814tldElN4dxR/cPleuyl21pRd+mUSmqKylbalope5DDerKhm9vxiyrfXMu3kLG69bAwD9Es70gGp6EVa+LD2AD9aspLHiioZ0rcr/3P16XzqxAFBxxL5xFT0ImHuzhNvb2be4pXs2t/ANeeP4IYL8+jaOTXoaCLHRUUvAlRs38vcBSW8Vl7Nqdl9uGtmAaOzegUdSyQmVPSS1Oobm/jV0nJ++WI5XdJSuPPyfL4wIZsUDZBKAlHRS9J6vbyaOQuKqdheyz+dMohbLhuti2hJQlLRS9KpqT3AXYtX8vjySob268pvrz6dSRpslQSmopek4e785e3NzFu0gj11jVw7aQTfukCDrZL4VPSSFMq372XO/GLeqKjhtGF9uWtmAScO7Bl0LJF2oaKXhFbX0DzY+qul5aSnpXDXzAKuPH2oBlslqajoJWG9Vr6DufNLqNhRy/RTBjFXg62SpFT0knBqag8wb9FK/vJ2Jdn9uvG7f53AeaMyg44lEhgVvSQMd+fPyyu5a/FK9tY1ct2nmgdb09M02CrJTUUvCaFs2x5mzy/hrXU1FA7ry12fLmDUCRpsFQEVvXRwdQ1N/PLFMn71Ujld01K5+9MFfK5Qg60ikVT00mH9vWwHcxeUsG5HLZePG8ScaWPI7Kn7kIq0pKKXDqd6bz3zFq3kiXc2MyyjG//31Qmcm6fBVpHDUdFLhxEKOX9evokfLVlFbX0j37pgJNd9aqQGW0WOQkUvHcLarXuYM7+Et9bXMCGnH/Nm5pOnwVaRqKjoJa7VNTRx/4tlPPBSOd06d+KeK07mM6cN0WCryDFQ0UvcenXtDuYuKGZ99T4+fepgZk8bTf8eGmwVOVYqeok7O/bWc+fTK1jw7gfkZHTjD1+byNkj+wcdS6TDUtFL3AiFnMeKmgdb9x1o5NsX5nHtpBEabBU5Tip6iQtrtu5h9hPFFG34kAm5/bhrZj4jB2iwVSQWVPQSqLqGJn7xwlp+/VIFPdI7cc9nTuazpw3BTIOtIrGiopfAvLxmO3MXlLCxZh9XjB/C7KknkaHBVpGYU9FLu9u+p547F63gyXc/YHj/7jzy9YmcNUKDrSJtRUUv7SYUch5dtom7l6ykriHEDRfm8U0Ntoq0ORW9tIvVW/Ywe34xyzd8yBnD+zFvZgEjMnsEHUskKajopU3tP9DEz19Yy3+/XEHP9E7852dP4YrxgzXYKtKOUqJZycwmm9lqMyszs+8fYb0rzMzNrDDitZvD2602s0tjEVo6hqWrt3HJfS/xq6XlXH7qYJ7/ziQ+oxk1Iu3uqEf0ZpYK3A9cDFQCy8xsobuvaLFeT+AG4M2I18YAVwJjgUHA38xslLs3xe6PIPFm25467nh6JU+99wHDM7vzx6+fwZkjMoKOJZK0ojl1MwEoc/cKADN7FJgBrGix3h3Aj4GbIl6bATzq7vXAOjMrC3+/1483uMSfUMh55K2N/PiZVdQ3hPj3i0ZxzaThdOmkwVaRIEVT9IOBTRHPK4GJkSuY2XhgqLsvMrObWmz7RottB7d8AzObBcwCyM7Oji65xJVVW3Yz+4li3t64k7NGZHDn5fkM12CrSFw47sFYM0sB7gWu+qTfw90fBB4EKCws9OPNJO1n/4Emfvb8Wn7zSgW9uqZx7+dOYeapGmwViSfRFP1mYGjE8yHh1w7qCeQDS8Mf7oHAQjObHsW20oG9uHobtywoofLD/XyucAg3TxlN3+6dg44lIi1EU/TLgDwzy6W5pK8EvnBwobvvAg79WqOZLQW+6+5FZrYfeMTM7qV5MDYPeCt28SUI23bXcdvTK1j0fhUjMrvz6KwzOGO4BltF4tVRi97dG83seuBZIBV42N1Lzex2oMjdFx5h21Ize4zmgdtG4DrNuOm4QiHnD29t5J4lq6hvCvGdi0cx63wNtorEO3OPr1PihYWFXlRUFHQMaWFl1W5mzy/mnY07OXtkBndeXkBu/+5BxxKRMDNb7u6FrS3Tb8bKEe070MjP/raW37y6jj5d07jvn8cxY9wgDbaKdCAqejmsF1dtY+6CEjbv3M+Vpw/l+1NOok83DbaKdDQqevmYrbvruP2pFSwqrmLkgB489o0zmZDbL+hYIvIJqejlkKaQ84c3N/CTZ1ZT3xTiu5eMYtZ5I+jcKapLIolInFLRCwClH+xi9vwS3tu0k3Pz+nPHjHxyNNgqkhBU9Elu34FG7vvbWh56dR19u6XxsyvHMf0UDbaKJBIVfRJbunobc+Y3D7Z+fsJQvjdZg60iiUhFn6RKNu/iq/9bxPD+3Xn8mjMpzNFgq0iiUtEnoaaQM2d+MX27pfH4NWfRu1ta0JFEpA1pOkUS+sObG3ivchdzp41RyYskARV9ktm6u46fPLOac0b2Z8a4QUHHEZF2oKJPMrc/vYL6phB3XJ6vmTUiSUJFn0SWrt7GoveruG7SSF2QTCSJqOiTxP4DTdzyZAnDM7tzzaThQccRkXakWTdJ4hcvrGVTzX4e+fpEXT9eJMnoiD4JrNm6hwdfruDT4wdz1oj+R99ARBKKij7BhcJz5nukd2LO1NFBxxGRAKjoE9yfl29i2foPuXnKSWT06BJ0HBEJgIo+gVXvredHS1Zxek5fPnva0KDjiEhAVPQJbN7ileyta2TezAJSUjRnXiRZqegT1GvlO3ji7c3MOm84o07oGXQcEQmQij4B1Tc2MXd+CUP7deVbF+QFHUdEAqZ59AnogaUVVOyo5bdXn07XzpozL5LsdESfYNbtqOX+pWVMOzmLSScOCDqOiMQBFX0CcXfmLiimS2oKt142Jug4IhInVPQJ5Ml3P+DvZdXcNPlEBvRKDzqOiMQJFX2C2LWvgTsXreCUIb35l4nDgo4jInFEg7EJ4u5nVlFTe4DfXj2BVM2ZF5EIOqJPAMs31PDHtzZy9dm55A/uHXQcEYkzKvoOrqEpxOwnShjUO50bLx4VdBwRiUM6ddPBPfTqOlZv3cODXzqN7l301ykiH6cj+g5sU80+7vvbGi4ecwKXjB0YdBwRiVMq+g7K3bl1YSkpZtw2fWzQcUQkjqnoO6hnSrbwwqpt3HjxKAb16Rp0HBGJYyr6DmhPXQM/fKqUMVm9uOqsnKDjiEic0+hdB/TT59awbU89v/5SIZ1S9bNaRI5MLdHBFFfu4nevr+eLE4cxbmifoOOISAcQVdGb2WQzW21mZWb2/VaWX2NmxWb2rpm9amZjwq/nmNn+8OvvmtkDsf4DJJOmkDN7fjEZPbpw0+QTg44jIh3EUU/dmFkqcD9wMVAJLDOzhe6+ImK1R9z9gfD604F7gcnhZeXuPi62sZPT715fT/HmXfzi86fSKz0t6Dgi0kFEc0Q/AShz9wp3PwA8CsyIXMHdd0c87Q547CIKwJZddfz0uTWcNyqTy07OCjqOiHQg0RT9YGBTxPPK8GsfYWbXmVk5cA/w7YhFuWb2jpm9ZGbntvYGZjbLzIrMrGj79u3HED953PZUKQ1NIe6ckY+ZLlomItGL2WCsu9/v7iOA7wFzwy9XAdnufipwI/CImfVqZdsH3b3Q3QszMzNjFSlhvLBqK0tKtvDtC/PIzugWdBwR6WCiKfrNwNCI50PCrx3Oo8DlAO5e7+7V4cfLgXJAV946BvsONHLLglLyBvTg6+cODzqOiHRA0RT9MiDPzHLNrDNwJbAwcgUzy4t4Og1YG349MzyYi5kNB/KAilgETxY/e34tm3fuZ97MAjp30mxYETl2R5114+6NZnY98CyQCjzs7qVmdjtQ5O4LgevN7CKgAfgQ+Ep48/OA282sAQgB17h7TVv8QRLRqi27eeiVdXyucAgTcvsFHUdEOihzj68JMoWFhV5UVBR0jMCFQs5nHniN9dX7eP7G8+nbvXPQkUQkjpnZcncvbG2ZzgXEqT8u28jbG3cye+polbyIHBcVfRzavqeeHy9ZxRnD+3HF+I/NZBUROSYq+jh056IV7G9o4s7LCzRnXkSOm4o+zryydjtPvvsB3zx/BCMH9Ag6jogkABV9HKlraOKWBSXkZHTj2k+NDDqOiCQIXY8+jvzyxTLWV+/j91+dSHpaatBxRCRB6Ig+TpRt28uvXipnxrhBnJPXP+g4IpJAVPRxwN2ZM7+YrmmpzJ02Jug4IpJgVPRx4C9vb+bNdTV8b8pJZPbsEnQcEUkwKvqAfVh7gLsWr2R8dh8+f3p20HFEJAGp6AP2oyUr2bW/gXkzC0hJ0Zx5EYk9FX2A3lpXw2NFlXztnFxGZ33sMv0iIjGhog/IgcYQs+cXM7hPV264KO/oG4iIfEKaRx+Q/36lgrJte3noK4V066y/BhFpOzqiD8CG6lp+/vxaJo8dyIWjTwg6jogkOBV9O3N3bnmylE4pxq3TNWdeRNqeir6dPf1+FS+v2c53Lz2RrN5dg44jIklARd+Odu1v4PanV1AwuDdfPjMn6DgikiQ0CtiO/vPZ1VTvrefhr5xOqubMi0g70RF9O3l3005+/+YGvnxmDgVDegcdR0SSiIq+HTQ2hZj9RDEDenbhO5eMCjqOiCQZnbppB799bT0rqnbzq38ZT8/0tKDjiEiS0RF9G9u8cz/3/nUNF5w0gMn5A4OOIyJJSEXfxn64sJSQO7dNH6sbfYtIIFT0bei50i38dcVW/u2iUQzt1y3oOCKSpFT0baS2vpEfLizlpIE9+eo5uUHHEZEkpqJvI//vr2v4YFcd82bmk5aq3SwiwVEDtYHSD3bxP6+t5/MTsjltWL+g44hIklPRx1hTyJk9v4S+3dL4/uSTgo4jIqKij7VH3tzAe5t2MnfaGHp305x5EQmeij6Gtu2u455nVnPOyP7MGDco6DgiIoCKPqZuf3oF9U0h7rg8X3PmRSRuqOhj5KU123n6/SqumzSS3P7dg44jInKIij4G6hqauGVBCcMzu3PNpOFBxxER+Qhd1CwGfvHCWjbW7OORr0+kS6fUoOOIiHyEjuiP09qte3jw5Qo+PX4wZ43oH3QcEZGPUdEfh1DImTO/hO5dOjFn6uig44iItEpFfxweX17JW+truHnKSWT06BJ0HBGRVkVV9GY22cxWm1mZmX2/leXXmFmxmb1rZq+a2ZiIZTeHt1ttZpfGMnyQqvfWc9eSlZye05fPnjY06DgiIod11KI3s1TgfmAKMAb4fGSRhz3i7gXuPg64B7g3vO0Y4EpgLDAZ+GX4+3V48xavZG9dI/NmFpCiG32LSByL5oh+AlDm7hXufgB4FJgRuYK774542h3w8OMZwKPuXu/u64Cy8Pfr0F4r38ETb29m1nnDGXVCz6DjiIgcUTTTKwcDmyKeVwITW65kZtcBNwKdgQsitn2jxbaDW9l2FjALIDs7O5rcgalvbGLu/BKG9uvKty7ICzqOiMhRxWww1t3vd/cRwPeAuce47YPuXujuhZmZmbGK1CYeWFpBxY5a7piRT9fOCXEWSkQSXDRFvxmIHG0cEn7tcB4FLv+E28a1dTtquX9pGdNOzmLSiQOCjiMiEpVoin4ZkGdmuWbWmebB1YWRK5hZ5DmMacDa8OOFwJVm1sXMcoE84K3jj93+3J25C4rpkprCrZe1HIsWEYlfRz1H7+6NZnY98CyQCjzs7qVmdjtQ5O4LgevN7CKgAfgQ+Ep421IzewxYATQC17l7Uxv9WdrUk+9+wN/Lqrl9xlgG9EoPOo6ISNTM3Y++VjsqLCz0oqKioGN8xK59DVx471IG9+nKE9eeTaqmU4pInDGz5e5e2NoyXdQsCnc/s4qa2gP89uoJKnkR6XB0CYSjWL6hhj++tZGrz84lf3DvoOOIiBwzFf0RNDSFmP1ECYN6p3PjxaOCjiMi8ono1M0RPPTqOlZv3cODXzqN7l20q0SkY9IR/WFsqtnHfX9bw8VjTuCSsQODjiMi8omp6Fvh7ty6sJQUM26bPjboOCIix0VF34pnSrbwwqpt3HjxKAb16Rp0HBGR46Kib2FPXQM/fKqUMVm9uOqsnKDjiIgcN40wtvDT59awbU89v/5SIZ1S9XNQRDo+NVmE4spd/O719Xxx4jDGDe0TdBwRkZhQ0Yc1hZzZ84vJ6NGFmyafGHQcEZGYUdGH/e719RRv3sUPLhtDr/S0oOOIiMSMih7YsquOnz63hvNGZXLZyVlBxxERiSkVPXDbU6U0NIW4c0Y+ZrpomYgklqQv+hdWbWVJyRa+fWEe2Rndgo4jIhJzSV30+w40csuCUvIG9ODr5w4POo6ISJtI6nn0P3t+LZt37uexb5xJ505J/TNPRBJY0rbbqi27eeiVdXyucAgTcvsFHUdEpM0kZdGHQs7sJ4rp1TWNm6eMDjqOiEibSsqif3TZJt7euJPZU0fTt3vnoOOIiLSppCv67XvquXvJSs4Y3o8rxg8OOo6ISJtLuqKft2gF+xuauPPyAs2ZF5GkkFRF/+raHSx49wO+ef4IRg7oEXQcEZF2kTRFX9fQxC1PlpCT0Y1rPzUy6DgiIu0maebR/3JpOet21PL7r04kPS016DgiIu0mKY7oy7fv5YGl5cwYN4hz8voHHUdEpF0lfNG7O3PmF5OelsLcaWOCjiMi0u4SvuifeHszb1TU8L0pJ5HZs0vQcURE2l1CF/2HtQeYt3gl47P78PnTs4OOIyISiIQu+ruXrGLX/gbmzSwgJUVz5kUkOSVs0b+1roY/FW3ia+fkMjqrV9BxREQCk5BFf6AxxJz5xQzu05UbLsoLOo6ISKASch79f79Swdpte3noK4V065yQf0QRkagl3BH9hupafv78WiaPHciFo08IOo6ISOASqujdnVueLKVTinHrdM2ZFxGBBCv6p9+v4uU12/nupSeS1btr0HFEROJCwhT9rv0N3P70CgoG9+bLZ+YEHUdEJG5EVfRmNtnMVptZmZl9v5XlN5rZCjN738yeN7NhEcuazOzd8H8LYxk+Un1jE6cO7cNdMwtI1Zx5EZFDjjolxcxSgfuBi4FKYJmZLXT3FRGrvQMUuvs+M/smcA/wz+Fl+919XIxzf8yAnuk8+OXCtn4bEZEOJ5q5hxOAMnevADCzR4EZwKGid/cXI9Z/A/hiLEOKBKWhoYHKykrq6uo+tiw9PZ0hQ4aQlpYWQDKR6EVT9IOBTRHPK4GJR1j/q8CSiOfpZlYENAJ3u/uClhuY2SxgFkB2tq5JI/GjsrKSnj17kpOT85FbT7o71dXVVFZWkpubG2BCkaOL6WCsmX0RKAR+EvHyMHcvBL4A3GdmI1pu5+4PunuhuxdmZmbGMpLIcamrqyMjI+Nj9xc2MzIyMlo90heJN9EU/WZgaMTzIeHXPsLMLgLmANPdvf7g6+6+Ofy1AlgKnHoceUXa3eFuIq+by0tHEU3RLwPyzCzXzDoDVwIfmT1jZqcCv6a55LdFvN7XzLqEH/cHzibi3L6IiLS9o56jd/dGM7seeBZIBR5291Izux0ocveFNJ+q6QH8OXyUs9HdpwOjgV+bWYjmHyp3t5itIyIibSyqK365+2JgcYvXfhDx+KLDbPcaUHA8AUWC5u6tnqZx9wDSiBy7hPnNWJG2kJ6eTnV19cdK/eCsm/T09ICSiUTP4u2oxMy2AxuO41v0B3bEKE4sKdexiYtcmZmZnebNm5eTk5PT1cwIhUIpKSkpIXdn/fr1++fMmbN++/btjUHnJE72VyuU69gcT65h7t7qtMW4K/rjZWZF4emccUW5jo1yHRvlOjbJlkunbkREEpyKXkQkwSVi0T8YdIDDUK5jo1zHRrmOTVLlSrhz9CIi8lGJeEQvIiIRVPQiIgmuQxZ9FHe86mJmfwovf9PMcuIk11Vmtj3ijltfa6dcD5vZNjMrOcxyM7Ofh3O/b2bj4yTXJDPbFbG/ftDaem2Qa6iZvRi+a1qpmd3Qyjrtvs+izNXu+8zM0s3sLTN7L5zrtlbWaffPZJS5AvlMht871czeMbOnW1kW2/3l7h3qP5qvt1MODAc6A+8BY1qscy3wQPjxlcCf4iTXVcB/BbDPzgPGAyWHWT6V5nsIGHAG8Gac5JoEPB3A/soCxocf9wTWtPJ32e77LMpc7b7PwvugR/hxGvAmcEaLdYL4TEaTK5DPZPi9bwQeae3vK9b7qyMe0R+645W7HwAO3vEq0gzgf8OPHwcutLa/pmw0uQLh7i8DNUdYZQbwO2/2BtDHzLLiIFcg3L3K3d8OP94DrKT5BjyR2n2fRZmr3YX3wd7w07Twfy1nebT7ZzLKXIEwsyHANOA3h1klpvurIxZ9a3e8avk/+6F13L0R2AVkxEEugCvC/9R/3MyGtrI8CNFmD8KZ4X96LzGzse395uF/Mp9K89FgpED32RFyQQD7LHwa4l1gG/BXdz/s/mrHz2Q0uSCYz+R9wH8AocMsj+n+6ohF35E9BeS4+8nAX/nHT2xp3ds0X7/jFOAXwMduQ9mWzKwH8Bfg39x9d3u+95EcJVcg+8zdm9x9HM03JppgZvnt8b5HE0Wudv9MmtllwDZ3X97W73VQRyz6aO54dWgdM+sE9Aaqg87l7tX+j7tv/QY4rY0zRSuqu4i1N3ffffCf3t58qew0a76BTZszszSay/QP7v5EK6sEss+OlivIfRZ+z53Ai8DkFouC+EweNVdAn8mzgelmtp7mUz639loAAAE8SURBVLwXmNnvW6wT0/3VEYv+qHe8Cj//SvjxZ4AXPDyqEWSuFudwp9N8jjUeLAS+HJ5Jcgawy92rgg5lZgMPnpc0swk0///a5uUQfs+HgJXufu9hVmv3fRZNriD2mZllmlmf8OOuwMXAqhartftnMppcQXwm3f1mdx/i7jk098QL7v7FFqvFdH9FdeOReOLR3fHqIeD/zKyM5sG+K+Mk17fNbDrQGM51VVvnAjCzP9I8G6O/mVUCt9I8MIW7P0DzTWWmAmXAPuDqOMn1GeCbZtYI7AeubIcf2NB8xPUloDh8fhdgNpAdkS2IfRZNriD2WRbwv2aWSvMPlsfc/emgP5NR5grkM9mattxfugSCiEiC64inbkRE5Bio6EVEEpyKXkQkwanoRUQSnIpeRCTBqehFRBKcil5EJMH9f76Zs5dVb3eqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCo0xF5kMMbH"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFVOZKY7tgsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Maximum number of examples to read\n",
        "MAX_EXAMPLES = 2000\n",
        "\n",
        "# Maximum length of input sequence in tokens\n",
        "INPUT_LENGTH = 25\n",
        "\n",
        "# Number of epochs to train for\n",
        "EPOCHS = 3\n",
        "\n",
        "# Optimizer learning rate\n",
        "LEARNING_RATE = 0.00002\n",
        "\n",
        "# Training batch size\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJbUTJMctmfc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "8b668442-86a2-46ad-e000-d3aa0fec081a"
      },
      "source": [
        "!pip3 install keras-bert"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/0f/cdc886c1018943ea62d3209bc964413d5aa9d0eb7e493abd8545be679294/keras-bert-0.81.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.3.1)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/22/b9/9040ec948ef895e71df6bee505a1f7e1c99ffedb409cb6eb329f04ece6e0/keras-transformer-0.33.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.4.1)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.81.0-cp36-none-any.whl size=37913 sha256=309d551fcc5180f4cc12801dfbb36ec3fdaca00c934221d5041f42da158e4ad6\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/27/da/ffc2d573aa48b87440ec4f98bc7c992e3a2d899edb2d22ef9e\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.33.0-cp36-none-any.whl size=13260 sha256=06987abd0655bfb5fdd54aa66e5a39db7aa5fb88072ae734be3560848a70ec6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/98/13/a28402939e1d48edd8704e6b02f223795af4a706815f4bf6d8\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=4ec3d58102ac4560b9380c4fee0b4bd5b4dbf75ed44bbaed9e14556a9d1e4d1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=afe7d2be652fd530f684fb731811379f31b4cfb85b3e06f009338d33c5d941b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=811b740289f11d8e5eb6ebb3c2be8a61b5138204450dd6866e334d4ae919b584\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=be67e34a08a666a70b7dca6cc7dcd3032c806c40229bb2cb6fb8dc4a6837dd60\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=c4f3aa7a062af4f6ecbd1e3904b7f86b85963711d35acf9a5d991a8cba545c31\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=b727b1d50d1b75c9d111dcd74d5bddf4e42dc5047672f3fee9a5deb69553b01f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.33.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibF6FQzCtpYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhHYRdgxtt4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e181add2-0ab7-4cd2-9284-d2a757def714"
      },
      "source": [
        "#download pretrained bert model\n",
        "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
        "!wget -nc https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-10 00:55:27--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.167.128, 2a00:1450:400c:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.167.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M  58.8MB/s    in 7.5s    \n",
            "\n",
            "2020-05-10 00:55:35 (51.3 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWf8U0cMuF5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "eea946d9-c139-4267-838a-2111531f9304"
      },
      "source": [
        "\n",
        "# Give -n argument so that existing files aren't overwritten \n",
        "!unzip -n cased_L-12_H-768_A-12.zip\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ynkApUUuH9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "bert_vocab_path = 'cased_L-12_H-768_A-12/vocab.txt'\n",
        "bert_config_path = 'cased_L-12_H-768_A-12/bert_config.json'\n",
        "bert_checkpoint_path = 'cased_L-12_H-768_A-12/bert_model.ckpt'    # suffixes not required"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEYyqSszuNd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_is_cased = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJpCRLnvuRvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#from keras_bert import Tokenizer\n",
        "\n",
        "\n",
        "#tokenizer = Tokenizer(token_dict, cased=model_is_cased)\n",
        "\n",
        "\n",
        "# Let's test that out\n",
        "#for s in ['Hello BERT!', 'Unknown: 你']:\n",
        " #   print('Original string:', s)\n",
        "  #  print('Tokenized:', tokenizer.tokenize(s))\n",
        "   # indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
        "    #print('Encoded:', indices)\n",
        "    #print('Segments:', segments)\n",
        "    #print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
        "    #print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1PJ__dUvOD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load pretrained bert model\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "\n",
        "pretrained_model = load_trained_model_from_checkpoint(\n",
        "    config_file = bert_config_path,\n",
        "    checkpoint_file = bert_checkpoint_path,\n",
        "    training = False,\n",
        "    trainable = True,\n",
        "    seq_len = INPUT_LENGTH\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0UNV9ECvknx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bd4d611-fdeb-44a4-fa3e-fa182cb28c95"
      },
      "source": [
        "\n",
        "# model.outputs is a list, here with a single item. We'll\n",
        "# add our output layer on top of that.\n",
        "bert_out = pretrained_model.outputs[0]\n",
        "\n",
        "print(bert_out)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Encoder-12-FeedForward-Norm/Identity:0\", shape=(None, 25, 768), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-GdDIltv8X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_labels = len(label_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lSYfw24vnwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import TimeDistributed, Dense\n",
        "\n",
        "\n",
        "out = TimeDistributed(Dense(num_labels, activation='softmax'))(bert_out)\n",
        "model = Model(\n",
        "    inputs=pretrained_model.inputs,\n",
        "    outputs=[out]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2shPM4ywNCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create optimizer\n",
        "\n",
        "from keras_bert import calc_train_steps, AdamWarmup\n",
        "\n",
        "\n",
        "# Calculate the number of steps for warmup\n",
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=len(sentences_train),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(\n",
        "    total_steps,\n",
        "    warmup_steps,\n",
        "    lr=LEARNING_RATE,\n",
        "    epsilon=1e-6,\n",
        "    weight_decay=0.01,\n",
        "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzM6dkHIwq_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluation metric\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "def label_categorical_accuracy(y_true, y_pred):\n",
        "    # Map one-hot targets and predictions to tag indices \n",
        "    y_true_idx = o.argmax(y_true, axis=-1)\n",
        "    y_pred_idx = o.argmax(y_pred, axis=-1)\n",
        "    # Compare targets to predicted elementwise and cast the\n",
        "    # resulting boolean values to floating point values.\n",
        "    # (K.floatx() returns current default float type.)\n",
        "    correct_preds = o.cast(o.equal(y_true_idx, y_pred_idx), K.floatx())\n",
        "    # Compare targets to the special NO_LABEL value and cast\n",
        "    # the resulting boolean values to floating point values.\n",
        "    # This gives a value of zero for NO_LABEL and one for others.\n",
        "    is_label = o.cast(o.not_equal(y_true_idx, tag_to_int[NO_LABEL]), K.floatx())\n",
        "    # Take elementwise product of the comparisons, giving values that\n",
        "    # are one if the prediction is equal to the target and the target\n",
        "    # is not the NO_LABEL value, and zero otherwise.\n",
        "    correct_label_preds = o.multiply(correct_preds, is_label)\n",
        "    # Accuracy is then the number of correct predictions for labels\n",
        "    # divided by the number of labels.\n",
        "    return o.reduce_sum(correct_label_preds)/o.reduce_sum(is_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt0YfB-6wxY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train model\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=[label_categorical_accuracy]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKA42eS5JlaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "b1b7c36f-3184-47cf-956f-5910b05ad314"
      },
      "source": [
        "np_train_texts"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['Sharon', 'Repudiates', 'the', 'Road', 'Map', '.']),\n",
              "       list(['AppleScript', 'is', 'just', 'one', 'of', 'several', 'client', 'languages', 'for', 'these', 'services', '(', 'it', 'was', \"n't\", 'even', 'the', 'first', ',', 'btw', ';', 'UserTalk', 'predates', 'it', ')', '.']),\n",
              "       list(['Your', 'qualifications', 'quote', 'end', 'quote', 'meaningless', '/.']),\n",
              "       ...,\n",
              "       list(['Possessing', 'a', 'corporate', 'farsightedness', 'and', 'determination', 'that', 'is', 'similar', 'to', 'Yoshinoya', ',', 'Ritek', 'has', 'been', 'groping', 'its', 'way', 'toward', 'international', 'affirmation', '.']),\n",
              "       list(['We', 'are', 'in', 'a', 'world', 'war', '/.']),\n",
              "       list(['The', 'success', 'of', 'Wu', 'Bai', '&', 'China', 'Blue', 'has', 'rested', 'primarily', ',', 'however', ',', 'on', 'Wu', \"'s\", 'reputation', 'as', 'a', 'singer', 'and', 'songwriter', ',', 'while', 'the', 'band', 'has', 'played', 'more', 'of', 'a', 'supporting', 'role', '.'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9FMhnpnHNLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "b3053c04-9193-4aae-9493-2a64adfd88fc"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np_train_texts=np.array(train_texts)\n",
        "tf_train_texts = tf.convert_to_tensor(np_train_texts, np.float32)\n",
        "np_train_labels=np.array(train_labels)\n",
        "tf_train_labels = tf.convert_to_tensor(np_train_labels, np.float32)\n",
        "np_dev_texts=np.array(dev_texts)\n",
        "tf_dev_texts = tf.convert_to_tensor(np_dev_texts, np.float32)\n",
        "np_dev_labels=np.array(dev_labels)\n",
        "tf_dev_labels = tf.convert_to_tensor(np_dev_labels, np.float32)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-4ad5d0247f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp_train_texts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtf_train_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_train_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnp_train_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLfJdABnw1EL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "6eaee423-753c-4ba4-c7d9-6966df2a6b33"
      },
      "source": [
        "history = model.fit(\n",
        "    np_train_texts,\n",
        "    np_train_labels,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(np_dev_texts, np_dev_labels)\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-e97e0084d0f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_dev_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_dev_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    813\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1006\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYVTubJI1nzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    train_metric = 'label_categorical_accuracy'\n",
        "    val_metric = 'val_label_categorical_accuracy'\n",
        "    plt.plot(history.history[val_metric],label=\"Validation set accuracy\")\n",
        "    plt.plot(history.history[train_metric],label=\"Training set accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99OdqAnG1wK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_tags(words):\n",
        "    # This function takes a sequence of words, vectorizes it,\n",
        "    # and returns the model predictions. The vectorization code\n",
        "    # we wrote expects labels, so we'll attach dummy labels to\n",
        "    # use the code and then discard the dummy labels it generates.\n",
        "    dummy_tagged = [(word, NO_LABEL) for word in words]\n",
        "    tokenized_test = tokenize_sentences([dummy_tagged])\n",
        "    test_X, dummy_Y = vectorize_dataset(tokenized_test)\n",
        "    # Run model.predict for this single sequence \n",
        "    predictions = model.predict(test_X)\n",
        "    # Our outputs are one-hot, take argmax to get indices\n",
        "    y = predictions[0].argmax(axis=1)\n",
        "    # For words tokenized into several subword parts, we\n",
        "    # only care about the predicted tag for the first part.\n",
        "    # Use the tokenization to identify these parts.\n",
        "    tags = []\n",
        "    i = 1    # Start at 1 to skip the special [CLS] token.\n",
        "    for tokens, _ in tokenized_test[0]:\n",
        "        tags.append(int_to_tag[y[i]])\n",
        "        i += len(tokens)\n",
        "    return tags\n",
        "\n",
        "\n",
        "# Test the model with a few word sequences\n",
        "test_words = [\n",
        "  'This model can predict parts of speech .'.split(),\n",
        "  'What would the best time be tomorrow, John wondered quietly .'.split()\n",
        "]\n",
        "\n",
        "\n",
        "#for words in test_words:\n",
        " #   tags = predict_tags(words)\n",
        "  #  for w, t in zip(words, tags):\n",
        "   #     print('{}\\t{}'.format(w, t))\n",
        "   # print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgSYNcerMI9R"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pODn-BJ_uNAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sRHpWGquHNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRDxKgLSL_uf"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlG6ZWkIL-HY"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}