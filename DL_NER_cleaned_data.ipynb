{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainvo/DeepLearning_NER/blob/master/DL_NER_cleaned_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTBsYI1tLeVk"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2GevEzfPP2",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/api/annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O5MwAmUALZ4V"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q3HiGQgMU5L",
        "outputId": "e23f34c5-d2da-4008-c8ae-d4721b264298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "# Training data: Used for training the model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
        "\n",
        "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
        "\n",
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-11 00:06:00--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17252156 (16M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "train.tsv           100%[===================>]  16.45M  33.0MB/s    in 0.5s    \n",
            "\n",
            "2020-05-11 00:06:01 (33.0 MB/s) - ‘train.tsv’ saved [17252156/17252156]\n",
            "\n",
            "--2020-05-11 00:06:02--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2419425 (2.3M) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>]   2.31M  12.6MB/s    in 0.2s    \n",
            "\n",
            "2020-05-11 00:06:02 (12.6 MB/s) - ‘dev.tsv’ saved [2419425/2419425]\n",
            "\n",
            "--2020-05-11 00:06:03--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1788466 (1.7M) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>]   1.71M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-05-11 00:06:03 (23.8 MB/s) - ‘test.tsv’ saved [1788466/1788466]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZhc7b6VFWYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a1983df-6fcf-4050-8cf2-c474915ac86f"
      },
      "source": [
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOOHEYpiMzFp",
        "outputId": "450ce777-85e8-4f66-8e04-77c964f43959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
        "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
        "\n",
        "def read_ontonotes(tsv_file): # \n",
        "    current_sent = [] # list of (word,label) lists\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
        "        for line in tsvreader:\n",
        "            if not line:\n",
        "                if current_sent:\n",
        "                    yield current_sent\n",
        "                    current_sent=[]\n",
        "                continue\n",
        "            current_sent.append(line) \n",
        "        else:\n",
        "            if current_sent:\n",
        "                yield current_sent\n",
        "\n",
        "full_train_data = list(read_ontonotes('train.tsv'))\n",
        "train_data = full_train_data[:30000]\n",
        "print(train_data[:5])\n",
        "print()\n",
        "dev_data = list(read_ontonotes('dev.tsv'))\n",
        "print(dev_data[:5])\n",
        "print()\n",
        "test_data = list(read_ontonotes('test.tsv'))\n",
        "print(test_data[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['Big\\tO'], ['Managers\\tO'], ['on\\tO'], ['Campus\\tO']], [['In\\tO'], ['recent\\tB-DATE'], ['years\\tI-DATE'], [',\\tO'], ['advanced\\tO'], ['education\\tO'], ['for\\tO'], ['professionals\\tO'], ['has\\tO'], ['become\\tO'], ['a\\tO'], ['hot\\tO'], ['topic\\tO'], ['in\\tO'], ['the\\tO'], ['business\\tO'], ['community\\tO'], ['.\\tO']], [['With\\tO'], ['this\\tO'], ['trend\\tO'], [',\\tO'], ['suddenly\\tO'], ['the\\tO'], ['mature\\tO'], ['faces\\tO'], ['of\\tO'], ['managers\\tO'], ['boasting\\tO'], ['an\\tO'], ['average\\tO'], ['of\\tO'], ['over\\tO'], ['ten\\tB-DATE'], ['years\\tI-DATE'], ['of\\tO'], ['professional\\tO'], ['experience\\tO'], ['have\\tO'], ['flooded\\tO'], ['in\\tO'], ['among\\tO'], ['the\\tO'], ['young\\tO'], ['people\\tO'], ['populating\\tO'], ['university\\tO'], ['campuses\\tO'], ['.\\tO']], [['In\\tO'], ['order\\tO'], ['to\\tO'], ['attract\\tO'], ['this\\tO'], ['group\\tO'], ['of\\tO'], ['seasoned\\tO'], ['adults\\tO'], ['pulling\\tO'], ['in\\tO'], ['over\\tO'], ['NT$\\tB-MONEY'], ['1\\tI-MONEY'], ['million\\tI-MONEY'], ['a\\tO'], ['year\\tO'], ['back\\tO'], ['to\\tO'], ['the\\tO'], ['ivory\\tO'], ['tower\\tO'], [',\\tO'], ['universities\\tO'], ['have\\tO'], ['begun\\tO'], ['to\\tO'], ['establish\\tO'], ['executive\\tO'], ['MBA\\tB-WORK_OF_ART'], ['(\\tO'], ['EMBA\\tB-WORK_OF_ART'], [')\\tO'], ['programs\\tO'], ['.\\tO']], [['In\\tO'], ['response\\tO'], [',\\tO'], ['each\\tO'], ['year\\tO'], ['over\\tO'], ['1000\\tB-CARDINAL'], ['mature\\tO'], ['professionals\\tO'], ['looking\\tO'], ['to\\tO'], ['recharge\\tO'], ['their\\tO'], ['minds\\tO'], ['and\\tO'], ['retool\\tO'], ['their\\tO'], ['know\\tO'], ['-\\tO'], ['how\\tO'], ['compete\\tO'], ['for\\tO'], ['a\\tO'], ['precious\\tO'], ['few\\tO'], ['openings\\tO'], ['in\\tO'], ['executive\\tO'], ['degree\\tO'], ['programs\\tO'], ['at\\tO'], ['top\\tO'], ['institutions\\tO'], ['such\\tO'], ['as\\tO'], ['National\\tB-ORG'], ['Taiwan\\tI-ORG'], ['University\\tI-ORG'], ['(\\tO'], ['NTU\\tB-ORG'], [')\\tO'], ['and\\tO'], ['National\\tB-ORG'], ['Chengchi\\tI-ORG'], ['University\\tI-ORG'], ['.\\tO']]]\n",
            "\n",
            "[[['President\\tB-WORK_OF_ART'], ['Chen\\tI-WORK_OF_ART'], ['Travels\\tI-WORK_OF_ART'], ['Abroad\\tI-WORK_OF_ART']], [['(\\tO'], ['Chang\\tB-PERSON'], ['Chiung\\tI-PERSON'], ['-\\tI-PERSON'], ['fang\\tI-PERSON'], ['/\\tO'], ['tr.\\tO'], ['by\\tO'], ['David\\tB-PERSON'], ['Mayer\\tI-PERSON'], [')\\tO']], [['President\\tO'], ['Chen\\tB-PERSON'], ['Shui\\tI-PERSON'], ['-\\tI-PERSON'], ['bian\\tI-PERSON'], ['visited\\tO'], ['the\\tB-FAC'], ['Nicaraguan\\tI-FAC'], ['National\\tI-FAC'], ['Assembly\\tI-FAC'], ['on\\tO'], ['August\\tB-DATE'], ['17\\tI-DATE'], [',\\tO'], ['where\\tO'], ['he\\tO'], ['received\\tO'], ['a\\tO'], ['medal\\tO'], ['from\\tO'], ['the\\tO'], ['president\\tO'], ['of\\tO'], ['the\\tO'], ['assembly\\tO'], [',\\tO'], ['Ivan\\tB-PERSON'], ['Escobar\\tI-PERSON'], ['Fornos\\tI-PERSON'], ['.\\tO']], [['(\\tO'], ['photo\\tO'], ['by\\tO'], ['Wu\\tB-PERSON'], ['Chi\\tI-PERSON'], ['-\\tI-PERSON'], ['chang\\tI-PERSON'], [',\\tO'], ['Central\\tB-ORG'], ['News\\tI-ORG'], ['Agency\\tI-ORG'], [')\\tO']], [['On\\tO'], ['August\\tB-DATE'], ['25\\tI-DATE'], ['President\\tO'], ['Chen\\tB-PERSON'], ['Shui\\tI-PERSON'], ['-\\tI-PERSON'], ['bian\\tI-PERSON'], ['wrapped\\tO'], ['up\\tO'], ['his\\tO'], ['first\\tB-ORDINAL'], ['overseas\\tO'], ['trip\\tO'], ['since\\tO'], ['taking\\tO'], ['office\\tO'], [',\\tO'], ['swinging\\tO'], ['through\\tO'], ['three\\tB-CARDINAL'], ['countries\\tO'], ['in\\tO'], ['Latin\\tB-LOC'], ['America\\tI-LOC'], ['and\\tO'], ['another\\tO'], ['three\\tB-CARDINAL'], ['in\\tO'], ['Africa\\tB-LOC'], ['.\\tO']]]\n",
            "\n",
            "[[['Powerful\\tB-WORK_OF_ART'], ['Tools\\tI-WORK_OF_ART'], ['for\\tI-WORK_OF_ART'], ['Biotechnology\\tI-WORK_OF_ART'], ['-\\tI-WORK_OF_ART'], ['Biochips\\tI-WORK_OF_ART']], [['(\\tO'], ['Chang\\tB-PERSON'], ['Chiung\\tI-PERSON'], ['-\\tI-PERSON'], ['fang\\tI-PERSON'], ['/\\tO'], ['photos\\tO'], ['by\\tO'], ['Hsueh\\tB-PERSON'], ['Chi\\tI-PERSON'], ['-\\tI-PERSON'], ['kuang\\tI-PERSON'], ['/\\tO'], ['tr.\\tO'], ['by\\tO'], ['Robert\\tB-PERSON'], ['Taylor\\tI-PERSON'], [')\\tO']], [['The\\tO'], ['enterovirus\\tO'], ['detection\\tO'], ['biochip\\tO'], ['developed\\tO'], ['by\\tO'], ['DR.\\tB-ORG'], ['Chip\\tI-ORG'], ['Biotechnology\\tI-ORG'], ['takes\\tO'], ['only\\tB-TIME'], ['six\\tI-TIME'], ['hours\\tI-TIME'], ['to\\tO'], ['give\\tO'], ['hospitals\\tO'], ['the\\tO'], ['answer\\tO'], ['to\\tO'], ['whether\\tO'], ['a\\tO'], ['sample\\tO'], ['contains\\tO'], ['enterovirus\\tO'], [',\\tO'], ['and\\tO'], ['if\\tO'], ['it\\tO'], ['is\\tO'], ['the\\tO'], ['deadly\\tO'], ['strain\\tO'], ['Entero\\tO'], ['71\\tO'], ['.\\tO']], [['Worldwide\\tO'], [',\\tO'], ['biotechnology\\tO'], ['is\\tO'], ['a\\tO'], ['rising\\tO'], ['star\\tO'], ['of\\tO'], ['the\\tO'], ['industrial\\tO'], ['stage\\tO'], ['.\\tO']], [['In\\tO'], ['Taiwan\\tB-GPE'], [',\\tO'], ['in\\tO'], ['recent\\tO'], ['years\\tO'], ['the\\tB-ORG'], ['Ministry\\tI-ORG'], ['of\\tI-ORG'], ['Economic\\tI-ORG'], ['Affairs\\tI-ORG'], [',\\tO'], ['the\\tB-ORG'], ['National\\tI-ORG'], ['Science\\tI-ORG'], ['Council\\tI-ORG'], ['and\\tO'], ['the\\tB-ORG'], ['National\\tI-ORG'], ['Health\\tI-ORG'], ['Research\\tI-ORG'], ['Institutes\\tI-ORG'], ['have\\tO'], ['been\\tO'], ['strongly\\tO'], ['pursuing\\tO'], ['\\tO\\nbiochip\\tO\\n\\tO'], ['research\\tO'], ['programs\\tO'], ['.\\tO']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q32R9o_mJZAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "82426a9e-2f4d-402e-d17e-47408443ccb2"
      },
      "source": [
        "import re\n",
        "#regex for empty space chars, \\t \\n\n",
        "tab = re.compile('[\\t]')\n",
        "\n",
        "def clean(list):\n",
        "    clean_data =[]\n",
        "    for sent in list:\n",
        "        clean_list = []\n",
        "        for item in sent:\n",
        "            str = ''.join(item)\n",
        "            #match_nl = re.match(r\"\\n\", str)\n",
        "            #print(match_nl)\n",
        "            count_tab =  re.findall(r\"\\t\", str)\n",
        "            #print(count_tab)\n",
        "            if len(count_tab) == 1: \n",
        "                item = re.split(\"\\t\", str)\n",
        "                if item[0] != '.':\n",
        "                    clean_list.append(item)\n",
        "            elif len(count_tab) > 1:\n",
        "                item = re.split(\"\\n\", str)\n",
        "                #print(item)\n",
        "                for i in range(len(item)):\n",
        "                    #print(item[i])\n",
        "                    if i == 0 or i == len(item)-1:\n",
        "                        item[i] = '\"'+item[i]\n",
        "                        item[i] = re.split(\"\\t\", item[i])\n",
        "                        #print(item[i])\n",
        "                    else:\n",
        "                        item[i] = re.split(\"\\t\", item[i])\n",
        "                        #print(item[i])\n",
        "                    clean_list.append(item[i])\n",
        "        clean_data.append(clean_list)        \n",
        "    return clean_data\n",
        "\n",
        "train_data_clean = clean(train_data)\n",
        "for item in train_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')\n",
        "dev_data_clean = clean(dev_data)\n",
        "for item in dev_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')\n",
        "test_data_clean = clean(test_data)\n",
        "for item in test_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')          "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Big', 'O'], ['Managers', 'O'], ['on', 'O'], ['Campus', 'O']]\n",
            "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O']]\n",
            "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O']]\n",
            "------------------------------------------\n",
            "[['President', 'B-WORK_OF_ART'], ['Chen', 'I-WORK_OF_ART'], ['Travels', 'I-WORK_OF_ART'], ['Abroad', 'I-WORK_OF_ART']]\n",
            "[['(', 'O'], ['Chang', 'B-PERSON'], ['Chiung', 'I-PERSON'], ['-', 'I-PERSON'], ['fang', 'I-PERSON'], ['/', 'O'], ['tr.', 'O'], ['by', 'O'], ['David', 'B-PERSON'], ['Mayer', 'I-PERSON'], [')', 'O']]\n",
            "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON']]\n",
            "------------------------------------------\n",
            "[['Powerful', 'B-WORK_OF_ART'], ['Tools', 'I-WORK_OF_ART'], ['for', 'I-WORK_OF_ART'], ['Biotechnology', 'I-WORK_OF_ART'], ['-', 'I-WORK_OF_ART'], ['Biochips', 'I-WORK_OF_ART']]\n",
            "[['(', 'O'], ['Chang', 'B-PERSON'], ['Chiung', 'I-PERSON'], ['-', 'I-PERSON'], ['fang', 'I-PERSON'], ['/', 'O'], ['photos', 'O'], ['by', 'O'], ['Hsueh', 'B-PERSON'], ['Chi', 'I-PERSON'], ['-', 'I-PERSON'], ['kuang', 'I-PERSON'], ['/', 'O'], ['tr.', 'O'], ['by', 'O'], ['Robert', 'B-PERSON'], ['Taylor', 'I-PERSON'], [')', 'O']]\n",
            "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O']]\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZHhXzVTQA_P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "16a1b380-855d-40a4-8b5d-268244411a06"
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for item in f: # list of lists (tokens)\n",
        "        #print(item)\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for token in item:\n",
        "            if len(token) ==2:\n",
        "                sent_text.append(token[0])\n",
        "                sent_tags.append(token[1])\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict['text'])\n",
        "        #print(sent_dict['tags'])\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))\n",
        "samp = train_data_sent[:3]\n",
        "print(samp)\n",
        "print()\n",
        "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
        "samp2 = dev_data_sent[:3]\n",
        "print(samp2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': ['Big', 'Managers', 'on', 'Campus'], 'tags': ['O', 'O', 'O', 'O']}, {'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "\n",
            "[{'text': ['President', 'Chen', 'Travels', 'Abroad'], 'tags': ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']}, {'text': ['(', 'Chang', 'Chiung', '-', 'fang', '/', 'tr.', 'by', 'David', 'Mayer', ')'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O']}, {'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpYBQMbcQGBi",
        "outputId": "decbeec7-6e6f-4ff2-e21a-d8e254d4b289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "random.seed(123)\n",
        "random.shuffle(train_data_sent)\n",
        "print(type(train_data_sent))\n",
        "print(type(train_data_sent[0]))\n",
        "\n",
        "train_texts=[i[\"text\"] for i in train_data_sent]\n",
        "train_labels=[i[\"tags\"] for i in train_data_sent]\n",
        "\n",
        "#print(type(train_texts))\n",
        "#print(type(train_texts[0]))\n",
        "\n",
        "print('Text: ', train_texts[:4])\n",
        "print('Labels: ', train_labels[:4])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "Text:  [['Sharon', 'Repudiates', 'the', 'Road', 'Map'], ['AppleScript', 'is', 'just', 'one', 'of', 'several', 'client', 'languages', 'for', 'these', 'services', '(', 'it', 'was', \"n't\", 'even', 'the', 'first', ',', 'btw', ';', 'UserTalk', 'predates', 'it', ')'], ['Your', 'qualifications', 'quote', 'end', 'quote', 'meaningless', '/.'], ['[dasanicool]', 'Young', 'people', 'like', 'to', 'race', 'high', '-', 'power', 'motorcycles', 'late', 'at', 'night', 'and', 'revel', 'in', 'bars', 'well', 'after', 'dark']]\n",
            "Labels:  [['O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQQRw0YO-Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## same for validation/dev data\n",
        "dev_texts=[i[\"text\"] for i in dev_data_sent]\n",
        "dev_labels=[i[\"tags\"] for i in dev_data_sent]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNKOr6YY28eC",
        "colab_type": "text"
      },
      "source": [
        "Let's add POS tags to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlIltk1uXIXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT RUN\n",
        "#import nltk\n",
        "#nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "#def sent_parser4pos(texts, labels):\n",
        "  #  for i in range(len(texts) -1):\n",
        "#        #print(texts[i])\n",
        "#        tokens_with_pos = nltk.pos_tag(texts[i])\n",
        "#        #getting the corresponding labels list\n",
        " #       labels_curr= labels[i]\n",
        "  #      print()\n",
        " #       print()\n",
        "        #print('current labels:',labels_curr)\n",
        "       #updating labels with pos label\n",
        "        #print('possed tokens:', tokens_with_pos)\n",
        "  #      for j in range(len(labels_curr) -1):\n",
        "   #         bio = labels_curr[j]\n",
        "  #          pos = tokens_with_pos[j][1]\n",
        "            #print('bio: ',bio)\n",
        "            #print('pos: ', pos)\n",
        "            #print('current labels: ',labels[i])\n",
        "   #         new_label = pos+' '+bio ## new kind of combination label\n",
        "   #         labels[i][j] = new_label\n",
        "        #print('No of Bio Labels: ',len(labels_curr), ' No of pos tags: ', len(tokens_with_pos), ' No of updated labels: ', len(labels[i]), labels[i])\n",
        "  #  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCqK8iji9Nur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT RUN\n",
        "#train_labels_posbio = sent_parser4pos(train_texts, train_labels)\n",
        "#print(train_texts[0])\n",
        "#print(train_labels[0]\n",
        "#print(train_labels_posbio[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICn08fOgbyXl",
        "colab_type": "code",
        "outputId": "020a5fe5-532a-46f7-962c-f45e9caf3d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘wiki-news-300d-1M.vec.zip’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFv2qclTbyXp",
        "colab_type": "code",
        "outputId": "430ee760-c141-484c-c38c-3a35d17b0675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_C5ii8byXt",
        "colab_type": "code",
        "outputId": "08cbf714-5a80-4314-e87d-761c8e2a8093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdgjgOlbyXx",
        "colab_type": "code",
        "outputId": "8fcf107f-a0f4-4102-aef5-bd38eeb93f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGaojUBhbyX2",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Ox5_ObyX3",
        "colab_type": "code",
        "outputId": "ec693832-1a45-4ed3-abdf-58164414db75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#Labels\n",
        "from pprint import pprint\n",
        "\n",
        "not_letter = re.compile(r'[^a-zA-Z]')\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in train_labels: #loops over sentences \n",
        "    #print(sentence_labels)\n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "       # match = not_letter.match(label)\n",
        "        #if match or label== 'O':\n",
        "        #    break\n",
        "        #else:    \n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 36,\n",
            " 'B-DATE': 14,\n",
            " 'B-EVENT': 2,\n",
            " 'B-FAC': 6,\n",
            " 'B-GPE': 10,\n",
            " 'B-LANGUAGE': 5,\n",
            " 'B-LAW': 15,\n",
            " 'B-LOC': 34,\n",
            " 'B-MONEY': 29,\n",
            " 'B-NORP': 18,\n",
            " 'B-ORDINAL': 13,\n",
            " 'B-ORG': 3,\n",
            " 'B-PERCENT': 8,\n",
            " 'B-PERSON': 9,\n",
            " 'B-PRODUCT': 27,\n",
            " 'B-QUANTITY': 16,\n",
            " 'B-TIME': 25,\n",
            " 'B-WORK_OF_ART': 0,\n",
            " 'I-CARDINAL': 11,\n",
            " 'I-DATE': 33,\n",
            " 'I-EVENT': 23,\n",
            " 'I-FAC': 21,\n",
            " 'I-GPE': 31,\n",
            " 'I-LANGUAGE': 28,\n",
            " 'I-LAW': 12,\n",
            " 'I-LOC': 7,\n",
            " 'I-MONEY': 26,\n",
            " 'I-NORP': 20,\n",
            " 'I-ORDINAL': 19,\n",
            " 'I-ORG': 1,\n",
            " 'I-PERCENT': 30,\n",
            " 'I-PERSON': 4,\n",
            " 'I-PRODUCT': 24,\n",
            " 'I-QUANTITY': 22,\n",
            " 'I-TIME': 17,\n",
            " 'I-WORK_OF_ART': 35,\n",
            " 'O': 32}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k8DshceEaI",
        "colab_type": "code",
        "outputId": "b1daddf0-1fab-4c00-db23-5e8566f93d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in train_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            if token in label_map:\n",
        "                vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
        "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "pprint(vectorized_labels[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[32, 32, 32, 32, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtqLdCMPf3X",
        "colab_type": "code",
        "outputId": "de3c24aa-27e1-4ab8-d18c-1683dedc437e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## vectorization of the texts\n",
        "def text_vectorizer(vocab, train_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(train_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
        "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "\n",
        "pprint(train_texts[0])\n",
        "pprint(vectorized_data[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sharon', 'Repudiates', 'the', 'Road', 'Map']\n",
            "[8346, 1, 3, 1685, 8936]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6FH5F1QGrq",
        "colab_type": "code",
        "outputId": "916f95ba-4192-45b0-d82f-8889c97b370b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max(lengths))\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max(lengths)), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])\n",
        "\n",
        "# Same stuff for the validation data\n",
        "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max(lengths))\n",
        "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max(lengths)), -1)\n",
        "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
        "validation_weights[validation_weights > 0] = 1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Old shape: (30000,)\n",
            "New shape: (30000, 17040)\n",
            "First example:\n",
            "[   0    0    0 ...    3 1685 8936]\n",
            "Padded labels shape: (30000, 17040, 1)\n",
            "{'B-CARDINAL': 36,\n",
            " 'B-DATE': 14,\n",
            " 'B-EVENT': 2,\n",
            " 'B-FAC': 6,\n",
            " 'B-GPE': 10,\n",
            " 'B-LANGUAGE': 5,\n",
            " 'B-LAW': 15,\n",
            " 'B-LOC': 34,\n",
            " 'B-MONEY': 29,\n",
            " 'B-NORP': 18,\n",
            " 'B-ORDINAL': 13,\n",
            " 'B-ORG': 3,\n",
            " 'B-PERCENT': 8,\n",
            " 'B-PERSON': 9,\n",
            " 'B-PRODUCT': 27,\n",
            " 'B-QUANTITY': 16,\n",
            " 'B-TIME': 25,\n",
            " 'B-WORK_OF_ART': 0,\n",
            " 'I-CARDINAL': 11,\n",
            " 'I-DATE': 33,\n",
            " 'I-EVENT': 23,\n",
            " 'I-FAC': 21,\n",
            " 'I-GPE': 31,\n",
            " 'I-LANGUAGE': 28,\n",
            " 'I-LAW': 12,\n",
            " 'I-LOC': 7,\n",
            " 'I-MONEY': 26,\n",
            " 'I-NORP': 20,\n",
            " 'I-ORDINAL': 19,\n",
            " 'I-ORG': 1,\n",
            " 'I-PERCENT': 30,\n",
            " 'I-PERSON': 4,\n",
            " 'I-PRODUCT': 24,\n",
            " 'I-QUANTITY': 22,\n",
            " 'I-TIME': 17,\n",
            " 'I-WORK_OF_ART': 35,\n",
            " 'O': 32}\n",
            "First example labels:\n",
            "array([[ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       ...,\n",
            "       [32],\n",
            "       [32],\n",
            "       [32]], dtype=int32)\n",
            "First weight vector:\n",
            "[0 0 0 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAOVAbBTRAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function\n",
        "import keras\n",
        "\n",
        "def _convert_to_entities(input_sequence):\n",
        "    \"\"\"\n",
        "    Reads a sequence of tags and converts them into a set of entities.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    previous_tag = label_map['O']\n",
        "    for i, tag in enumerate(input_sequence):\n",
        "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "            current_entity.append((tag, i))\n",
        "        elif tag == label_map['O']: # Entity has ended\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "        elif tag == previous_tag: # Current entity continues\n",
        "            current_entity.append((tag, i))\n",
        "        previous_tag = tag\n",
        "    \n",
        "    # Add the last entity to our entity list if the sentences ends with an entity\n",
        "    if len(current_entity) > 0:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    entity_offsets = set()\n",
        "    \n",
        "    for e in entities:\n",
        "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
        "    return entity_offsets\n",
        "\n",
        "def _entity_level_PRF(predictions, gold, lengths):\n",
        "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
        "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
        "    \n",
        "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
        "    pred_count = sum([len(e) for e in pred_entities])\n",
        "    \n",
        "    try:\n",
        "        precision = tp / pred_count # tp / (tp+np)\n",
        "        recall = tp / sum([len(e) for e in gold_entities])\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    except Exception as e:\n",
        "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
        "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
        "    return precision, recall, fscore             \n",
        "\n",
        "def evaluate(predictions, gold, lengths):\n",
        "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
        "    return precision, recall, fscore\n",
        "\n",
        "class EvaluateEntities(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.precision = []\n",
        "        self.recall = []\n",
        "        self.fscore = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
        "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
        "        self.precision.append(evaluation_parameters[0])\n",
        "        self.recall.append(evaluation_parameters[1])\n",
        "        self.fscore.append(evaluation_parameters[2])\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ydCexfTg5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oUO3GLfTrl3",
        "colab_type": "code",
        "outputId": "dbab9f8c-d6fb-4767-fb9c-9e3be624c945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 17040)             0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 17040, 300)        15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 17040, 100)        30100     \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 17040, 37)         3737      \n",
            "=================================================================\n",
            "Total params: 15,034,437\n",
            "Trainable params: 33,837\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIPQrLXUVrWr",
        "colab_type": "code",
        "outputId": "190533d4-ad72-498d-9c54-5315a028dc56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# train the model\n",
        "optimizer=Adam(lr=0.001) # define the learning rate\n",
        "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=5, callbacks=[evaluation_function])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c01470afcd57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# define the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'temporal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevaluation_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEvaluateEntities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bah0URSVV5GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    print(\"History:\", fscores)\n",
        "    print(\"Highest f-score:\", max(fscores))\n",
        "    plt.plot(fscores)\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwMwwo9-MPv4"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9pXRewbyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#expanding to RNN model with context\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "rnn_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
        "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
        "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP-kwoNXUMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIal9meVXnN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer=Adam(lr=0.01) # define the learning rate\n",
        "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKNs4t8X3Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCo0xF5kMMbH"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgSYNcerMI9R"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRDxKgLSL_uf"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlG6ZWkIL-HY"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}