{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTBsYI1tLeVk"
   },
   "source": [
    "# Deep Learning NER task\n",
    "\n",
    "Tatjana Cucic and Sanna Volanen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9T2GevEzfPP2"
   },
   "source": [
    "https://spacy.io/api/annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5MwAmUALZ4V"
   },
   "source": [
    "# Milestones\n",
    "\n",
    "## 1.1 Predicting word labels independently\n",
    "\n",
    "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
    "* Evaluate the results on token level and entity level. \n",
    "* Report your results with different network hyperparameters. \n",
    "* Also discuss whether the token level accuracy is a reasonable metric.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "0Q3HiGQgMU5L",
    "outputId": "f8887991-249d-43d4-e1e2-82d06fa88e7f"
   },
   "outputs": [],
   "source": [
    "# Training data: Used for training the model\n",
    "#!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
    "\n",
    "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
    "#!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
    "\n",
    "# Test data: Never touched during training / model development, used for evaluating the final model\n",
    "#!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FZhc7b6VFWYX",
    "outputId": "e3e21561-95bb-48a0-e2ee-d1c76a3c381e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import csv\n",
    "# max_int = sys.maxsize\n",
    "# while True:\n",
    "#     try:\n",
    "#         csv.field_size_limit(sys.max_int)\n",
    "#         break\n",
    "#     except:\n",
    "#         max_int = int(max_int/10)\n",
    "csv.field_size_limit(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Tanja\\\\Desktop\\\\NER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "zOOHEYpiMzFp",
    "outputId": "8ecd59f2-7afa-4164-9701-b2afb0b8fbb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33409\n",
      "[[['Big\\tO'], ['Managers\\tO'], ['on\\tO'], ['Campus\\tO']], [['In\\tO'], ['recent\\tB-DATE'], ['years\\tI-DATE'], [',\\tO'], ['advanced\\tO'], ['education\\tO'], ['for\\tO'], ['professionals\\tO'], ['has\\tO'], ['become\\tO'], ['a\\tO'], ['hot\\tO'], ['topic\\tO'], ['in\\tO'], ['the\\tO'], ['business\\tO'], ['community\\tO'], ['.\\tO']], [['With\\tO'], ['this\\tO'], ['trend\\tO'], [',\\tO'], ['suddenly\\tO'], ['the\\tO'], ['mature\\tO'], ['faces\\tO'], ['of\\tO'], ['managers\\tO'], ['boasting\\tO'], ['an\\tO'], ['average\\tO'], ['of\\tO'], ['over\\tO'], ['ten\\tB-DATE'], ['years\\tI-DATE'], ['of\\tO'], ['professional\\tO'], ['experience\\tO'], ['have\\tO'], ['flooded\\tO'], ['in\\tO'], ['among\\tO'], ['the\\tO'], ['young\\tO'], ['people\\tO'], ['populating\\tO'], ['university\\tO'], ['campuses\\tO'], ['.\\tO']], [['In\\tO'], ['order\\tO'], ['to\\tO'], ['attract\\tO'], ['this\\tO'], ['group\\tO'], ['of\\tO'], ['seasoned\\tO'], ['adults\\tO'], ['pulling\\tO'], ['in\\tO'], ['over\\tO'], ['NT$\\tB-MONEY'], ['1\\tI-MONEY'], ['million\\tI-MONEY'], ['a\\tO'], ['year\\tO'], ['back\\tO'], ['to\\tO'], ['the\\tO'], ['ivory\\tO'], ['tower\\tO'], [',\\tO'], ['universities\\tO'], ['have\\tO'], ['begun\\tO'], ['to\\tO'], ['establish\\tO'], ['executive\\tO'], ['MBA\\tB-WORK_OF_ART'], ['(\\tO'], ['EMBA\\tB-WORK_OF_ART'], [')\\tO'], ['programs\\tO'], ['.\\tO']], [['In\\tO'], ['response\\tO'], [',\\tO'], ['each\\tO'], ['year\\tO'], ['over\\tO'], ['1000\\tB-CARDINAL'], ['mature\\tO'], ['professionals\\tO'], ['looking\\tO'], ['to\\tO'], ['recharge\\tO'], ['their\\tO'], ['minds\\tO'], ['and\\tO'], ['retool\\tO'], ['their\\tO'], ['know\\tO'], ['-\\tO'], ['how\\tO'], ['compete\\tO'], ['for\\tO'], ['a\\tO'], ['precious\\tO'], ['few\\tO'], ['openings\\tO'], ['in\\tO'], ['executive\\tO'], ['degree\\tO'], ['programs\\tO'], ['at\\tO'], ['top\\tO'], ['institutions\\tO'], ['such\\tO'], ['as\\tO'], ['National\\tB-ORG'], ['Taiwan\\tI-ORG'], ['University\\tI-ORG'], ['(\\tO'], ['NTU\\tB-ORG'], [')\\tO'], ['and\\tO'], ['National\\tB-ORG'], ['Chengchi\\tI-ORG'], ['University\\tI-ORG'], ['.\\tO']]]\n",
      "\n",
      "5806\n",
      "[[['President\\tB-WORK_OF_ART'], ['Chen\\tI-WORK_OF_ART'], ['Travels\\tI-WORK_OF_ART'], ['Abroad\\tI-WORK_OF_ART']], [['(\\tO'], ['Chang\\tB-PERSON'], ['Chiung\\tI-PERSON'], ['-\\tI-PERSON'], ['fang\\tI-PERSON'], ['/\\tO'], ['tr.\\tO'], ['by\\tO'], ['David\\tB-PERSON'], ['Mayer\\tI-PERSON'], [')\\tO']], [['President\\tO'], ['Chen\\tB-PERSON'], ['Shui\\tI-PERSON'], ['-\\tI-PERSON'], ['bian\\tI-PERSON'], ['visited\\tO'], ['the\\tB-FAC'], ['Nicaraguan\\tI-FAC'], ['National\\tI-FAC'], ['Assembly\\tI-FAC'], ['on\\tO'], ['August\\tB-DATE'], ['17\\tI-DATE'], [',\\tO'], ['where\\tO'], ['he\\tO'], ['received\\tO'], ['a\\tO'], ['medal\\tO'], ['from\\tO'], ['the\\tO'], ['president\\tO'], ['of\\tO'], ['the\\tO'], ['assembly\\tO'], [',\\tO'], ['Ivan\\tB-PERSON'], ['Escobar\\tI-PERSON'], ['Fornos\\tI-PERSON'], ['.\\tO']], [['(\\tO'], ['photo\\tO'], ['by\\tO'], ['Wu\\tB-PERSON'], ['Chi\\tI-PERSON'], ['-\\tI-PERSON'], ['chang\\tI-PERSON'], [',\\tO'], ['Central\\tB-ORG'], ['News\\tI-ORG'], ['Agency\\tI-ORG'], [')\\tO']], [['On\\tO'], ['August\\tB-DATE'], ['25\\tI-DATE'], ['President\\tO'], ['Chen\\tB-PERSON'], ['Shui\\tI-PERSON'], ['-\\tI-PERSON'], ['bian\\tI-PERSON'], ['wrapped\\tO'], ['up\\tO'], ['his\\tO'], ['first\\tB-ORDINAL'], ['overseas\\tO'], ['trip\\tO'], ['since\\tO'], ['taking\\tO'], ['office\\tO'], [',\\tO'], ['swinging\\tO'], ['through\\tO'], ['three\\tB-CARDINAL'], ['countries\\tO'], ['in\\tO'], ['Latin\\tB-LOC'], ['America\\tI-LOC'], ['and\\tO'], ['another\\tO'], ['three\\tB-CARDINAL'], ['in\\tO'], ['Africa\\tB-LOC'], ['.\\tO']]]\n",
      "\n",
      "4875\n"
     ]
    }
   ],
   "source": [
    "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
    "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
    "\n",
    "def read_ontonotes(tsv_file): # \n",
    "    current_sent = [] # list of (word,label) lists\n",
    "    #with open(tsv_file) as f:\n",
    "    with open(tsv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
    "        for line in tsvreader:\n",
    "            #print(line)\n",
    "            if not line:\n",
    "                if current_sent:\n",
    "                    yield current_sent\n",
    "                    current_sent=[]\n",
    "                continue\n",
    "            current_sent.append(line) \n",
    "        else:\n",
    "            if current_sent:\n",
    "                yield current_sent\n",
    "\n",
    "full_train_data = list(read_ontonotes('train.tsv'))\n",
    "size_tr = int(len(full_train_data)/2)\n",
    "print(size_tr)\n",
    "##slice train\n",
    "train_data_sample = full_train_data[:size_tr]\n",
    "print(train_data_sample[:5])\n",
    "print()\n",
    "full_dev_data = list(read_ontonotes('dev.tsv'))\n",
    "size_dv = int(len(full_dev_data)/2)\n",
    "print(size_dv)\n",
    "#slice dev\n",
    "dev_data_sample = full_dev_data[:size_dv]\n",
    "print(dev_data_sample[:5])\n",
    "print()\n",
    "full_test_data = list(read_ontonotes('test.tsv'))\n",
    "size_ts = int(len(full_test_data)/2)\n",
    "print(size_ts)\n",
    "test_data_sample = full_test_data[:size_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "q32R9o_mJZAt",
    "outputId": "90185b9d-d7c0-4726-ff64-4f48d88845c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33409\n",
      "[['Big', 'O'], ['Managers', 'O'], ['on', 'O'], ['Campus', 'O']]\n",
      "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O']]\n",
      "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O']]\n",
      "------------------------------------------\n",
      "5806\n",
      "[['President', 'B-WORK_OF_ART'], ['Chen', 'I-WORK_OF_ART'], ['Travels', 'I-WORK_OF_ART'], ['Abroad', 'I-WORK_OF_ART']]\n",
      "[['(', 'O'], ['Chang', 'B-PERSON'], ['Chiung', 'I-PERSON'], ['-', 'I-PERSON'], ['fang', 'I-PERSON'], ['/', 'O'], ['tr.', 'O'], ['by', 'O'], ['David', 'B-PERSON'], ['Mayer', 'I-PERSON'], [')', 'O']]\n",
      "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON']]\n",
      "------------------------------------------\n",
      "4875\n",
      "[['Powerful', 'B-WORK_OF_ART'], ['Tools', 'I-WORK_OF_ART'], ['for', 'I-WORK_OF_ART'], ['Biotechnology', 'I-WORK_OF_ART'], ['-', 'I-WORK_OF_ART'], ['Biochips', 'I-WORK_OF_ART']]\n",
      "[['(', 'O'], ['Chang', 'B-PERSON'], ['Chiung', 'I-PERSON'], ['-', 'I-PERSON'], ['fang', 'I-PERSON'], ['/', 'O'], ['photos', 'O'], ['by', 'O'], ['Hsueh', 'B-PERSON'], ['Chi', 'I-PERSON'], ['-', 'I-PERSON'], ['kuang', 'I-PERSON'], ['/', 'O'], ['tr.', 'O'], ['by', 'O'], ['Robert', 'B-PERSON'], ['Taylor', 'I-PERSON'], [')', 'O']]\n",
      "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O']]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#regex for empty space chars, \\t \\n\n",
    "tab = re.compile('[\\t]')\n",
    "\n",
    "def clean(list):\n",
    "    clean_data =[]\n",
    "    for sent in list:\n",
    "        clean_list = []\n",
    "        for item in sent:\n",
    "            str = ''.join(item)\n",
    "            #match_nl = re.match(r\"\\n\", str)\n",
    "            #print(match_nl)\n",
    "            count_tab =  re.findall(r\"\\t\", str)\n",
    "            #print(count_tab)\n",
    "            if len(count_tab) == 1: \n",
    "                item = re.split(\"\\t\", str)\n",
    "                if item[0] != '.':\n",
    "                    clean_list.append(item)\n",
    "            elif len(count_tab) > 1:\n",
    "                item = re.split(\"\\n\", str)\n",
    "                #print(item)\n",
    "                for i in range(len(item)):\n",
    "                    #print(item[i])\n",
    "                    if i == 0 or i == len(item)-1:\n",
    "                        item[i] = '\"'+item[i]\n",
    "                        item[i] = re.split(\"\\t\", item[i])\n",
    "                        #print(item[i])\n",
    "                    else:\n",
    "                        item[i] = re.split(\"\\t\", item[i])\n",
    "                        #print(item[i])\n",
    "                    clean_list.append(item[i])\n",
    "        clean_data.append(clean_list)        \n",
    "    return clean_data\n",
    "\n",
    "train_data_clean = clean(train_data_sample)\n",
    "print(len(train_data_clean))\n",
    "for item in train_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')\n",
    "dev_data_clean = clean(dev_data_sample)\n",
    "print(len(dev_data_clean))\n",
    "for item in dev_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')\n",
    "test_data_clean = clean(test_data_sample)\n",
    "print(len(test_data_clean))\n",
    "for item in test_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "cZHhXzVTQA_P",
    "outputId": "61915d82-b534-4ab3-9ebb-9b06f9a47551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': ['Big', 'Managers', 'on', 'Campus'], 'tags': ['O', 'O', 'O', 'O']}, {'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
      "\n",
      "[{'text': ['President', 'Chen', 'Travels', 'Abroad'], 'tags': ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']}, {'text': ['(', 'Chang', 'Chiung', '-', 'fang', '/', 'tr.', 'by', 'David', 'Mayer', ')'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O']}, {'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON']}]\n"
     ]
    }
   ],
   "source": [
    "# shape into dicts per sentence\n",
    "\n",
    "def reshape_sent2dicts(f):\n",
    "    data_dict = []\n",
    "    for item in f: # list of lists (tokens)\n",
    "        #print(item)\n",
    "        sent_text= [] \n",
    "        sent_tags = []\n",
    "        for token in item:\n",
    "            if len(token) ==2:\n",
    "                sent_text.append(token[0])\n",
    "                sent_tags.append(token[1])\n",
    "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
    "        #print(sent_dict['text'])\n",
    "        #print(sent_dict['tags'])\n",
    "        data_dict.append(sent_dict)\n",
    "    return data_dict\n",
    "\n",
    "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))   #30000\n",
    "samp = train_data_sent[:3]\n",
    "print(samp)\n",
    "print()\n",
    "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
    "samp2 = dev_data_sent[:3]\n",
    "print(samp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "VpYBQMbcQGBi",
    "outputId": "b0f28077-f547-4786-e8ed-a1e3a79b67d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "Text:  [['Sharon', 'Repudiates', 'the', 'Road', 'Map'], ['AppleScript', 'is', 'just', 'one', 'of', 'several', 'client', 'languages', 'for', 'these', 'services', '(', 'it', 'was', \"n't\", 'even', 'the', 'first', ',', 'btw', ';', 'UserTalk', 'predates', 'it', ')'], ['Your', 'qualifications', 'quote', 'end', 'quote', 'meaningless', '/.'], ['[dasanicool]', 'Young', 'people', 'like', 'to', 'race', 'high', '-', 'power', 'motorcycles', 'late', 'at', 'night', 'and', 'revel', 'in', 'bars', 'well', 'after', 'dark']]\n",
      "Labels:  [['O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(train_data_sent)\n",
    "print(type(train_data_sent))\n",
    "print(type(train_data_sent[0]))\n",
    "\n",
    "train_texts=[i[\"text\"] for i in train_data_sent]\n",
    "train_labels=[i[\"tags\"] for i in train_data_sent]\n",
    "\n",
    "#print(type(train_texts))\n",
    "#print(type(train_texts[0]))\n",
    "\n",
    "print('Text: ', train_texts[:4])\n",
    "print('Labels: ', train_labels[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNQQRw0YO-Ng"
   },
   "outputs": [],
   "source": [
    "## same for validation/dev data\n",
    "dev_texts=[i[\"text\"] for i in dev_data_sent]\n",
    "dev_labels=[i[\"tags\"] for i in dev_data_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ICn08fOgbyXl",
    "outputId": "4019782c-2cd6-488a-8a30-85dfc37189d6"
   },
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "#!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bFv2qclTbyXp",
    "outputId": "0e085d9f-271b-458c-8893-140a1adbd227"
   },
   "outputs": [],
   "source": [
    "# Give -n argument so that a possible existing file isn't overwritten \n",
    "#!unzip -n wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "kx_C5ii8byXt",
    "outputId": "a840f04e-b6dc-4977-f7e8-c00392cf619d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\tanja\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from gensim) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from gensim) (1.17.4)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from gensim) (2.0.0)\n",
      "Requirement already satisfied: boto in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.13.7)\n",
      "Requirement already satisfied: requests in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.7 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.16.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from botocore<1.17.0,>=1.16.7->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from botocore<1.17.0,>=1.16.7->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)   #50000\n",
    "\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
    "print(\"Words from embedding model:\", len(words))\n",
    "print(\"First 50 words:\", words[:50])\n",
    "\n",
    "# Normalize the vectors to unit length\n",
    "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NkdgjgOlbyXx",
    "outputId": "3740c1ec-523f-4ac2-b12e-d304f8833959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary mappings\n",
    "\n",
    "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
    "# Also reserve an index for out-of-vocabulary items.\n",
    "vocabulary={\n",
    "    \"<PAD>\": 0,\n",
    "    \"<OOV>\": 1\n",
    "}\n",
    "\n",
    "for word in words: # These are words from the word2vec model\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
    "\n",
    "\n",
    "# Embedding matrix\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGaojUBhbyX2"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "_M9Ox5_ObyX3",
    "outputId": "36177f59-cde2-4e5d-e466-e32b9af57bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-CARDINAL': 20,\n",
      " 'B-DATE': 4,\n",
      " 'B-EVENT': 7,\n",
      " 'B-FAC': 10,\n",
      " 'B-GPE': 24,\n",
      " 'B-LANGUAGE': 9,\n",
      " 'B-LAW': 18,\n",
      " 'B-LOC': 17,\n",
      " 'B-MONEY': 36,\n",
      " 'B-NORP': 8,\n",
      " 'B-ORDINAL': 33,\n",
      " 'B-ORG': 23,\n",
      " 'B-PERCENT': 5,\n",
      " 'B-PERSON': 2,\n",
      " 'B-PRODUCT': 6,\n",
      " 'B-QUANTITY': 22,\n",
      " 'B-TIME': 16,\n",
      " 'B-WORK_OF_ART': 0,\n",
      " 'I-CARDINAL': 26,\n",
      " 'I-DATE': 25,\n",
      " 'I-EVENT': 13,\n",
      " 'I-FAC': 14,\n",
      " 'I-GPE': 3,\n",
      " 'I-LANGUAGE': 29,\n",
      " 'I-LAW': 19,\n",
      " 'I-LOC': 30,\n",
      " 'I-MONEY': 12,\n",
      " 'I-NORP': 32,\n",
      " 'I-ORDINAL': 35,\n",
      " 'I-ORG': 31,\n",
      " 'I-PERCENT': 28,\n",
      " 'I-PERSON': 15,\n",
      " 'I-PRODUCT': 34,\n",
      " 'I-QUANTITY': 11,\n",
      " 'I-TIME': 1,\n",
      " 'I-WORK_OF_ART': 27,\n",
      " 'O': 21}\n"
     ]
    }
   ],
   "source": [
    "#Labels\n",
    "from pprint import pprint\n",
    "\n",
    "not_letter = re.compile(r'[^a-zA-Z]')\n",
    "# Label mappings\n",
    "# 1) gather a set of unique labels\n",
    "label_set = set()\n",
    "for sentence_labels in train_labels: #loops over sentences \n",
    "    #print(sentence_labels)\n",
    "    for label in sentence_labels: #loops over labels in one sentence\n",
    "       # match = not_letter.match(label)\n",
    "        #if match or label== 'O':\n",
    "        #    break\n",
    "        #else:    \n",
    "        label_set.add(label)\n",
    "\n",
    "# 2) index these\n",
    "label_map = {}\n",
    "for index, label in enumerate(label_set):\n",
    "    label_map[label]=index\n",
    "    \n",
    "pprint(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r8k8DshceEaI",
    "outputId": "cf16ed82-8615-48c9-8119-15dbaa1f66cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 21, 21, 21, 21]\n"
     ]
    }
   ],
   "source": [
    "# vectorize the labels\n",
    "def label_vectorizer(train_labels,label_map):\n",
    "    vectorized_labels = []\n",
    "    for label in train_labels:\n",
    "        vectorized_example_label = []\n",
    "        for token in label:\n",
    "            if token in label_map:\n",
    "                vectorized_example_label.append(label_map[token])\n",
    "        vectorized_labels.append(vectorized_example_label)\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    return vectorized_labels\n",
    "        \n",
    "\n",
    "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
    "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
    "\n",
    "pprint(vectorized_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YUtqLdCMPf3X",
    "outputId": "38be2fd5-5f93-4737-f7f8-ecbe132b919d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sharon', 'Repudiates', 'the', 'Road', 'Map']\n",
      "[8346, 1, 3, 1685, 8936]\n"
     ]
    }
   ],
   "source": [
    "## vectorization of the texts\n",
    "def text_vectorizer(vocab, train_texts):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(train_texts):\n",
    "        vectorized_example = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "\n",
    "        vectorized_data.append(vectorized_example)     \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
    "    \n",
    "    return vectorized_data, sentence_lengths\n",
    "\n",
    "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
    "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
    "\n",
    "pprint(train_texts[0])\n",
    "pprint(vectorized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "9e6FH5F1QGrq",
    "outputId": "e2a94f3a-a8fc-427c-95b3-8df9072a5348"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (30000,)\n",
      "New shape: (30000, 17040)\n",
      "First example:\n",
      "[   0    0    0 ...    3 1685 8936]\n",
      "Padded labels shape: (30000, 17040, 1)\n",
      "{'B-CARDINAL': 20,\n",
      " 'B-DATE': 4,\n",
      " 'B-EVENT': 7,\n",
      " 'B-FAC': 10,\n",
      " 'B-GPE': 24,\n",
      " 'B-LANGUAGE': 9,\n",
      " 'B-LAW': 18,\n",
      " 'B-LOC': 17,\n",
      " 'B-MONEY': 36,\n",
      " 'B-NORP': 8,\n",
      " 'B-ORDINAL': 33,\n",
      " 'B-ORG': 23,\n",
      " 'B-PERCENT': 5,\n",
      " 'B-PERSON': 2,\n",
      " 'B-PRODUCT': 6,\n",
      " 'B-QUANTITY': 22,\n",
      " 'B-TIME': 16,\n",
      " 'B-WORK_OF_ART': 0,\n",
      " 'I-CARDINAL': 26,\n",
      " 'I-DATE': 25,\n",
      " 'I-EVENT': 13,\n",
      " 'I-FAC': 14,\n",
      " 'I-GPE': 3,\n",
      " 'I-LANGUAGE': 29,\n",
      " 'I-LAW': 19,\n",
      " 'I-LOC': 30,\n",
      " 'I-MONEY': 12,\n",
      " 'I-NORP': 32,\n",
      " 'I-ORDINAL': 35,\n",
      " 'I-ORG': 31,\n",
      " 'I-PERCENT': 28,\n",
      " 'I-PERSON': 15,\n",
      " 'I-PRODUCT': 34,\n",
      " 'I-QUANTITY': 11,\n",
      " 'I-TIME': 1,\n",
      " 'I-WORK_OF_ART': 27,\n",
      " 'O': 21}\n",
      "First example labels:\n",
      "array([[ 0],\n",
      "       [ 0],\n",
      "       [ 0],\n",
      "       ...,\n",
      "       [21],\n",
      "       [21],\n",
      "       [21]])\n",
      "First weight vector:\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# padding for tensor\n",
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max(lengths))\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\")\n",
    "print( vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max(lengths)), -1)\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "pprint(label_map)\n",
    "print(\"First example labels:\")\n",
    "pprint(vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\")\n",
    "print( weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max(lengths))\n",
    "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max(lengths)), -1)\n",
    "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhAOVAbBTRAv"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "import keras\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    return entity_offsets\n",
    "\n",
    "def _entity_level_PRF(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    \n",
    "    try:\n",
    "        precision = tp / pred_count # tp / (tp+np)\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "    return precision, recall, fscore             \n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.fscore = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
    "        self.precision.append(evaluation_parameters[0])\n",
    "        self.recall.append(evaluation_parameters[1])\n",
    "        self.fscore.append(evaluation_parameters[2])\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1ydCexfTg5Q"
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "model = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "-oUO3GLfTrl3",
    "outputId": "beddefb3-b8f8-4226-bed3-b9d2c3561318"
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "FIPQrLXUVrWr",
    "outputId": "3865fb67-c82e-416f-83c9-9219bd39b628"
   },
   "outputs": [],
   "source": [
    "# train the model 1\n",
    "optimizer=Adam(lr=0.05) # define the learning rate\n",
    "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "vanilla_hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo pip install h5py   #we need this to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "bah0URSVV5GP",
    "outputId": "9e291831-c2eb-40ac-cbed-260c0c8898e9"
   },
   "outputs": [],
   "source": [
    "# plot the f scores\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(fscores):\n",
    "    print(\"History:\", fscores)\n",
    "    print(\"Highest f-score:\", max(fscores))\n",
    "    plt.plot(fscores)\n",
    "    plt.legend(loc='lower center', borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKpy2xq0NeSX"
   },
   "outputs": [],
   "source": [
    "# model 2\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "model2 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "bTYjLB28KfUR",
    "outputId": "b151caa5-f9c9-41e8-9813-ce89d217c7c3"
   },
   "outputs": [],
   "source": [
    "# train the model 2\n",
    "optimizer=Adamax(lr=0.01) # define the learning rate\n",
    "model2.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "vanilla_hist=model2.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "RIm0vXkjKqYf",
    "outputId": "0a4de211-6762-4cba-c9ea-593ef48f1091"
   },
   "outputs": [],
   "source": [
    "# plot the f scores\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(fscores):\n",
    "    print(\"History:\", fscores)\n",
    "    print(\"Highest f-score:\", max(fscores))\n",
    "    plt.plot(fscores)\n",
    "    plt.legend(loc='lower center', borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVS-GKkQUpue"
   },
   "outputs": [],
   "source": [
    "# model 3\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "model3 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "QuWSrGPVUw9Z",
    "outputId": "7614f4ae-9d42-446a-b74b-06f651ff1cfa"
   },
   "outputs": [],
   "source": [
    "# train the model 3\n",
    "optimizer=Adadelta(lr=0.07) # define the learning rate  #0.01\n",
    "model3.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train   #batch 100\n",
    "vanilla_hist=model3.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=5, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('C:\\\\Users\\\\Tanja\\\\Desktop\\\\NER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwMwwo9-MPv4"
   },
   "source": [
    "## 1.2 Expand context\n",
    "\n",
    "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lw9pXRewbyX6"
   },
   "outputs": [],
   "source": [
    "#expanding to RNN model with context\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "rnn_size = 100\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
    "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "MPP-kwoNXUMb",
    "outputId": "bbd3adad-3b95-4dd1-ee50-7ab2e3d4d3a3"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KIal9meVXnN_",
    "outputId": "72a23a6d-66dd-47d6-d436-8da706ddbd3f"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer=Adam(lr=0.01) # define the learning rate\n",
    "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRKNs4t8X3Ed"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCo0xF5kMMbH"
   },
   "source": [
    "## 2.1 Use deep contextual representations\n",
    "\n",
    "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgSYNcerMI9R"
   },
   "source": [
    "## 2.2 Error analysis\n",
    "\n",
    "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Tanja\\\\Desktop\\\\NER')   #local drive where the models are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 17040)]           0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 17040, 300)        15000600  \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 17040, 50)         15050     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 17040, 37)         1887      \n",
      "=================================================================\n",
      "Total params: 15,017,537\n",
      "Trainable params: 16,937\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model3, including its weights and the optimizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "new_model = tf.keras.models.load_model('Adadelta0.90.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorized_data_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.np_utils.probas_to_classes(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "blbl = new_model.predict(vectorized_data_padded)  #save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08892131, 0.02688256, 0.0095024 , 0.0187302 , 0.02342887,\n",
       "       0.03504962, 0.06626129, 0.01667434, 0.01818836, 0.01585606,\n",
       "       0.01212635, 0.01231169, 0.02075939, 0.01226838, 0.01391629,\n",
       "       0.02552154, 0.08118048, 0.04636185, 0.01880098, 0.01383605,\n",
       "       0.01867937, 0.03393837, 0.03781938, 0.03164722, 0.0340524 ,\n",
       "       0.02948727, 0.01912063, 0.01546681, 0.02990779, 0.01902025,\n",
       "       0.0165079 , 0.03000472, 0.02418155, 0.02181449, 0.01876216,\n",
       "       0.02881646, 0.01419528], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blbl[0][0]   #see what the output looks like\n",
    "#The outputs are probabilities for each class (tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "list_of_tag_args = []    #create a list of saved positions for each entity label\n",
    "for i in blbl:\n",
    "    for j in i:\n",
    "        temp = np.argmax(j)\n",
    "        list_of_tag_args.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tag_args  #Now that we have this list, we can use our label map dictionary to identify the actual tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRDxKgLSL_uf"
   },
   "source": [
    "## 3.1 Predictions on unannotated text\n",
    "\n",
    "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlG6ZWkIL-HY"
   },
   "source": [
    "## 3.2 Statistically analyze the results\n",
    "\n",
    "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DL_NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
