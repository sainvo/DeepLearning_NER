{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTBsYI1tLeVk"
   },
   "source": [
    "# Deep Learning NER task\n",
    "\n",
    "Tatjana Cucic and Sanna Volanen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9T2GevEzfPP2"
   },
   "source": [
    "https://spacy.io/api/annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5MwAmUALZ4V"
   },
   "source": [
    "# Milestones\n",
    "\n",
    "## 1.1 Predicting word labels independently\n",
    "\n",
    "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
    "* Evaluate the results on token level and entity level. \n",
    "* Report your results with different network hyperparameters. \n",
    "* Also discuss whether the token level accuracy is a reasonable metric.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "0Q3HiGQgMU5L",
    "outputId": "c0b2e3e1-9449-40d3-fc64-1af39c2b517f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-19 08:37:36--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17252156 (16M) [text/plain]\n",
      "Saving to: ‘train.tsv’\n",
      "\n",
      "train.tsv           100%[===================>]  16.45M  53.4MB/s    in 0.3s    \n",
      "\n",
      "2020-05-19 08:37:37 (53.4 MB/s) - ‘train.tsv’ saved [17252156/17252156]\n",
      "\n",
      "--2020-05-19 08:37:39--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2419425 (2.3M) [text/plain]\n",
      "Saving to: ‘dev.tsv’\n",
      "\n",
      "dev.tsv             100%[===================>]   2.31M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-05-19 08:37:40 (20.7 MB/s) - ‘dev.tsv’ saved [2419425/2419425]\n",
      "\n",
      "--2020-05-19 08:37:42--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1788466 (1.7M) [text/plain]\n",
      "Saving to: ‘test.tsv’\n",
      "\n",
      "test.tsv            100%[===================>]   1.71M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2020-05-19 08:37:43 (43.5 MB/s) - ‘test.tsv’ saved [1788466/1788466]\n",
      "\n",
      "--2020-05-19 08:37:44--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2020-05-19 08:37:44 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training data: Used for training the model\n",
    "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
    "\n",
    "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
    "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
    "\n",
    "# Test data: Never touched during training / model development, used for evaluating the final model\n",
    "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
    "\n",
    "#saved model\n",
    "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FZhc7b6VFWYX",
    "outputId": "a35b5f5d-2c9e-49d2-b93c-774b77cd0b1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOOHEYpiMzFp"
   },
   "outputs": [],
   "source": [
    "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
    "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
    "\n",
    "def read_ontonotes(tsv_file): # \n",
    "    current_sent = [] # list of (word,label) lists\n",
    "    #with open(tsv_file) as f:\n",
    "    with open(tsv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
    "        for line in tsvreader:\n",
    "            #print(line)\n",
    "            if not line:\n",
    "                if current_sent:\n",
    "                    yield current_sent\n",
    "                    current_sent=[]\n",
    "                continue\n",
    "            current_sent.append(line[0]) \n",
    "        else:\n",
    "            if current_sent:\n",
    "                yield current_sent\n",
    "\n",
    "train_data_full = list(read_ontonotes('train.tsv'))\n",
    "dev_data_full = list(read_ontonotes('dev.tsv'))\n",
    "test_data_full = list(read_ontonotes('test.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "-QALBoWU0JQB",
    "outputId": "f2aec0b7-3d09-47b5-e12c-e4133725b3c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And\\tO',\n",
       " 'what\\tO',\n",
       " 'effect\\tO',\n",
       " 'does\\tO',\n",
       " 'their\\tO',\n",
       " 'return\\tO',\n",
       " 'have\\tO',\n",
       " 'on\\tO',\n",
       " 'campus\\tO',\n",
       " '?\\tO']"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "q32R9o_mJZAt",
    "outputId": "146ee246-a589-494a-c72b-c5159d90166a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50252\n",
      "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O'], ['.', 'O']]\n",
      "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O'], ['.', 'O']]\n",
      "[['In', 'O'], ['order', 'O'], ['to', 'O'], ['attract', 'O'], ['this', 'O'], ['group', 'O'], ['of', 'O'], ['seasoned', 'O'], ['adults', 'O'], ['pulling', 'O'], ['in', 'O'], ['over', 'O'], ['NT$', 'B-MONEY'], ['1', 'I-MONEY'], ['million', 'I-MONEY'], ['a', 'O'], ['year', 'O'], ['back', 'O'], ['to', 'O'], ['the', 'O'], ['ivory', 'O'], ['tower', 'O'], [',', 'O'], ['universities', 'O'], ['have', 'O'], ['begun', 'O'], ['to', 'O'], ['establish', 'O'], ['executive', 'O'], ['MBA', 'B-WORK_OF_ART'], ['(', 'O'], ['EMBA', 'B-WORK_OF_ART'], [')', 'O'], ['programs', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "#regex for empty space chars, \\t \\n\n",
    "tab = re.compile('[\\t]')\n",
    "line = re.compile('[\\n]')\n",
    "punct = re.compile('[.?!:;]')\n",
    "\n",
    "def splitter(sent):\n",
    "    #print('----------------------------------------')\n",
    "    #print(\"one sentence in raw data:\", sent)\n",
    "    split_list = []\n",
    "    # loop over tokens items inside sentence, supposedly item= token+ \\t +tag\n",
    "    for item in sent: \n",
    "        #print(\"Item in sentence: \", item)\n",
    "        if item != None:\n",
    "            match1 = item.count('\\n')\n",
    "            #print(match1)\n",
    "            match2 = item.count('\\t')\n",
    "            #print(match2)\n",
    "            if match1 ==0: # no new lines nested\n",
    "                if match2 == 1: #just one tab inside token\n",
    "                    item_pair = item.split('\\t')\n",
    "                if item_pair[0] =='': # replacing empty string with missing quote marks\n",
    "                    item_pair[0] = '\\\"'\n",
    "                split_list.append(item_pair) \n",
    "            else:\n",
    "                subitems_list = item.split('\\n') ## check if token has \\n -> bundled, quotes\n",
    "                if len(subitems_list) > 1:  ## item string has more than one sentence nested in it\n",
    "                    #print(\"Found nested sentences: \", subitems_list)\n",
    "                    #print(\"subseq start\")\n",
    "                    for j in range(len(subitems_list)):  \n",
    "                        token = subitems_list[j]  \n",
    "                        #print(token)\n",
    "                        subtoken_listed_again = token.split('\\n') \n",
    "                        for token in subtoken_listed_again:\n",
    "                            match1=token.count('\\n')\n",
    "                            match2=token.count('\\t')\n",
    "                            if  match1 == 0: # no new lines nested\n",
    "                               if  match2 == 1: #just one tab inside token\n",
    "                                    token = token.split('\\t')\n",
    "                            if token =='': # replacing empty string with missing quote marks\n",
    "                                token = '\\\"'\n",
    "                            if token == '.':\n",
    "                                split_list.append(token)\n",
    "                                continue\n",
    "                                split_list=[]\n",
    "                            else:\n",
    "                                split_list.append(token)\n",
    "                    #print(\"subseq end\")\n",
    "    for item in split_list:\n",
    "        #print(\"Item in split list: \",item)\n",
    "        if type(item) != list:\n",
    "            split_list.remove(item)\n",
    "        if item[0] =='': # replacing empty string with missing quote marks\n",
    "            item[0] = '\\\"'\n",
    "    #print(\"Resplitted sentence :\", split_list)\n",
    "    return split_list\n",
    "\n",
    "def clean(raw_data): ## input list is list of lists of strings \n",
    "    clean_data =[]  #list of lists that have one clean sentence per list\n",
    "    for sent in raw_data: # split by [] lines, supposedly a sentence line\n",
    "        one_sentence = [] #collects the new sentence if there has been need to resplit items\n",
    "        splitted= splitter(sent)\n",
    "        for item in splitted:\n",
    "            #print(item)\n",
    "            matchi = re.match(punct, item[0])\n",
    "            if matchi:\n",
    "                #print(\"collected sentence\")\n",
    "                one_sentence.append(item)\n",
    "                clean_data.append(one_sentence)\n",
    "                one_sentence=[]\n",
    "                break\n",
    "            else:\n",
    "                one_sentence.append(item)\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "train_data_clean = clean(train_data_full)\n",
    "print(len(train_data_clean))\n",
    "for item in train_data_clean[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "jQrs8MSroPfa",
    "outputId": "cf87288a-94c0-48c8-8d44-f581a919625c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence: 168 index:  8041\n",
      "[168, 145, 123, 121, 120, 108, 106, 106, 102, 101, 97, 96, 94, 94, 93, 93, 93, 92, 91, 91, 91, 90, 90, 90, 90, 90, 90, 89, 88, 88, 88, 87, 87, 86, 85, 85, 85, 85, 84, 84, 84, 84, 83, 83, 83, 83, 82, 82, 82, 82, 81, 81, 80, 80, 80, 79, 79, 79, 79, 79, 79, 78, 78, 78, 78, 78, 77, 77, 77, 77, 77, 77, 77, 77, 76, 76, 76, 76, 76, 76, 76, 76, 76, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n"
     ]
    }
   ],
   "source": [
    "# final check on the sentences\n",
    "item_lengths = []\n",
    "max_text = 0\n",
    "for item in train_data_clean:\n",
    "    item_lengths.append(len(item))\n",
    "    if len(item) > max_text:\n",
    "        max_text = len(item)\n",
    "        ind = train_data_clean.index(item)\n",
    "print(\"Longest sentence:\", max_text, \"index: \",ind)\n",
    "\n",
    "lengths_sorted = sorted(item_lengths, reverse=True)\n",
    "max = item_lengths.index(max_text)\n",
    "#print(items_sorted[0])\n",
    "#pprint(train_data_clean[max])\n",
    "print(lengths_sorted[:300]) # longest sentences\n",
    "# checking long items\n",
    "#for item in train_data_clean:\n",
    "    #if len(item) == 123:\n",
    "        #pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BjUDEzLB0JQO",
    "outputId": "91c9f95b-b9db-485b-cd0b-4217c52fe7b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'O'],\n",
       " ['rational', 'O'],\n",
       " ['approach', 'O'],\n",
       " ['to', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['would', 'O'],\n",
       " ['entail', 'O'],\n",
       " ['(', 'O'],\n",
       " ['1', 'O'],\n",
       " [')', 'O'],\n",
       " ['creating', 'O'],\n",
       " ['communities', 'O'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['mix', 'O'],\n",
       " ['of', 'O'],\n",
       " ['housing', 'O'],\n",
       " ['/', 'O'],\n",
       " ['office', 'O'],\n",
       " ['/', 'O'],\n",
       " ['retail', 'O'],\n",
       " ['/', 'O'],\n",
       " ['amenities', 'O'],\n",
       " ['to', 'O'],\n",
       " ['reduce', 'O'],\n",
       " ['the', 'O'],\n",
       " ['number', 'O'],\n",
       " ['and', 'O'],\n",
       " ['length', 'O'],\n",
       " ['of', 'O'],\n",
       " ['automobile', 'O'],\n",
       " ['trips', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['2', 'O'],\n",
       " [')', 'O'],\n",
       " ['reforming', 'O'],\n",
       " ['counterproductive', 'O'],\n",
       " ['zoning', 'O'],\n",
       " ['codes', 'O'],\n",
       " [',', 'O'],\n",
       " ['subdivision', 'O'],\n",
       " ['ordinances', 'O'],\n",
       " ['and', 'O'],\n",
       " ['comprehensive', 'O'],\n",
       " ['plans', 'O'],\n",
       " ['in', 'O'],\n",
       " ['order', 'O'],\n",
       " ['to', 'O'],\n",
       " ['create', 'O'],\n",
       " ['more', 'O'],\n",
       " ['transit', 'O'],\n",
       " ['-', 'O'],\n",
       " ['friendly', 'O'],\n",
       " ['and', 'O'],\n",
       " ['pedestrian', 'O'],\n",
       " ['-', 'O'],\n",
       " ['friendly', 'O'],\n",
       " ['communities', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['3', 'O'],\n",
       " [')', 'O'],\n",
       " ['creating', 'O'],\n",
       " ['funding', 'O'],\n",
       " ['sources', 'O'],\n",
       " ['for', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['that', 'O'],\n",
       " ['establishes', 'O'],\n",
       " ['a', 'O'],\n",
       " ['rational', 'O'],\n",
       " ['nexus', 'O'],\n",
       " ['between', 'O'],\n",
       " ['those', 'O'],\n",
       " ['who', 'O'],\n",
       " ['use', 'O'],\n",
       " ['/', 'O'],\n",
       " ['benefit', 'O'],\n",
       " ['from', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['facilities', 'O'],\n",
       " ['and', 'O'],\n",
       " ['those', 'O'],\n",
       " ['who', 'O'],\n",
       " ['pay', 'O'],\n",
       " ['for', 'O'],\n",
       " ['them', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['4', 'O'],\n",
       " [')', 'O'],\n",
       " ['promoting', 'O'],\n",
       " ['telework', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['5', 'O'],\n",
       " [')', 'O'],\n",
       " ['liberating', 'O'],\n",
       " ['mass', 'O'],\n",
       " ['transit', 'O'],\n",
       " ['from', 'O'],\n",
       " ['government', 'O'],\n",
       " ['monopolies', 'O'],\n",
       " ['and', 'O'],\n",
       " ['encouraging', 'O'],\n",
       " ['private', 'O'],\n",
       " ['sector', 'O'],\n",
       " ['innovation', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['6', 'O'],\n",
       " [')', 'O'],\n",
       " ['using', 'O'],\n",
       " ['a', 'O'],\n",
       " ['wide', 'O'],\n",
       " ['range', 'O'],\n",
       " ['of', 'O'],\n",
       " ['tools', 'O'],\n",
       " [',', 'O'],\n",
       " ['from', 'O'],\n",
       " ['corridor', 'O'],\n",
       " ['management', 'O'],\n",
       " ['to', 'O'],\n",
       " ['intelligent', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['systems', 'O'],\n",
       " [',', 'O'],\n",
       " ['to', 'O'],\n",
       " ['increase', 'O'],\n",
       " ['the', 'O'],\n",
       " ['capacity', 'O'],\n",
       " ['of', 'O'],\n",
       " ['existing', 'O'],\n",
       " ['thoroughfares', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['7', 'O'],\n",
       " [')', 'O'],\n",
       " ['stretching', 'O'],\n",
       " ['the', 'O'],\n",
       " ['value', 'O'],\n",
       " ['of', 'O'],\n",
       " ['road', 'O'],\n",
       " ['construction', 'O'],\n",
       " ['/', 'O'],\n",
       " ['maintenance', 'O'],\n",
       " ['dollars', 'O'],\n",
       " ['through', 'O'],\n",
       " ['outsourcing', 'O'],\n",
       " [',', 'O'],\n",
       " ['and', 'O'],\n",
       " ['(', 'O'],\n",
       " ['8', 'O'],\n",
       " [')', 'O'],\n",
       " ['prioritizing', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['projects', 'O'],\n",
       " ['on', 'O'],\n",
       " ['a', 'O'],\n",
       " ['congestion', 'O'],\n",
       " ['-', 'O'],\n",
       " ['mitigation', 'O'],\n",
       " ['Return', 'O'],\n",
       " ['on', 'O'],\n",
       " ['Investment', 'O'],\n",
       " ['basis', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np   #want to see the longest sentence\n",
    "\n",
    "lengths = np.array(item_lengths)\n",
    "train_data_clean[np.argmax(lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "UxmF7-eH0JQS",
    "outputId": "8d003c4f-7385-468c-9d4a-559109d03c24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In', 'O'],\n",
       " ['recent', 'B-DATE'],\n",
       " ['years', 'I-DATE'],\n",
       " [',', 'O'],\n",
       " ['advanced', 'O'],\n",
       " ['education', 'O'],\n",
       " ['for', 'O'],\n",
       " ['professionals', 'O'],\n",
       " ['has', 'O'],\n",
       " ['become', 'O'],\n",
       " ['a', 'O'],\n",
       " ['hot', 'O'],\n",
       " ['topic', 'O'],\n",
       " ['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['business', 'O'],\n",
       " ['community', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "tk8Brh3sd6dl",
    "outputId": "3611cf67-2ae7-45d7-b17a-6a62aa897a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "9954\n",
      "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON'], ['.', 'O']]\n",
      "[['On', 'O'], ['August', 'B-DATE'], ['25', 'I-DATE'], ['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['wrapped', 'O'], ['up', 'O'], ['his', 'O'], ['first', 'B-ORDINAL'], ['overseas', 'O'], ['trip', 'O'], ['since', 'O'], ['taking', 'O'], ['office', 'O'], [',', 'O'], ['swinging', 'O'], ['through', 'O'], ['three', 'B-CARDINAL'], ['countries', 'O'], ['in', 'O'], ['Latin', 'B-LOC'], ['America', 'I-LOC'], ['and', 'O'], ['another', 'O'], ['three', 'B-CARDINAL'], ['in', 'O'], ['Africa', 'B-LOC'], ['.', 'O']]\n",
      "[['While', 'O'], ['in', 'O'], ['the', 'B-GPE'], ['Dominican', 'I-GPE'], ['Republic', 'I-GPE'], ['to', 'O'], ['attend', 'O'], ['the', 'O'], ['inauguration', 'O'], ['of', 'O'], ['President', 'O'], ['Hipolito', 'B-PERSON'], ['Mejia', 'I-PERSON'], [',', 'O'], ['Chen', 'B-PERSON'], ['had', 'O'], ['a', 'O'], ['chance', 'O'], ['to', 'O'], ['meet', 'O'], ['with', 'O'], ['the', 'O'], ['leaders', 'O'], ['of', 'O'], ['many', 'O'], ['different', 'O'], ['nations', 'O'], ['.', 'O']]\n",
      "------------------------------------------\n",
      "7920\n",
      "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O'], ['.', 'O']]\n",
      "[['Worldwide', 'O'], [',', 'O'], ['biotechnology', 'O'], ['is', 'O'], ['a', 'O'], ['rising', 'O'], ['star', 'O'], ['of', 'O'], ['the', 'O'], ['industrial', 'O'], ['stage', 'O'], ['.', 'O']]\n",
      "[['In', 'O'], ['Taiwan', 'B-GPE'], [',', 'O'], ['in', 'O'], ['recent', 'O'], ['years', 'O'], ['the', 'B-ORG'], ['Ministry', 'I-ORG'], ['of', 'I-ORG'], ['Economic', 'I-ORG'], ['Affairs', 'I-ORG'], [',', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Science', 'I-ORG'], ['Council', 'I-ORG'], ['and', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Health', 'I-ORG'], ['Research', 'I-ORG'], ['Institutes', 'I-ORG'], ['have', 'O'], ['been', 'O'], ['strongly', 'O'], ['pursuing', 'O'], ['\"', 'O'], ['biochip', 'O'], ['\"', 'O'], ['research', 'O'], ['programs', 'O'], ['.', 'O']]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('------------------------------------------')\n",
    "dev_data_clean = clean(dev_data_full)\n",
    "print(len(dev_data_clean))\n",
    "for item in dev_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')\n",
    "test_data_clean = clean(test_data_full)\n",
    "print(len(test_data_clean))\n",
    "for item in test_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "cZHhXzVTQA_P",
    "outputId": "7fff5460-db57-4131-ef74-aae425096363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
      "\n",
      "[{'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']}, {'text': ['On', 'August', '25', 'President', 'Chen', 'Shui', '-', 'bian', 'wrapped', 'up', 'his', 'first', 'overseas', 'trip', 'since', 'taking', 'office', ',', 'swinging', 'through', 'three', 'countries', 'in', 'Latin', 'America', 'and', 'another', 'three', 'in', 'Africa', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-CARDINAL', 'O', 'B-LOC', 'O']}, {'text': ['While', 'in', 'the', 'Dominican', 'Republic', 'to', 'attend', 'the', 'inauguration', 'of', 'President', 'Hipolito', 'Mejia', ',', 'Chen', 'had', 'a', 'chance', 'to', 'meet', 'with', 'the', 'leaders', 'of', 'many', 'different', 'nations', '.'], 'tags': ['O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n"
     ]
    }
   ],
   "source": [
    "# shape into dicts per sentence\n",
    "\n",
    "def reshape_sent2dicts(f):\n",
    "    data_dict = []\n",
    "    for item in f: # list of lists (tokens)\n",
    "        #print(item)\n",
    "        sent_text= [] \n",
    "        sent_tags = []\n",
    "        for token in item:\n",
    "            if len(token) ==2:\n",
    "                sent_text.append(token[0])\n",
    "                sent_tags.append(token[1])\n",
    "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
    "        #print(sent_dict['text'])\n",
    "        #print(sent_dict['tags'])\n",
    "        data_dict.append(sent_dict)\n",
    "    return data_dict\n",
    "\n",
    "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))\n",
    "samp = train_data_sent[:2]\n",
    "print(samp)\n",
    "print()\n",
    "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
    "samp2 = dev_data_sent[:3]\n",
    "print(samp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "VpYBQMbcQGBi",
    "outputId": "f243cc11-6960-4d5e-8362-79ec0d85a6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'text': ['Does', 'it', 'wake', 'you', 'up', '?'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O']}\n",
      "\n",
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "------------\n",
      "0\n",
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----------------------------\n",
      "30000\n",
      "-----------------------\n",
      "Text:  ['Does', 'it', 'wake', 'you', 'up', '?']\n",
      " Texts length:  30000\n",
      "Label:  ['O', 'O', 'O', 'O', 'O', 'O']\n",
      " Labels length:  30000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(train_data_sent)\n",
    "#max_sent = [max(len(i[\"text\"])) for i in train_data_sent]\n",
    "#print(max_sent)\n",
    "print(type(train_data_sent))\n",
    "print(train_data_sent[0]) ##one dict\n",
    "print()\n",
    "print(train_data_sent[0][\"text\"])\n",
    "print()\n",
    "print(train_data_sent[0][\"tags\"])\n",
    "print('------------')\n",
    "\n",
    "def typed_listing(data, key):\n",
    "    listed = []\n",
    "    max_length = 0\n",
    "    for item in data: # dictionary {text:\"\", tags:\"\"}\n",
    "        #print('Item: ', item)\n",
    "        #print('Key: ', key, ' content: ', item[key], 'length: ',len(item[key]))\n",
    "        if len(item[key]) > max_length:\n",
    "            max = len(item[key])\n",
    "        listed.append(item[key])\n",
    "    return listed, max_length\n",
    "\n",
    "listed_texts= typed_listing(train_data_sent, \"text\")\n",
    "train_texts = listed_texts[0]\n",
    "train_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(train_data_sent, \"tags\")\n",
    "train_labels= listed_labels[0]\n",
    "train_lbl_max = listed_labels[1]\n",
    "print(train_txt_max)\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "\n",
    "\n",
    "print('-----------------------------')\n",
    "print(len(train_texts))\n",
    "print('-----------------------')\n",
    "print('Text: ', train_texts[0])\n",
    "print(' Texts length: ',len(train_texts))\n",
    "print('Label: ', train_labels[0])\n",
    "print(' Labels length: ',len(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "DNQQRw0YO-Ng",
    "outputId": "e4fbc4ac-42ed-4cda-e7d8-868b3f8e76db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.']\n",
      " Texts length:  9954\n",
      "Label:  ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']\n",
      " Labels length:  9954\n"
     ]
    }
   ],
   "source": [
    "## same for validation/dev data\n",
    "listed_texts= typed_listing(dev_data_sent, \"text\")\n",
    "dev_texts = listed_texts[0]\n",
    "dev_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(dev_data_sent, \"tags\")\n",
    "dev_labels= listed_labels[0]\n",
    "dev_lbl_max = listed_labels[1]\n",
    "print('Text: ', dev_texts[0])\n",
    "print(' Texts length: ',len(dev_texts))\n",
    "print('Label: ', dev_labels[0])\n",
    "print(' Labels length: ',len(dev_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ICn08fOgbyXl",
    "outputId": "c660f2cf-e7c3-403d-8a9d-0742a9951f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-19 08:46:17--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4a8e, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  11.3MB/s    in 66s     \n",
      "\n",
      "2020-05-19 08:47:25 (9.80 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained embeddings\n",
    "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bFv2qclTbyXp",
    "outputId": "ee0cd2a1-e0bb-430e-8e7d-13094c83d1fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "# Give -n argument so that a possible existing file isn't overwritten \n",
    "!unzip -n wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "kx_C5ii8byXt",
    "outputId": "1c5f2a63-6c99-4088-a038-708ce2cf57c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
    "print(\"Words from embedding model:\", len(words))\n",
    "print(\"First 50 words:\", words[:50])\n",
    "\n",
    "# Normalize the vectors to unit length\n",
    "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NkdgjgOlbyXx",
    "outputId": "6caeac5d-0288-4905-f007-1dd209df08c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary mappings\n",
    "\n",
    "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
    "# Also reserve an index for out-of-vocabulary items.\n",
    "vocabulary={\n",
    "    \"<PAD>\": 0,\n",
    "    \"<OOV>\": 1\n",
    "}\n",
    "\n",
    "for word in words: # These are words from the word2vec model\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
    "\n",
    "\n",
    "# Embedding matrix\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGaojUBhbyX2"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "_M9Ox5_ObyX3",
    "outputId": "3699dca5-beca-414a-d247-33376b06cd1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-CARDINAL': 1,\n",
      " 'B-DATE': 0,\n",
      " 'B-EVENT': 18,\n",
      " 'B-FAC': 25,\n",
      " 'B-GPE': 19,\n",
      " 'B-LANGUAGE': 32,\n",
      " 'B-LAW': 14,\n",
      " 'B-LOC': 30,\n",
      " 'B-MONEY': 9,\n",
      " 'B-NORP': 10,\n",
      " 'B-ORDINAL': 20,\n",
      " 'B-ORG': 27,\n",
      " 'B-PERCENT': 11,\n",
      " 'B-PERSON': 17,\n",
      " 'B-PRODUCT': 24,\n",
      " 'B-QUANTITY': 4,\n",
      " 'B-TIME': 16,\n",
      " 'B-WORK_OF_ART': 28,\n",
      " 'I-CARDINAL': 8,\n",
      " 'I-DATE': 33,\n",
      " 'I-EVENT': 13,\n",
      " 'I-FAC': 22,\n",
      " 'I-GPE': 12,\n",
      " 'I-LANGUAGE': 6,\n",
      " 'I-LAW': 35,\n",
      " 'I-LOC': 23,\n",
      " 'I-MONEY': 15,\n",
      " 'I-NORP': 34,\n",
      " 'I-ORDINAL': 7,\n",
      " 'I-ORG': 26,\n",
      " 'I-PERCENT': 2,\n",
      " 'I-PERSON': 31,\n",
      " 'I-PRODUCT': 36,\n",
      " 'I-QUANTITY': 29,\n",
      " 'I-TIME': 21,\n",
      " 'I-WORK_OF_ART': 5,\n",
      " 'O': 3}\n"
     ]
    }
   ],
   "source": [
    "#Labels\n",
    "\n",
    "\n",
    "not_letter = re.compile(r'[^a-zA-Z]')\n",
    "# Label mappings\n",
    "# 1) gather a set of unique labels\n",
    "label_set = set()\n",
    "for sentence_labels in train_labels: #loops over sentences \n",
    "    #print(sentence_labels)\n",
    "    for label in sentence_labels: #loops over labels in one sentence\n",
    "       # match = not_letter.match(label)\n",
    "        #if match or label== 'O':\n",
    "        #    break\n",
    "        #else:    \n",
    "        label_set.add(label)\n",
    "\n",
    "# 2) index these\n",
    "label_map = {}\n",
    "for index, label in enumerate(label_set):\n",
    "    label_map[label]=index\n",
    "    \n",
    "pprint(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r8k8DshceEaI",
    "outputId": "da8aa7bc-ec38-41bb-e703-48047a000cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# vectorize the labels\n",
    "def label_vectorizer(train_labels,label_map):\n",
    "    vectorized_labels = []\n",
    "    for label in train_labels:\n",
    "        vectorized_example_label = []\n",
    "        for token in label:\n",
    "            if token in label_map:\n",
    "                vectorized_example_label.append(label_map[token])\n",
    "        vectorized_labels.append(vectorized_example_label)\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    return vectorized_labels\n",
    "        \n",
    "\n",
    "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
    "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
    "\n",
    "print(vectorized_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "YUtqLdCMPf3X",
    "outputId": "70876258-9984-41d2-aede-67f918a86d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "[3328, 21, 8846, 32, 91, 39]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## vectorization of the texts\n",
    "def text_vectorizer(vocab, train_texts):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(train_texts):\n",
    "        vectorized_example = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "\n",
    "        vectorized_data.append(vectorized_example)     \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
    "    \n",
    "    return vectorized_data, sentence_lengths\n",
    "\n",
    "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
    "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
    "\n",
    "print(train_texts[0])\n",
    "print(vectorized_data[0])\n",
    "pprint(type(lengths))\n",
    "#max = lengths.index(17040)\n",
    "#print(max)\n",
    "#pprint(train_texts[11103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "9e6FH5F1QGrq",
    "outputId": "6a5491a3-c174-49ef-d5fb-10d018ed5c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (30000,)\n",
      "New shape: (30000, 168)\n",
      "First example:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 3328   21 8846   32   91   39]\n",
      "<class 'numpy.ndarray'>\n",
      "Padded labels shape: (30000, 168, 1)\n",
      "First example labels:\n",
      "First weight vector:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# padding for tensor\n",
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "max_len = np.max(np.array(lengths))\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max_len)\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\")\n",
    "print( vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=np.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max_len), -1)\n",
    "\n",
    "print(type(vectorized_labels_padded))\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "#pprint(label_map)\n",
    "print(\"First example labels:\")\n",
    "#pprint(vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\")\n",
    "print( weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max_len)\n",
    "validation_vectorized_labels_padded=np.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max_len), -1)\n",
    "validation_weights = np.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhAOVAbBTRAv"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "import keras\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    return entity_offsets\n",
    "\n",
    "def _entity_level_PRF(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    \n",
    "    try:\n",
    "        precision = tp / pred_count # tp / (tp+np)\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "    return precision, recall, fscore             \n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.fscore = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
    "        self.precision.append(evaluation_parameters[0])\n",
    "        self.recall.append(evaluation_parameters[1])\n",
    "        self.fscore.append(evaluation_parameters[2])\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVS-GKkQUpue"
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed, SimpleRNN\n",
    "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam, RMSprop \n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = Dense(hidden_size, activation=\"relu\")(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "model1 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gj3x13FELJLt"
   },
   "outputs": [],
   "source": [
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuWSrGPVUw9Z"
   },
   "outputs": [],
   "source": [
    "# train model 1\n",
    "optimizer=Adam(lr=1e-4) # define the learning rate\n",
    "model1.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "vanilla_hist=model1.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=1,verbose=2,epochs=5, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGoIwwHj0JRK"
   },
   "source": [
    "RELU and Adam gave bad results with different learning rates and batch sizes. \n",
    "Tried multiple opttimizers, as well as activation functions, but nothing worked except lowering the learning rate. \n",
    "However, even with a low learning rate, the f score reached around 0.4 and no more than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecFC_-OhC1lN"
   },
   "outputs": [],
   "source": [
    "# plot the f scores\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "def plot_history(fscores):\n",
    "    max_f = np.max(np.array(fscores))\n",
    "    print(\"History:\", fscores)\n",
    "    print(\"Highest f-score:\", max_f)\n",
    "    plt.plot(fscores)\n",
    "    plt.legend(loc='lower center', borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5TNurFK0JRP"
   },
   "outputs": [],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZcv3f3W0JRS"
   },
   "outputs": [],
   "source": [
    "model1.save(\"NEWModel1_Adam0.38.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwMwwo9-MPv4"
   },
   "source": [
    "## 1.2 Expand context\n",
    "\n",
    "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "Lw9pXRewbyX6",
    "outputId": "a6f8cc36-469a-491f-9e49-8014115a3941"
   },
   "outputs": [],
   "source": [
    "#expanding to RNN model with context\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "rnn_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "    rnn = LSTM(rnn_size, activation='sigmoid', return_sequences=True)(embeddings)\n",
    "    outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPP-kwoNXUMb"
   },
   "outputs": [],
   "source": [
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIal9meVXnN_"
   },
   "outputs": [],
   "source": [
    "optimizer=Adam(lr=1e-10) # define the learning rate\n",
    "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were some serious problems with this LSTM. \n",
    "At first, it would onlt produce nan values for loss and an f score of around 0.9.\n",
    "However, the predictions were nan values, as well in this case.\n",
    "So, the best thing we managed to do was get it to\n",
    "have a declining loss and 0 precision and f score, which is still very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-oYGVn00JRh"
   },
   "outputs": [],
   "source": [
    "rnn_model.save(\"BadLSTMLossDeclining0.04FZero.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRKNs4t8X3Ed",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCo0xF5kMMbH"
   },
   "source": [
    "## 2.1 Use deep contextual representations\n",
    "\n",
    "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5WxyoYS0JRn"
   },
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbaUAX4x0JRn"
   },
   "outputs": [],
   "source": [
    "# Maximum number of examples to read\n",
    "MAX_EXAMPLES = 2000\n",
    "\n",
    "# Maximum length of input sequence in tokens\n",
    "INPUT_LENGTH = 25\n",
    "\n",
    "# Number of epochs to train for\n",
    "EPOCHS = 3\n",
    "\n",
    "# Optimizer learning rate\n",
    "LEARNING_RATE = 0.00002\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "5DMFxnCV0JRt",
    "outputId": "ba0ff2a0-8426-4f3c-ecd7-193fdc47fd13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-bert in /usr/local/lib/python3.6/dist-packages (0.81.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.4)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.3.1)\n",
      "Requirement already satisfied: keras-transformer>=0.30.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (0.33.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.4.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n",
      "Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.24.0)\n",
      "Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.11.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.14.0)\n",
      "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.7.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert) (0.6.0)\n",
      "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert) (0.41.0)\n"
     ]
    }
   ],
   "source": [
    "pip install keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qn73pBDl0JRw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_t8avxr-0JRz",
    "outputId": "ac8fed8b-1b34-44b8-e23d-3e899521ca55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘cased_L-12_H-768_A-12.zip’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download pretrained BERT model\n",
    "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
    "!wget -nc https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nGlMGcSQ0JR2",
    "outputId": "46d8890a-4429-42e6-c93e-f9338c6e1aa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cased_L-12_H-768_A-12.zip\n"
     ]
    }
   ],
   "source": [
    "# Give -n argument so that existing files aren't overwritten \n",
    "!unzip -n cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kStXHCjp0JR4"
   },
   "outputs": [],
   "source": [
    "#os.chdir(\"C:\\\\Users\\\\Tanja\\\\Desktop\\\\NER\")\n",
    "\n",
    "bert_vocab_path = 'cased_L-12_H-768_A-12/vocab.txt'\n",
    "bert_config_path = 'cased_L-12_H-768_A-12/bert_config.json'\n",
    "bert_checkpoint_path = 'cased_L-12_H-768_A-12/bert_model.ckpt'    # suffixes not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OPPV2510JR7"
   },
   "outputs": [],
   "source": [
    "model_is_cased = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Nxc8u9sd0JR-",
    "outputId": "1243379a-19af-4be7-abd6-4e1696bbbc01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', 'щ', '吉', 'told', 'space', 'operations', 'proposed', 'Oxford', 'showing', 'domestic', 'mountains', 'commission', 'voices', 'associate', 'hills', 'Guide', 'relaxed', 'Page', 'Heights', 'singers', 'Interior', 'considers', 'facilitate', 'shouting', '1826', 'constitute', 'alter', 'clip', 'Into', 'Memory', 'ballad', 'Owens', 'Langdon', 'aquatic', 'stereo', 'Cass', 'Shock', '195', '##tec', '##sonic', 'attested', '##rdes', '1840s', '##90', 'Guys', '##rien', 'Munro', 'Ursula', 'mesh', 'diplomacy', 'Newmarket', '##oughs', 'synthesizers', 'Drugs', 'monstrous', '##ynamic', 'troll', '##ٹ']\n"
     ]
    }
   ],
   "source": [
    "#Load BERT vocabulary\n",
    "\n",
    "vocab = []\n",
    "with open(bert_vocab_path,  encoding=\"utf8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vocab.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
    "\n",
    "\n",
    "# Print a list with every 500th vocabulary item\n",
    "print(vocab[0::500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SNiHqJ2B0JSD",
    "outputId": "c614e4bc-37a8-4689-a349-ab5ba670fe15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_probs_dropout_prob': 0.1,\n",
      " 'hidden_act': 'gelu',\n",
      " 'hidden_dropout_prob': 0.1,\n",
      " 'hidden_size': 768,\n",
      " 'initializer_range': 0.02,\n",
      " 'intermediate_size': 3072,\n",
      " 'max_position_embeddings': 512,\n",
      " 'num_attention_heads': 12,\n",
      " 'num_hidden_layers': 12,\n",
      " 'type_vocab_size': 2,\n",
      " 'vocab_size': 28996}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pprint import pprint    # pretty-printer for output\n",
    "\n",
    "\n",
    "with open(bert_config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "# Print configuration contents\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "kj7xUooX0JSG",
    "outputId": "197fcada-39ad-4483-ee9d-fd425f946aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'##bor': 12207,\n",
      " '##utile': 26555,\n",
      " 'However': 1438,\n",
      " 'Title': 11772,\n",
      " 'calcium': 15355,\n",
      " 'celebrities': 13073,\n",
      " 'compares': 26153,\n",
      " 'dome': 10945,\n",
      " 'tap': 12999,\n",
      " 'threatens': 18241}\n"
     ]
    }
   ],
   "source": [
    "#Create BERT tokenizer\n",
    "# Create mapping from vocabulary items to their indices in the vocabulary\n",
    "token_dict = { v: i for i, v in enumerate(vocab) }\n",
    "\n",
    "\n",
    "# Print some random examples of the mapping\n",
    "pprint(dict(random.choices(list(token_dict.items()), k=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zpM78dZi0JSJ",
    "outputId": "a8891276-885b-4086-98b7-aa1b13d271eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: Hello BERT!\n",
      "Tokenized: ['[CLS]', 'Hello', 'B', '##ER', '##T', '!', '[SEP]']\n",
      "Encoded: [101, 8667, 139, 9637, 1942, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded: Hello B ##ER ##T !\n",
      "\n",
      "Original string: Unknown: 你\n",
      "Tokenized: ['[CLS]', 'Unknown', ':', '你', '[SEP]']\n",
      "Encoded: [101, 16285, 131, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded: Unknown : [UNK]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(token_dict, cased=model_is_cased)\n",
    "\n",
    "\n",
    "# Let's test that out\n",
    "for s in ['Hello BERT!', 'Unknown: 你']:\n",
    "    print('Original string:', s)\n",
    "    print('Tokenized:', tokenizer.tokenize(s))\n",
    "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
    "    print('Encoded:', indices)\n",
    "    print('Segments:', segments)\n",
    "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "eGtlvCNp0JSL",
    "outputId": "dd85f7f3-4761-4a2c-a7d2-d40c63119287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 38\n",
      "Tags: ['NONE', 'B-CARDINAL', 'B-DATE', 'B-EVENT', 'B-FAC', 'B-GPE', 'B-LANGUAGE', 'B-LAW', 'B-LOC', 'B-MONEY', 'B-NORP', 'B-ORDINAL', 'B-ORG', 'B-PERCENT', 'B-PERSON', 'B-PRODUCT', 'B-QUANTITY', 'B-TIME', 'B-WORK_OF_ART', 'I-CARDINAL', 'I-DATE', 'I-EVENT', 'I-FAC', 'I-GPE', 'I-LANGUAGE', 'I-LAW', 'I-LOC', 'I-MONEY', 'I-NORP', 'I-ORDINAL', 'I-ORG', 'I-PERCENT', 'I-PERSON', 'I-PRODUCT', 'I-QUANTITY', 'I-TIME', 'I-WORK_OF_ART', 'O']\n",
      "Mapping: {'NONE': 0, 'B-CARDINAL': 1, 'B-DATE': 2, 'B-EVENT': 3, 'B-FAC': 4, 'B-GPE': 5, 'B-LANGUAGE': 6, 'B-LAW': 7, 'B-LOC': 8, 'B-MONEY': 9, 'B-NORP': 10, 'B-ORDINAL': 11, 'B-ORG': 12, 'B-PERCENT': 13, 'B-PERSON': 14, 'B-PRODUCT': 15, 'B-QUANTITY': 16, 'B-TIME': 17, 'B-WORK_OF_ART': 18, 'I-CARDINAL': 19, 'I-DATE': 20, 'I-EVENT': 21, 'I-FAC': 22, 'I-GPE': 23, 'I-LANGUAGE': 24, 'I-LAW': 25, 'I-LOC': 26, 'I-MONEY': 27, 'I-NORP': 28, 'I-ORDINAL': 29, 'I-ORG': 30, 'I-PERCENT': 31, 'I-PERSON': 32, 'I-PRODUCT': 33, 'I-QUANTITY': 34, 'I-TIME': 35, 'I-WORK_OF_ART': 36, 'O': 37}\n",
      "Inverted: {0: 'NONE', 1: 'B-CARDINAL', 2: 'B-DATE', 3: 'B-EVENT', 4: 'B-FAC', 5: 'B-GPE', 6: 'B-LANGUAGE', 7: 'B-LAW', 8: 'B-LOC', 9: 'B-MONEY', 10: 'B-NORP', 11: 'B-ORDINAL', 12: 'B-ORG', 13: 'B-PERCENT', 14: 'B-PERSON', 15: 'B-PRODUCT', 16: 'B-QUANTITY', 17: 'B-TIME', 18: 'B-WORK_OF_ART', 19: 'I-CARDINAL', 20: 'I-DATE', 21: 'I-EVENT', 22: 'I-FAC', 23: 'I-GPE', 24: 'I-LANGUAGE', 25: 'I-LAW', 26: 'I-LOC', 27: 'I-MONEY', 28: 'I-NORP', 29: 'I-ORDINAL', 30: 'I-ORG', 31: 'I-PERCENT', 32: 'I-PERSON', 33: 'I-PRODUCT', 34: 'I-QUANTITY', 35: 'I-TIME', 36: 'I-WORK_OF_ART', 37: 'O'}\n"
     ]
    }
   ],
   "source": [
    "#Vectorize data\n",
    "# This is our special placeholder label value\n",
    "NO_LABEL = 'NONE'\n",
    "\n",
    "\n",
    "# This just flattens out tags from the sentences list-of-lists\n",
    "#train_tags = [tag for sentence in tag_texts for word, tag in sentence]\n",
    "# Use set() to get the unique tags and add our special label to the list\n",
    "#unique_tags = [NO_LABEL] + sorted(set(train_tags))\n",
    "unique_tags = [NO_LABEL] + sorted(set(label_set))\n",
    "\n",
    "# Create mappings from tags to integer values and back\n",
    "tag_to_int = { t: i for i, t in enumerate(unique_tags) }\n",
    "int_to_tag = { i: t for t, i in tag_to_int.items() }\n",
    "\n",
    "# Take note of how many unique labels (tags) there are in the data\n",
    "num_labels = len(unique_tags)\n",
    "\n",
    "\n",
    "# Let's see what we got\n",
    "print('Number of unique labels:', num_labels)\n",
    "print('Tags:', unique_tags)\n",
    "print('Mapping:', tag_to_int)\n",
    "print('Inverted:', int_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "T4xWQ6fN0JSN",
    "outputId": "a570bffe-2d6e-4f1f-e07a-cd397336126b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In'] ['O']\n",
      "['recent'] ['B-DATE']\n",
      "['years'] ['I-DATE']\n",
      "[','] ['O']\n",
      "['advanced'] ['O']\n",
      "['education'] ['O']\n",
      "['for'] ['O']\n",
      "['professionals'] ['O']\n",
      "['has'] ['O']\n",
      "['become'] ['O']\n",
      "['a'] ['O']\n",
      "['hot'] ['O']\n",
      "['topic'] ['O']\n",
      "['in'] ['O']\n",
      "['the'] ['O']\n",
      "['business'] ['O']\n",
      "['community'] ['O']\n",
      "['.'] ['O']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_word(word):\n",
    "    # keras-bert tokenizer wraps its inputs with the special BERT tokens\n",
    "    # [CLS] and [SEP]. To tokenize individual words, we remove these.\n",
    "    wrapped = tokenizer.tokenize(word)\n",
    "    return wrapped[1:-1]    # remove [CLS] at the first and [SEP] at the last position\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokenized = []\n",
    "    for word, tag in sentence:\n",
    "        tokens = tokenize_word(word)\n",
    "        # Original tag for first wordpiece, fill in placeholder tags for\n",
    "        # any continuation pieces.\n",
    "        tags = [tag] + [NO_LABEL] * (len(tokens)-1)\n",
    "        tokenized.append((tokens, tags))\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    return [tokenize_sentence(s) for s in sentences]\n",
    "\n",
    "\n",
    "tokenized_train = tokenize_sentences(train_data_clean)\n",
    "tokenized_dev = tokenize_sentences(dev_data_clean)\n",
    "\n",
    "\n",
    "# Let's look at an example sentence after tokenization\n",
    "for tokens, tags in tokenized_train[0]:\n",
    "    print(tokens, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5PE4tQYQ0JSP",
    "outputId": "6fbd18df-0d45-41fa-fb18-f4cad16ff628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X[0].shape: (50252, 25)\n",
      "train_X[1].shape: (50252, 25)\n",
      "train_Y[0].shape: (25, 38)\n",
      "train_X[0][0] (first sentence token IDs):\n",
      "[ 101 1130 2793 1201  117 3682 1972 1111 8799 1144 1561  170 2633 8366\n",
      " 1107 1103 1671 1661  119  102    0    0    0    0    0]\n",
      "train_X[1][0] (first sentence segment IDs):\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_Y[0][0] (first sentence tag IDs):\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "decoded train_X[0][0] (first sentence tokens):\n",
      "[CLS] In recent years , advanced education for professionals has become a hot topic in the business community . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "decoded train_Y[0] (first sentence tags):\n",
      "NONE O B-DATE I-DATE O O O O O O O O O O O O O O O NONE NONE NONE NONE NONE NONE\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def vectorize_tokenized(tokenized, length=INPUT_LENGTH):\n",
    "    # Flatten lists-of-lists of tokens and tags\n",
    "    token_sequence, tag_sequence = [], []\n",
    "    for tokens, tags in tokenized:\n",
    "        token_sequence.extend(tokens)\n",
    "        tag_sequence.extend(tags)\n",
    "\n",
    "    # Truncation and padding using length-2 to leave space for\n",
    "    # wrapping with the special [CLS] and [SEP] tokens\n",
    "    if len(token_sequence) > length-2:\n",
    "        token_sequence = token_sequence[:length-2]\n",
    "        tag_sequence = tag_sequence[:length-2]\n",
    "\n",
    "    # Wrap with [CLS] and [SEP], adding corresponding placeholder labels\n",
    "    token_sequence = ['[CLS]'] + token_sequence + ['[SEP]']\n",
    "    tag_sequence = [NO_LABEL] + tag_sequence + [NO_LABEL]\n",
    "\n",
    "    while len(token_sequence) < length:\n",
    "        token_sequence.append('[PAD]')\n",
    "        tag_sequence.append(NO_LABEL)\n",
    "    \n",
    "    # Use keras-bert tokenizer and previously created label mapping\n",
    "    token_ids = tokenizer._convert_tokens_to_ids(token_sequence)\n",
    "    tag_ids = [tag_to_int[tag] for tag in tag_sequence]\n",
    "    \n",
    "    # Also create all-zeros segment IDs for BERT\n",
    "    segment_ids = [0] * INPUT_LENGTH\n",
    "\n",
    "    return token_ids, segment_ids, tag_ids\n",
    "\n",
    "\n",
    "def vectorize_dataset(tokenized_data, length=INPUT_LENGTH):\n",
    "    # Create separate lists of token ID, segment ID, and tag ID lists,\n",
    "    # each of the inner lists representing one sentence.\n",
    "    token_ids_list, segment_ids_list, tag_ids_list = [], [], []\n",
    "    for tokenized in tokenized_data:\n",
    "        token_ids, segment_ids, tag_ids = vectorize_tokenized(tokenized, length=length)\n",
    "        token_ids_list.append(token_ids)\n",
    "        segment_ids_list.append(segment_ids)\n",
    "        tag_ids_list.append(tag_ids)\n",
    "\n",
    "    # Return as numpy arrays. Input (X) consists of the token and\n",
    "    # segment IDs, output (Y) of the tag ids. We'll use a one-hot\n",
    "    # representation for the output.\n",
    "    X = [np.array(token_ids_list), np.array(segment_ids_list)]\n",
    "    Y = to_categorical(np.array(tag_ids_list))\n",
    "\n",
    "    return X, Y\n",
    "    \n",
    "\n",
    "train_X, train_Y = vectorize_dataset(tokenized_train)\n",
    "dev_X, dev_Y = vectorize_dataset(tokenized_dev)\n",
    "\n",
    "\n",
    "# Let's have a bit of a look at that\n",
    "print('train_X[0].shape:', train_X[0].shape)\n",
    "print('train_X[1].shape:', train_X[1].shape)\n",
    "print('train_Y[0].shape:', train_Y[0].shape)\n",
    "print('train_X[0][0] (first sentence token IDs):')\n",
    "print(train_X[0][0])\n",
    "print('train_X[1][0] (first sentence segment IDs):')\n",
    "print(train_X[1][0])\n",
    "print('train_Y[0][0] (first sentence tag IDs):')\n",
    "print(train_Y[0])\n",
    "print('decoded train_X[0][0] (first sentence tokens):')\n",
    "print(' '.join([tokenizer._token_dict_inv[t] for t in train_X[0][0]]))\n",
    "print('decoded train_Y[0] (first sentence tags):')\n",
    "print(' '.join([int_to_tag[t.argmax()] for t in train_Y[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XS-bqCZp0JSR"
   },
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "\n",
    "pretrained_model = load_trained_model_from_checkpoint(\n",
    "    config_file = bert_config_path,\n",
    "    checkpoint_file = bert_checkpoint_path,\n",
    "    training = False,\n",
    "    trainable = True,\n",
    "    seq_len = INPUT_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hrYxnrRX0JSW",
    "outputId": "db7bd0d5-965b-4cba-caf8-f39b7ba325a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Input-Token:0' shape=(None, 25) dtype=float32>,\n",
       " <tf.Tensor 'Input-Segment:0' shape=(None, 25) dtype=float32>]"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s5abSqLK0JSY",
    "outputId": "2fe165ac-fdce-40a9-9abd-7b99cd9f4bda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Encoder-12-FeedForward-Norm/Identity:0' shape=(None, 25, 768) dtype=float32>]"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MHG_bJjT0JSf",
    "outputId": "21afc6e8-4f7c-45ef-ec77-876d083254fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 25, 768), (2 22268928    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 25, 768)      1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 25, 768)      0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 25, 768)      19200       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 25, 768)      0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 25, 768)      1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 25, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 25, 768)      0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 25, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 25, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 25, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 25, 768)      0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 25, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 25, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 25, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 25, 768)      0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 25, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 25, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
      "==================================================================================================\n",
      "Total params: 107,345,664\n",
      "Trainable params: 107,345,664\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.summary()  #We'll use a 12-layer-pretrained-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jbRINsra0JSh",
    "outputId": "ecae2f2c-6e7b-4a60-9d49-5373aa75e429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder-12-FeedForward-Norm/Identity:0\", shape=(None, 25, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# model.outputs is a list, here with a single item. We'll\n",
    "# add our output layer on top of that.\n",
    "bert_out = pretrained_model.outputs[0]\n",
    "\n",
    "print(bert_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yNrkzLv0JSk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "\n",
    "\n",
    "out = TimeDistributed(Dense(num_labels, activation='softmax'))(bert_out)\n",
    "model = Model(\n",
    "    inputs=pretrained_model.inputs,\n",
    "    outputs=[out]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPHvIyqr0JSm"
   },
   "outputs": [],
   "source": [
    "from keras_bert import calc_train_steps, AdamWarmup\n",
    "\n",
    "\n",
    "# Calculate the number of steps for warmup\n",
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=len(train_data_clean),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(\n",
    "    total_steps,\n",
    "    warmup_steps,\n",
    "    lr=LEARNING_RATE,\n",
    "    epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VppBMiU70JSo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops as o\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "def label_categorical_accuracy(y_true, y_pred):\n",
    "    # Map one-hot targets and predictions to tag indices \n",
    "    y_true_idx = o.argmax(y_true, axis=-1)\n",
    "    y_pred_idx = o.argmax(y_pred, axis=-1)\n",
    "    # Compare targets to predicted elementwise and cast the\n",
    "    # resulting boolean values to floating point values.\n",
    "    # (K.floatx() returns current default float type.)\n",
    "    correct_preds = o.cast(o.equal(y_true_idx, y_pred_idx), K.floatx())\n",
    "    # Compare targets to the special NO_LABEL value and cast\n",
    "    # the resulting boolean values to floating point values.\n",
    "    # This gives a value of zero for NO_LABEL and one for others.\n",
    "    is_label = o.cast(o.not_equal(y_true_idx, tag_to_int[NO_LABEL]), K.floatx())\n",
    "    # Take elementwise product of the comparisons, giving values that\n",
    "    # are one if the prediction is equal to the target and the target\n",
    "    # is not the NO_LABEL value, and zero otherwise.\n",
    "    correct_label_preds = o.multiply(correct_preds, is_label)\n",
    "    # Accuracy is then the number of correct predictions for labels\n",
    "    # divided by the number of labels.\n",
    "    return o.reduce_sum(correct_label_preds)/o.reduce_sum(is_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quqIBe8x0JSr"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[label_categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "I5lW6dZg0JSt",
    "outputId": "2396abb6-6cf0-431b-a63f-dbf6bdc56cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6282/6282 [==============================] - 2418s 385ms/step - loss: 0.1180 - label_categorical_accuracy: 0.9569 - val_loss: 0.1015 - val_label_categorical_accuracy: 0.9557\n",
      "Epoch 2/3\n",
      "6282/6282 [==============================] - 2416s 385ms/step - loss: 0.0240 - label_categorical_accuracy: 0.9864 - val_loss: 0.1113 - val_label_categorical_accuracy: 0.9507\n",
      "Epoch 3/3\n",
      "6282/6282 [==============================] - 2423s 386ms/step - loss: 0.0112 - label_categorical_accuracy: 0.9937 - val_loss: 0.1132 - val_label_categorical_accuracy: 0.9563\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(dev_X, dev_Y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vf43zatSEn8Z",
    "outputId": "7fbf8d57-3cea-45cd-ca5b-691dc2aae9f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2o6bVyrEkK-"
   },
   "outputs": [],
   "source": [
    "model.save(\"BERT_0.95.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "3AqEZk6QRk9O",
    "outputId": "768d45d0-df68-4c0d-82ca-38a58cf3281f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    train_metric = 'label_categorical_accuracy'\n",
    "    val_metric = 'val_label_categorical_accuracy'\n",
    "    plt.plot(history.history[val_metric],label=\"Validation set accuracy\")\n",
    "    plt.plot(history.history[train_metric],label=\"Training set accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgSYNcerMI9R"
   },
   "source": [
    "## 2.2 Error analysis\n",
    "\n",
    "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQkKBZ520JSz"
   },
   "source": [
    "Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "TTsj_fuu0JSz",
    "outputId": "ed6045ec-00ea-4708-e1f0-76609fab96f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 168)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 168, 50)           15050     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 168, 37)           1887      \n",
      "=================================================================\n",
      "Total params: 15,017,537\n",
      "Trainable params: 16,937\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#I want to see how the model predicts classes\n",
    "\n",
    "\n",
    "# Recreate the exact same rnn, including its weights and the optimizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "simple_rnn = tf.keras.models.load_model('NEWModel1_Adam0.3.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtLak0Yh0JS2"
   },
   "outputs": [],
   "source": [
    "simple_rnn_predictions = simple_rnn.predict(vectorized_data_padded)  #save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "r5r10ewb0JS4",
    "outputId": "854368c1-f07e-4a29-ee89-ac79d3c5fcb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4354858 , 0.01699266, 0.01746663, 0.02376078, 0.01758664,\n",
       "       0.01486108, 0.01326818, 0.00977711, 0.029234  , 0.01739111,\n",
       "       0.00686423, 0.01310816, 0.01929541, 0.01577151, 0.01108016,\n",
       "       0.03385628, 0.02223525, 0.01406394, 0.01525715, 0.00805973,\n",
       "       0.03214354, 0.0136274 , 0.00238292, 0.01606015, 0.00495875,\n",
       "       0.01371827, 0.01491944, 0.00959745, 0.01508046, 0.01765896,\n",
       "       0.00878316, 0.015236  , 0.01735945, 0.01086606, 0.00991656,\n",
       "       0.01698111, 0.02529456], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rnn_predictions[0][0]   #see what the output looks like\n",
    "#The outputs are probabilities for each class (tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y99-ITqg0JS8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "list_of_tag_args = []    #create a list of saved positions for each entity label\n",
    "for i in simple_rnn_predictions:\n",
    "    for j in i:\n",
    "        temp = np.argmax(j)\n",
    "        list_of_tag_args.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XOmi9qYf0JS-",
    "outputId": "14b192d2-165b-43f5-a791-6435dffb663b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tag_args  #Now that we have this list, we can use our label map dictionary to identify the actual tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEcvUR4C0mRw"
   },
   "outputs": [],
   "source": [
    "labels = list(label_map.keys())\n",
    "\n",
    "decoded_RNN_tags = []\n",
    "for i in list_of_tag_args:\n",
    "    decoded_RNN_tags.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-qqbVnYv0wbK",
    "outputId": "8693624a-d3aa-456d-f8b2-dad221cba22b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " 'B-DATE',\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_RNN_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "fcThvVgP0O4M",
    "outputId": "609b5a02-8d0f-4b28-da33-e85dc6be10b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tNONE\n",
      "model\tNONE\n",
      "can\tNONE\n",
      "predict\tNONE\n",
      "BIO\tNONE\n",
      "tags\tNONE\n",
      "quite\tB-DATE\n",
      "badly\tNONE\n",
      ".\tNONE\n",
      "\n",
      "I\tB-QUANTITY\n",
      "love\tNONE\n",
      "eating\tNONE\n",
      "ice-cream\tNONE\n",
      "with\tI-TIME\n",
      "Dave\tB-LOC\n",
      "Grohl\tNONE\n",
      "and\tNONE\n",
      "jogging\tNONE\n",
      "with\tI-TIME\n",
      "Madonna\tNONE\n",
      "to\tNONE\n",
      "New\tNONE\n",
      "York\tNONE\n",
      "and\tNONE\n",
      "Egypt.\tNONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rnn_predict_tags(words):\n",
    "    # This function takes a sequence of words, vectorizes it,\n",
    "    # and returns the model predictions. The vectorization code\n",
    "    # we wrote expects labels, so we'll attach dummy labels to\n",
    "    # use the code and then discard the dummy labels it generates.\n",
    "    dummy_tagged = [(word, NO_LABEL) for word in words]\n",
    "    tokenized_test = tokenize_sentences([dummy_tagged])\n",
    "    test_X, dummy_Y = vectorize_dataset(tokenized_test)\n",
    "    # Run model.predict for this single sequence \n",
    "    predictions = simple_rnn.predict(test_X)\n",
    "    # Our outputs are one-hot, take argmax to get indices\n",
    "    y = predictions[0].argmax(axis=1)\n",
    "    # For words tokenized into several subword parts, we\n",
    "    # only care about the predicted tag for the first part.\n",
    "    # Use the tokenization to identify these parts.\n",
    "    tags = []\n",
    "    i = 1    # Start at 1 to skip the special [CLS] token.\n",
    "    for tokens, _ in tokenized_test[0]:\n",
    "        tags.append(int_to_tag[y[i]])\n",
    "        i += len(tokens)\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Test the model with a few word sequences\n",
    "test_words = [\n",
    "  'This model can predict BIO tags quite badly .'.split(),\n",
    "  'I love eating ice-cream with Dave Grohl and jogging with Madonna to New York and Egypt.'.split()\n",
    "  ]\n",
    "\n",
    "\n",
    "for words in test_words:\n",
    "    tags = rnn_predict_tags(words)\n",
    "    for w, t in zip(words, tags):\n",
    "        print('{}\\t{}'.format(w, t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our simple RNN model did not have a very good score, it was expected to perform badly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-Hd2PK_0JTC"
   },
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "2Ya7_Sr90JTC",
    "outputId": "37337404-8f46-4c75-e049-c659da8551bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 168)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 168, 50)           70200     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 168, 37)           1887      \n",
      "=================================================================\n",
      "Total params: 15,072,687\n",
      "Trainable params: 72,087\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#I want to see how the model predicts classes\n",
    "\n",
    "\n",
    "# Recreate the exact same lstm, including its weights and the optimizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "lstm = tf.keras.models.load_model('BadLSTMLossDeclining0.04FZero.h5')   #This model has an f-score of 0.99\n",
    "\n",
    "# Show the model architecture\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3URMGQeS0JTE"
   },
   "outputs": [],
   "source": [
    "lstm_predictions = lstm.predict(vectorized_data_padded)  #save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "lAaPb9NQ0JTI",
    "outputId": "2c8172a2-a379-464e-bbea-2208edc26a26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02631832, 0.02742383, 0.02606142, 0.02615814, 0.02595535,\n",
       "       0.02584755, 0.02914128, 0.02726994, 0.02590375, 0.03963802,\n",
       "       0.02600654, 0.02565163, 0.02797289, 0.02586091, 0.02869107,\n",
       "       0.02663114, 0.02622616, 0.02715563, 0.02636004, 0.02914733,\n",
       "       0.02637641, 0.02571471, 0.02558731, 0.02647568, 0.03089567,\n",
       "       0.02555509, 0.02609051, 0.0259408 , 0.02579284, 0.02857378,\n",
       "       0.02601798, 0.02698946, 0.02596169, 0.02622531, 0.02574262,\n",
       "       0.02674478, 0.02589447], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predictions[0][0]   #see what the output looks like\n",
    "#The outputs are probabilities for each class (tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkFuOncw0JTK"
   },
   "outputs": [],
   "source": [
    "list_of_lstm_tag_args = []    #create a list of saved positions for each entity label\n",
    "for i in lstm_predictions:\n",
    "    for j in i:\n",
    "        temp = np.argmax(j)\n",
    "        list_of_lstm_tag_args.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Xo6K-QFe0JTM",
    "outputId": "4720837e-ecbb-4915-fa5e-02646e8d3052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " ...]"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lstm_tag_args  #Now that we have this list, we can use our label map dictionary to identify the actual tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr8cre9dyHGv"
   },
   "outputs": [],
   "source": [
    "labels = list(label_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i6Ahvn8IyVDd",
    "outputId": "501c6507-06f1-450d-c9ee-e69c4b7e9e3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-MONEY'"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[9]   #short demontration to check how to actually retrieve a tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNYfiNH3y-Ec"
   },
   "outputs": [],
   "source": [
    "decoded_tags = []\n",
    "for i in list_of_lstm_tag_args:\n",
    "    decoded_tags.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lZKX5H1RzIm7",
    "outputId": "0b1dadd0-dca1-49d3-9bb2-00fbc4d82144"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " 'B-MONEY',\n",
       " ...]"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tags   #Since our LSTM model was extremely bad, it is no wonder that it give this bad results :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "J4lW6ZqHzevE",
    "outputId": "653d7b93-8290-433d-a54e-d35d791b5de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tB-MONEY\n",
      "model\tB-MONEY\n",
      "can\tB-MONEY\n",
      "predict\tB-MONEY\n",
      "BIO\tB-MONEY\n",
      "tags\tB-MONEY\n",
      "quite\tB-MONEY\n",
      "badly\tB-MONEY\n",
      ".\tB-MONEY\n",
      "\n",
      "I\tB-MONEY\n",
      "love\tB-MONEY\n",
      "eating\tB-MONEY\n",
      "ice-cream\tB-MONEY\n",
      "with\tB-MONEY\n",
      "Dave\tB-MONEY\n",
      "Grohl\tB-MONEY\n",
      ".\tB-MONEY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lstm_predict_tags(words):\n",
    "    # This function takes a sequence of words, vectorizes it,\n",
    "    # and returns the model predictions. The vectorization code\n",
    "    # we wrote expects labels, so we'll attach dummy labels to\n",
    "    # use the code and then discard the dummy labels it generates.\n",
    "    dummy_tagged = [(word, NO_LABEL) for word in words]\n",
    "    tokenized_test = tokenize_sentences([dummy_tagged])\n",
    "    test_X, dummy_Y = vectorize_dataset(tokenized_test)\n",
    "    # Run model.predict for this single sequence \n",
    "    predictions = lstm.predict(test_X)\n",
    "    # Our outputs are one-hot, take argmax to get indices\n",
    "    y = predictions[0].argmax(axis=1)\n",
    "    # For words tokenized into several subword parts, we\n",
    "    # only care about the predicted tag for the first part.\n",
    "    # Use the tokenization to identify these parts.\n",
    "    tags = []\n",
    "    i = 1    # Start at 1 to skip the special [CLS] token.\n",
    "    for tokens, _ in tokenized_test[0]:\n",
    "        tags.append(int_to_tag[y[i]])\n",
    "        i += len(tokens)\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Test the model with a few word sequences\n",
    "test_words = [\n",
    "  'This model can predict BIO tags quite badly .'.split(),\n",
    "  'I love eating ice-cream with Dave Grohl .'.split()\n",
    "  ]\n",
    "\n",
    "\n",
    "for words in test_words:\n",
    "    tags = lstm_predict_tags(words)\n",
    "    for w, t in zip(words, tags):\n",
    "        print('{}\\t{}'.format(w, t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LSTM had an accuracy of zero and we had huge problems with it while training, so this is the worst model we have by far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpHVy0kXJmct"
   },
   "source": [
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "25C-cbecJrLV",
    "outputId": "e253e50b-0a4d-427c-b934-33146f3f9a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tO\n",
      "model\tO\n",
      "can\tO\n",
      "predict\tO\n",
      "BIO\tO\n",
      "tags\tO\n",
      ".\tO\n",
      "\n",
      "What\tO\n",
      "would\tO\n",
      "the\tO\n",
      "best\tO\n",
      "time\tO\n",
      "in\tO\n",
      "recent\tB-DATE\n",
      "years,\tI-DATE\n",
      "Michael\tB-PERSON\n",
      "Jackson\tI-PERSON\n",
      "wondered\tO\n",
      "quietly\tO\n",
      "in\tO\n",
      "July\tB-DATE\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_tags(words):\n",
    "    # This function takes a sequence of words, vectorizes it,\n",
    "    # and returns the model predictions. The vectorization code\n",
    "    # we wrote expects labels, so we'll attach dummy labels to\n",
    "    # use the code and then discard the dummy labels it generates.\n",
    "    dummy_tagged = [(word, NO_LABEL) for word in words]\n",
    "    tokenized_test = tokenize_sentences([dummy_tagged])\n",
    "    test_X, dummy_Y = vectorize_dataset(tokenized_test)\n",
    "    # Run model.predict for this single sequence \n",
    "    predictions = model.predict(test_X)\n",
    "    # Our outputs are one-hot, take argmax to get indices\n",
    "    y = predictions[0].argmax(axis=1)\n",
    "    # For words tokenized into several subword parts, we\n",
    "    # only care about the predicted tag for the first part.\n",
    "    # Use the tokenization to identify these parts.\n",
    "    tags = []\n",
    "    i = 1    # Start at 1 to skip the special [CLS] token.\n",
    "    for tokens, _ in tokenized_test[0]:\n",
    "        tags.append(int_to_tag[y[i]])\n",
    "        i += len(tokens)\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Test the model with a few word sequences\n",
    "test_words = [\n",
    "  'This model can predict BIO tags .'.split(),\n",
    "  'What would the best time in recent years, Michael Jackson wondered quietly in July .'.split()\n",
    "]\n",
    "\n",
    "\n",
    "for words in test_words:\n",
    "    tags = predict_tags(words)\n",
    "    for w, t in zip(words, tags):\n",
    "        print('{}\\t{}'.format(w, t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "Lgj_gU45FAQf",
    "outputId": "94b24b67-a504-4507-e967-b7a82abb0051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tO\n",
      "model\tO\n",
      "can\tO\n",
      "predict\tO\n",
      "BIO\tO\n",
      "tags\tO\n",
      ".\tO\n",
      "\n",
      "I\tO\n",
      "am\tO\n",
      "evil\tO\n",
      "Homer!\tB-PERSON\n",
      "Kurt\tB-PERSON\n",
      "Cobain\tI-PERSON\n",
      "rocks\tO\n",
      "since\tO\n",
      "1990\tB-DATE\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a few more word sequences\n",
    "test_words = [\n",
    "  'This model can predict BIO tags .'.split(),\n",
    "  'I am evil Homer! Kurt Cobain rocks since 1990 .'.split()\n",
    "]\n",
    "\n",
    "\n",
    "for words in test_words:\n",
    "    tags = predict_tags(words)\n",
    "    for w, t in zip(words, tags):\n",
    "        print('{}\\t{}'.format(w, t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, BERT gave some great results while training, as well as when looking at the predictions, so this is nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRDxKgLSL_uf"
   },
   "source": [
    "## 3.1 Predictions on unannotated text\n",
    "\n",
    "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlG6ZWkIL-HY"
   },
   "source": [
    "## 3.2 Statistically analyze the results\n",
    "\n",
    "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DL_NER_everything_so_far_plus_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
