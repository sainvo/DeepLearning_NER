{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER_with_context.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTBsYI1tLeVk"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2GevEzfPP2",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/api/annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O5MwAmUALZ4V"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q3HiGQgMU5L",
        "colab": {}
      },
      "source": [
        "# Training data: Used for training the model\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
        "\n",
        "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
        "\n",
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
        "\n",
        "#saved model\n",
        "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeoOlUGTwijU",
        "colab_type": "code",
        "outputId": "4a9378c9-b0a0-4fb7-d769-0875b55db75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9223372036854775807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOOHEYpiMzFp",
        "colab": {}
      },
      "source": [
        "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
        "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
        "\n",
        "def read_ontonotes(tsv_file): # \n",
        "    current_sent = [] # list of (word,label) lists\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line:\n",
        "                if current_sent:\n",
        "                    yield current_sent\n",
        "                    current_sent=[]\n",
        "                continue\n",
        "            current_sent.append(line[0]) \n",
        "        else:\n",
        "            if current_sent:\n",
        "                yield current_sent\n",
        "\n",
        "train_data_full = list(read_ontonotes('train.tsv'))\n",
        "dev_data_full = list(read_ontonotes('dev.tsv'))\n",
        "test_data_full = list(read_ontonotes('test.tsv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qm0zpFpw2Rt",
        "colab_type": "code",
        "outputId": "97968c66-d2fe-470b-f9c9-0be05d581d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import re\n",
        "from pprint import pprint\n",
        "#regex for empty space chars, \\t \\n\n",
        "#tab = re.compile('[\\t]')\n",
        "#line = re.compile('[\\n]')\n",
        "punct = re.compile('[.?!:;]')\n",
        "\n",
        "def splitter(sent):\n",
        "    #print('----------------------------------------')\n",
        "    #print(\"one sentence in raw data:\", sent)\n",
        "    split_list = []\n",
        "    # loop over tokens items inside sentence, supposedly item= token+ \\t +tag\n",
        "    for item in sent: \n",
        "        #print(\"Item in sentence: \", item)\n",
        "        if item != None:\n",
        "            match1 = item.count('\\n')\n",
        "            #print(match1)\n",
        "            match2 = item.count('\\t')\n",
        "            #print(match2)\n",
        "            if match1 ==0: # no new lines nested\n",
        "                if match2 == 1: #just one tab inside token\n",
        "                    item_pair = item.split('\\t')\n",
        "                if item_pair[0] =='': # replacing empty string with missing quote marks\n",
        "                    item_pair[0] = '\\\"'\n",
        "                split_list.append(item_pair) \n",
        "            else:\n",
        "                subitems_list = item.split('\\n') ## check if token has \\n -> bundled, quotes\n",
        "                if len(subitems_list) > 1:  ## item string has more than one sentence nested in it\n",
        "                    #print(\"Found nested sentences: \", subitems_list)\n",
        "                    #print(\"subseq start\")\n",
        "                    for j in range(len(subitems_list)):  \n",
        "                        token = subitems_list[j]  \n",
        "                        #print(token)\n",
        "                        subtoken_listed_again = token.split('\\n') \n",
        "                        for token in subtoken_listed_again:\n",
        "                            match1=token.count('\\n')\n",
        "                            match2=token.count('\\t')\n",
        "                            if  match1 == 0: # no new lines nested\n",
        "                               if  match2 == 1: #just one tab inside token\n",
        "                                    token = token.split('\\t')\n",
        "                            if token =='': # replacing empty string with missing quote marks\n",
        "                                token = '\\\"'\n",
        "                            if token == '.':\n",
        "                                split_list.append(token)\n",
        "                                continue\n",
        "                                split_list=[]\n",
        "                            else:\n",
        "                                split_list.append(token)\n",
        "                    #print(\"subseq end\")\n",
        "    for item in split_list:\n",
        "        #print(\"Item in split list: \",item)\n",
        "        if type(item) != list:\n",
        "            split_list.remove(item)\n",
        "        if item[0] =='': # replacing empty string with missing quote marks\n",
        "            item[0] = '\\\"'\n",
        "    #print(\"Resplitted sentence :\", split_list)\n",
        "    return split_list\n",
        "\n",
        "def clean(raw_data): ## input list is list of lists of strings \n",
        "    clean_data =[]  #list of lists that have one clean sentence per list\n",
        "    for sent in raw_data: # split by [] lines, supposedly a sentence line\n",
        "        one_sentence = [] #collects the new sentence if there has been need to resplit items\n",
        "        splitted= splitter(sent)\n",
        "        for item in splitted:\n",
        "            #print(item)\n",
        "            matchi = re.match(punct, item[0])\n",
        "            if matchi:\n",
        "                #print(\"collected sentence\")\n",
        "                one_sentence.append(item)\n",
        "                clean_data.append(one_sentence)\n",
        "                one_sentence=[]\n",
        "                break\n",
        "            else:\n",
        "                one_sentence.append(item)\n",
        "\n",
        "    return clean_data\n",
        "\n",
        "train_data_clean = clean(train_data_full)\n",
        "print(len(train_data_clean))\n",
        "for item in train_data_clean[:3]:\n",
        "    print(item)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50252\n",
            "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O'], ['.', 'O']]\n",
            "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O'], ['.', 'O']]\n",
            "[['In', 'O'], ['order', 'O'], ['to', 'O'], ['attract', 'O'], ['this', 'O'], ['group', 'O'], ['of', 'O'], ['seasoned', 'O'], ['adults', 'O'], ['pulling', 'O'], ['in', 'O'], ['over', 'O'], ['NT$', 'B-MONEY'], ['1', 'I-MONEY'], ['million', 'I-MONEY'], ['a', 'O'], ['year', 'O'], ['back', 'O'], ['to', 'O'], ['the', 'O'], ['ivory', 'O'], ['tower', 'O'], [',', 'O'], ['universities', 'O'], ['have', 'O'], ['begun', 'O'], ['to', 'O'], ['establish', 'O'], ['executive', 'O'], ['MBA', 'B-WORK_OF_ART'], ['(', 'O'], ['EMBA', 'B-WORK_OF_ART'], [')', 'O'], ['programs', 'O'], ['.', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJOw362Aw7Gq",
        "colab_type": "code",
        "outputId": "b478ca8b-2954-43e1-8c4e-e7d60554f0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# final check on the sentences\n",
        "item_lengths = []\n",
        "max_text = 0\n",
        "max_ind=0\n",
        "for item in train_data_clean:\n",
        "    item_lengths.append(len(item))\n",
        "    if len(item) > max_text:\n",
        "        max_text = len(item)\n",
        "        max_ind = train_data_clean.index(item)\n",
        "print(\"Longest sentence:\", max_text, \"index: \",max_ind)\n",
        "\n",
        "lengths_sorted = sorted(item_lengths, reverse=True)\n",
        "max = item_lengths.index(max_text)\n",
        "\n",
        "#pprint(train_data_clean[max])\n",
        "print(lengths_sorted[:300]) # longest sentences\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest sentence: 168 index:  8041\n",
            "[168, 145, 123, 121, 120, 108, 106, 106, 102, 101, 97, 96, 94, 94, 93, 93, 93, 92, 91, 91, 91, 90, 90, 90, 90, 90, 90, 89, 88, 88, 88, 87, 87, 86, 85, 85, 85, 85, 84, 84, 84, 84, 83, 83, 83, 83, 82, 82, 82, 82, 81, 81, 80, 80, 80, 79, 79, 79, 79, 79, 79, 78, 78, 78, 78, 78, 77, 77, 77, 77, 77, 77, 77, 77, 76, 76, 76, 76, 76, 76, 76, 76, 76, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-1cxTQd1t2q",
        "colab_type": "code",
        "outputId": "5591a320-e4cf-43eb-c653-a5f2ea91461d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# pop the longest\n",
        "train_data_clean.pop(max_ind)\n",
        "print(len(train_data_clean))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1B44CdNw-64",
        "colab_type": "code",
        "outputId": "62856dbf-ab78-4682-c4b8-79e4580ea540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "\n",
        "print('------------------------------------------')\n",
        "dev_data_clean = clean(dev_data_full)\n",
        "print(len(dev_data_clean))\n",
        "for item in dev_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')\n",
        "test_data_clean = clean(test_data_full)\n",
        "print(len(test_data_clean))\n",
        "for item in test_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')    "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------\n",
            "9954\n",
            "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON'], ['.', 'O']]\n",
            "[['On', 'O'], ['August', 'B-DATE'], ['25', 'I-DATE'], ['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['wrapped', 'O'], ['up', 'O'], ['his', 'O'], ['first', 'B-ORDINAL'], ['overseas', 'O'], ['trip', 'O'], ['since', 'O'], ['taking', 'O'], ['office', 'O'], [',', 'O'], ['swinging', 'O'], ['through', 'O'], ['three', 'B-CARDINAL'], ['countries', 'O'], ['in', 'O'], ['Latin', 'B-LOC'], ['America', 'I-LOC'], ['and', 'O'], ['another', 'O'], ['three', 'B-CARDINAL'], ['in', 'O'], ['Africa', 'B-LOC'], ['.', 'O']]\n",
            "[['While', 'O'], ['in', 'O'], ['the', 'B-GPE'], ['Dominican', 'I-GPE'], ['Republic', 'I-GPE'], ['to', 'O'], ['attend', 'O'], ['the', 'O'], ['inauguration', 'O'], ['of', 'O'], ['President', 'O'], ['Hipolito', 'B-PERSON'], ['Mejia', 'I-PERSON'], [',', 'O'], ['Chen', 'B-PERSON'], ['had', 'O'], ['a', 'O'], ['chance', 'O'], ['to', 'O'], ['meet', 'O'], ['with', 'O'], ['the', 'O'], ['leaders', 'O'], ['of', 'O'], ['many', 'O'], ['different', 'O'], ['nations', 'O'], ['.', 'O']]\n",
            "------------------------------------------\n",
            "7920\n",
            "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O'], ['.', 'O']]\n",
            "[['Worldwide', 'O'], [',', 'O'], ['biotechnology', 'O'], ['is', 'O'], ['a', 'O'], ['rising', 'O'], ['star', 'O'], ['of', 'O'], ['the', 'O'], ['industrial', 'O'], ['stage', 'O'], ['.', 'O']]\n",
            "[['In', 'O'], ['Taiwan', 'B-GPE'], [',', 'O'], ['in', 'O'], ['recent', 'O'], ['years', 'O'], ['the', 'B-ORG'], ['Ministry', 'I-ORG'], ['of', 'I-ORG'], ['Economic', 'I-ORG'], ['Affairs', 'I-ORG'], [',', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Science', 'I-ORG'], ['Council', 'I-ORG'], ['and', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Health', 'I-ORG'], ['Research', 'I-ORG'], ['Institutes', 'I-ORG'], ['have', 'O'], ['been', 'O'], ['strongly', 'O'], ['pursuing', 'O'], ['\"', 'O'], ['biochip', 'O'], ['\"', 'O'], ['research', 'O'], ['programs', 'O'], ['.', 'O']]\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZHhXzVTQA_P",
        "outputId": "b6435c95-8187-4b63-dffb-0ec3beac7c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for item in f: # list of lists (tokens)\n",
        "        #print(item)\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for token in item:\n",
        "            if len(token) ==2:\n",
        "                sent_text.append(token[0])\n",
        "                sent_tags.append(token[1])\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict['text'])\n",
        "        #print(sent_dict['tags'])\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "train_data_sent = list(reshape_sent2dicts(train_data_clean))\n",
        "samp = train_data_sent[:2]\n",
        "print(samp)\n",
        "print()\n",
        "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
        "samp2 = dev_data_sent[:3]\n",
        "print(samp2)\n",
        "test_data_sent = list(reshape_sent2dicts(test_data_clean))\n",
        "samp3 = test_data_sent[:3]\n",
        "print(samp3)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "\n",
            "[{'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']}, {'text': ['On', 'August', '25', 'President', 'Chen', 'Shui', '-', 'bian', 'wrapped', 'up', 'his', 'first', 'overseas', 'trip', 'since', 'taking', 'office', ',', 'swinging', 'through', 'three', 'countries', 'in', 'Latin', 'America', 'and', 'another', 'three', 'in', 'Africa', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-CARDINAL', 'O', 'B-LOC', 'O']}, {'text': ['While', 'in', 'the', 'Dominican', 'Republic', 'to', 'attend', 'the', 'inauguration', 'of', 'President', 'Hipolito', 'Mejia', ',', 'Chen', 'had', 'a', 'chance', 'to', 'meet', 'with', 'the', 'leaders', 'of', 'many', 'different', 'nations', '.'], 'tags': ['O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "[{'text': ['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['Worldwide', ',', 'biotechnology', 'is', 'a', 'rising', 'star', 'of', 'the', 'industrial', 'stage', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['In', 'Taiwan', ',', 'in', 'recent', 'years', 'the', 'Ministry', 'of', 'Economic', 'Affairs', ',', 'the', 'National', 'Science', 'Council', 'and', 'the', 'National', 'Health', 'Research', 'Institutes', 'have', 'been', 'strongly', 'pursuing', '\"', 'biochip', '\"', 'research', 'programs', '.'], 'tags': ['O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpYBQMbcQGBi",
        "outputId": "2b988a32-ebb7-43da-c1c1-b83c02640421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "random.seed(125)\n",
        "random.shuffle(train_data_sent)\n",
        "#max_sent = [max(len(i[\"text\"])) for i in train_data_sent]\n",
        "#print(max_sent)\n",
        "print(type(train_data_sent))\n",
        "print(train_data_sent[0]) ##one dict\n",
        "print()\n",
        "print(train_data_sent[0][\"text\"])\n",
        "print()\n",
        "print(train_data_sent[0][\"tags\"])\n",
        "print('------------')\n",
        "\n",
        "def typed_listing(data, key):\n",
        "    listed = []\n",
        "    max_length = 0\n",
        "    for item in data: # dictionary {text:\"\", tags:\"\"}\n",
        "        #print('Item: ', item)\n",
        "        #print('Key: ', key, ' content: ', item[key], 'length: ',len(item[key]))\n",
        "        if len(item[key]) > max_length:\n",
        "            max = len(item[key])\n",
        "        listed.append(item[key])\n",
        "    return listed, max_length\n",
        "\n",
        "listed_texts= typed_listing(train_data_sent, \"text\")\n",
        "train_texts = listed_texts[0]\n",
        "train_txt_max = listed_texts[1]\n",
        "listed_labels = typed_listing(train_data_sent, \"tags\")\n",
        "train_labels= listed_labels[0]\n",
        "train_lbl_max = listed_labels[1]\n",
        "print(train_txt_max)\n",
        "print(train_texts[0])\n",
        "print(train_labels[0])\n",
        "\n",
        "\n",
        "print('-----------------------------')\n",
        "print(len(train_texts))\n",
        "print('-----------------------')\n",
        "print('Text: ', train_texts[0])\n",
        "print(' Texts length: ',len(train_texts))\n",
        "print('Label: ', train_labels[0])\n",
        "print(' Labels length: ',len(train_labels))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "{'text': ['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.'], 'tags': ['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']}\n",
            "\n",
            "['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.']\n",
            "\n",
            "['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']\n",
            "------------\n",
            "0\n",
            "['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.']\n",
            "['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']\n",
            "-----------------------------\n",
            "50251\n",
            "-----------------------\n",
            "Text:  ['But', 'the', 'Bush', 'administration', '``', 'has', 'abdicated', 'that', 'obligation', ',', \"''\", 'says', 'Frelick', '.']\n",
            " Texts length:  50251\n",
            "Label:  ['O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'O']\n",
            " Labels length:  50251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQQRw0YO-Ng",
        "colab_type": "code",
        "outputId": "46cb4c29-7e69-41f7-c808-3f9a22cc5b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "## same for validation/dev data\n",
        "listed_texts_dev = typed_listing(dev_data_sent, \"text\")\n",
        "dev_texts = listed_texts_dev[0]\n",
        "dev_txt_max = listed_texts_dev[1]\n",
        "\n",
        "listed_labels_dev = typed_listing(dev_data_sent, \"tags\")\n",
        "dev_labels= listed_labels_dev[0]\n",
        "dev_lbl_max = listed_labels_dev[1]\n",
        "\n",
        "print('Text: ', dev_texts[0])\n",
        "print(' Texts length: ',len(dev_texts))\n",
        "print('Label: ', dev_labels[0])\n",
        "print(' Labels length: ',len(dev_labels))\n",
        "\n",
        "##and test data\n",
        "print(\"length of test data: \",len(test_data_sent))\n",
        "listed_texts_test = typed_listing(test_data_sent, \"text\")\n",
        "test_texts = listed_texts_test[0]\n",
        "test_txt_max = listed_texts_test[1]\n",
        "print(\"Length of test texts: \",test_txt_max) # for some reason colab gives zero?\n",
        "print(\"first item of text texts: \",test_texts[0])\n",
        "listed_labels_test = typed_listing(test_data_sent, \"tags\")\n",
        "test_labels= listed_labels_test[0]\n",
        "test_lbl_max = listed_labels_test[1]\n",
        "print(\"Length of test labels: \",test_lbl_max) # for some reason colab gives zero?\n",
        "print(test_texts[:2])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.']\n",
            " Texts length:  9954\n",
            "Label:  ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']\n",
            " Labels length:  9954\n",
            "length of test data:  7920\n",
            "Length of test texts:  0\n",
            "first item of text texts:  ['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71', '.']\n",
            "Length of test labels:  0\n",
            "[['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71', '.'], ['Worldwide', ',', 'biotechnology', 'is', 'a', 'rising', 'star', 'of', 'the', 'industrial', 'stage', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICn08fOgbyXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFv2qclTbyXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_C5ii8byXt",
        "colab_type": "code",
        "outputId": "96db86f4-5cf1-4242-96d6-8d310a8262c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdgjgOlbyXx",
        "colab_type": "code",
        "outputId": "58647cee-b3d6-4271-b15c-e9c2304792f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGaojUBhbyX2",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Ox5_ObyX3",
        "colab_type": "code",
        "outputId": "4def3a57-68cc-423b-e43c-799f47f9093c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#Labels\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in train_labels: #loops over sentences \n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 10,\n",
            " 'B-DATE': 2,\n",
            " 'B-EVENT': 11,\n",
            " 'B-FAC': 13,\n",
            " 'B-GPE': 1,\n",
            " 'B-LANGUAGE': 21,\n",
            " 'B-LAW': 6,\n",
            " 'B-LOC': 36,\n",
            " 'B-MONEY': 22,\n",
            " 'B-NORP': 8,\n",
            " 'B-ORDINAL': 33,\n",
            " 'B-ORG': 19,\n",
            " 'B-PERCENT': 30,\n",
            " 'B-PERSON': 17,\n",
            " 'B-PRODUCT': 4,\n",
            " 'B-QUANTITY': 16,\n",
            " 'B-TIME': 20,\n",
            " 'B-WORK_OF_ART': 0,\n",
            " 'I-CARDINAL': 9,\n",
            " 'I-DATE': 7,\n",
            " 'I-EVENT': 32,\n",
            " 'I-FAC': 34,\n",
            " 'I-GPE': 24,\n",
            " 'I-LANGUAGE': 15,\n",
            " 'I-LAW': 3,\n",
            " 'I-LOC': 23,\n",
            " 'I-MONEY': 5,\n",
            " 'I-NORP': 29,\n",
            " 'I-ORDINAL': 12,\n",
            " 'I-ORG': 35,\n",
            " 'I-PERCENT': 18,\n",
            " 'I-PERSON': 28,\n",
            " 'I-PRODUCT': 14,\n",
            " 'I-QUANTITY': 27,\n",
            " 'I-TIME': 31,\n",
            " 'I-WORK_OF_ART': 26,\n",
            " 'O': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k8DshceEaI",
        "colab_type": "code",
        "outputId": "4344a790-8805-420c-d28a-033d646441df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in train_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
        "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "pprint(vectorized_labels[0])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[25, 19, 35, 35, 25, 25, 25, 25, 25, 25, 25, 25, 17, 25]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtqLdCMPf3X",
        "colab_type": "code",
        "outputId": "5b8258a6-69ce-49c0-a54b-c1450343b1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "## vectorization of the texts\n",
        "def text_vectorizer(vocab, train_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(train_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
        "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "test_vectorized_data, test_lengths = text_vectorizer(vocabulary,test_texts)\n",
        "\n",
        "pprint(train_texts[0])\n",
        "pprint(vectorized_data[0])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['But',\n",
            " 'the',\n",
            " 'Bush',\n",
            " 'administration',\n",
            " '``',\n",
            " 'has',\n",
            " 'abdicated',\n",
            " 'that',\n",
            " 'obligation',\n",
            " ',',\n",
            " \"''\",\n",
            " 'says',\n",
            " 'Frelick',\n",
            " '.']\n",
            "[129, 3, 1417, 1132, 1, 45, 1, 13, 7820, 2, 1, 208, 1, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6FH5F1QGrq",
        "colab_type": "code",
        "outputId": "7832b945-79b0-49d7-d9ec-f15350ce6838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "arr_lengths = numpy.array(lengths)\n",
        "max_len = numpy.max(arr_lengths)\n",
        "print(max_len)\n",
        "\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max_len)\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max_len), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])\n",
        "\n",
        "# Same stuff for the validation data\n",
        "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max_len)\n",
        "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post',maxlen=max_len), -1)\n",
        "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
        "validation_weights[validation_weights > 0] = 1\n",
        "\n",
        "#and for test data\n",
        "test_vectorized_data_padded=pad_sequences(test_vectorized_data, padding='post', maxlen=max_len)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "145\n",
            "Old shape: (50251,)\n",
            "New shape: (50251, 145)\n",
            "First example:\n",
            "[ 129    3 1417 1132    1   45    1   13 7820    2    1  208    1    4\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "Padded labels shape: (50251, 145, 1)\n",
            "{'B-CARDINAL': 10,\n",
            " 'B-DATE': 2,\n",
            " 'B-EVENT': 11,\n",
            " 'B-FAC': 13,\n",
            " 'B-GPE': 1,\n",
            " 'B-LANGUAGE': 21,\n",
            " 'B-LAW': 6,\n",
            " 'B-LOC': 36,\n",
            " 'B-MONEY': 22,\n",
            " 'B-NORP': 8,\n",
            " 'B-ORDINAL': 33,\n",
            " 'B-ORG': 19,\n",
            " 'B-PERCENT': 30,\n",
            " 'B-PERSON': 17,\n",
            " 'B-PRODUCT': 4,\n",
            " 'B-QUANTITY': 16,\n",
            " 'B-TIME': 20,\n",
            " 'B-WORK_OF_ART': 0,\n",
            " 'I-CARDINAL': 9,\n",
            " 'I-DATE': 7,\n",
            " 'I-EVENT': 32,\n",
            " 'I-FAC': 34,\n",
            " 'I-GPE': 24,\n",
            " 'I-LANGUAGE': 15,\n",
            " 'I-LAW': 3,\n",
            " 'I-LOC': 23,\n",
            " 'I-MONEY': 5,\n",
            " 'I-NORP': 29,\n",
            " 'I-ORDINAL': 12,\n",
            " 'I-ORG': 35,\n",
            " 'I-PERCENT': 18,\n",
            " 'I-PERSON': 28,\n",
            " 'I-PRODUCT': 14,\n",
            " 'I-QUANTITY': 27,\n",
            " 'I-TIME': 31,\n",
            " 'I-WORK_OF_ART': 26,\n",
            " 'O': 25}\n",
            "First example labels:\n",
            "array([[25],\n",
            "       [19],\n",
            "       [35],\n",
            "       [35],\n",
            "       [25],\n",
            "       [25],\n",
            "       [25],\n",
            "       [25],\n",
            "       [25],\n",
            "       [25],\n",
            "       [25],\n",
            "       [25],\n",
            "       [17],\n",
            "       [25],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0]], dtype=int32)\n",
            "First weight vector:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmC32aOqM7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function 1, token level\n",
        "import keras\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def accuracy(predictions, gold, lengths):\n",
        "    pred_tags = numpy.concatenate([labels[:lengths[i]] for i, labels in enumerate(predictions)]).ravel()\n",
        "    \n",
        "    gold_tags = numpy.concatenate([labels[:lengths[i], 0] for i, labels in enumerate(gold)]).ravel()\n",
        "    accuracy = accuracy_score(gold_tags, pred_tags)\n",
        "    print('Accuracy:', accuracy )\n",
        "    return accuracy\n",
        "#local variables\n",
        "valid_VD_pad = validation_vectorized_data_padded\n",
        "valid_VL_pad = validation_vectorized_labels_padded\n",
        "valid_lengths = validation_lengths\n",
        "\n",
        "class EvaluateTags(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.accuracy = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        pred = numpy.argmax(self.model.predict(valid_VD_pad), axis=-1)\n",
        "        eval_parameter = accuracy(pred, valid_VL_pad, valid_lengths)\n",
        "        self.accuracy.append(eval_parameter)\n",
        "        return  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAOVAbBTRAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function 2, entities\n",
        "\n",
        "def _convert_to_entities(input_sequence):\n",
        "    \"\"\"\n",
        "    Reads a sequence of tags and converts them into a set of entities.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    previous_tag = label_map['O']\n",
        "    for i, tag in enumerate(input_sequence):\n",
        "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "            current_entity.append((tag, i))\n",
        "        elif tag == label_map['O']: # Entity has ended\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "        elif tag == previous_tag: # Current entity continues\n",
        "            current_entity.append((tag, i))\n",
        "        previous_tag = tag\n",
        "    \n",
        "    # Add the last entity to our entity list if the sentences ends with an entity\n",
        "    if len(current_entity) > 0:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    entity_offsets = set()\n",
        "    \n",
        "    for e in entities:\n",
        "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
        "    return entity_offsets\n",
        "\n",
        "def _entity_level_PRF(predictions, gold, lengths):\n",
        "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
        "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
        "    \n",
        "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
        "    pred_count = sum([len(e) for e in pred_entities])\n",
        "    \n",
        "    try:\n",
        "        precision = tp / pred_count # tp / (tp+np)\n",
        "        recall = tp / sum([len(e) for e in gold_entities])\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    except Exception as e:\n",
        "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
        "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
        "    return precision, recall, fscore             \n",
        "\n",
        "def evaluate(predictions, gold, lengths):\n",
        "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
        "    return precision, recall, fscore\n",
        "\n",
        "class EvaluateEntities(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.precision = []\n",
        "        self.recall = []\n",
        "        self.fscore = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
        "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
        "        self.precision.append(evaluation_parameters[0])\n",
        "        self.recall.append(evaluation_parameters[1])\n",
        "        self.fscore.append(evaluation_parameters[2])\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ydCexfTg5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model, basic with relu and Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model1 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)\n",
        "\n",
        "print(model1.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIPQrLXUVrWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "optimizer=Adam(lr=0.007) # define the learning rate\n",
        "model1.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function1=EvaluateEntities()\n",
        "evaluation_function2=EvaluateTags()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model1.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function1, evaluation_function2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PovjRyU3AWP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "#save model and init weights\n",
        "model1.save('model1.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bah0URSVV5GP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores,precision,accuracy):\n",
        "    f_arr = numpy.array(fscores)\n",
        "    p_arr = numpy.array(precision)\n",
        "    a_arr = numpy.array(accuracy)\n",
        "    print(\"HISTORY:\")\n",
        "    print(\"Entity level\")\n",
        "    print(\"F-scores, entity level: \",fscores)\n",
        "    print(\"Highest f-score:\", numpy.max(f_arr))\n",
        "    print(\"Precision: \",precision)\n",
        "    print(\"Highest precision:\", numpy.max(p_arr))\n",
        "    print('-----------------------------')\n",
        "    print(\"Token level\")\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"Highest precision:\", numpy.max(a_arr))\n",
        "    \n",
        "    fig,ax1 =plt.subplots()\n",
        "    \n",
        "    # entity level\n",
        "    color1 = 'tab:blue'\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('Entity level evaluation', color= color1 )\n",
        "    ax1.plot(fscores, label= 'f-score', color= color1)\n",
        "    color2 = 'tab:green'\n",
        "    ax1.plot(precision, label= 'precision', color= color2)\n",
        "    ax1.set_ylim([0.300,0.70])\n",
        "    # token level\n",
        "    ax2 = ax1.twinx()\n",
        "    color3 = 'tab:red'\n",
        "    ax2.set_ylabel('Token level evaluation', color = color3)\n",
        "    ax2.plot(accuracy, label= 'accuracy', color=color3, linestyle= '--')\n",
        "    ax2.tick_params(axis='y', labelcolor=color3)\n",
        "    ax2.set_ylim([0.70,1.00])\n",
        "    fig.legend(bbox_to_anchor=(0.,1.02,1., .102),loc='lower left', borderaxespad=0.)\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function1.fscore, evaluation_function1.precision, evaluation_function2.accuracy)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BimErOSrMhqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prediction on test data\n",
        "m1_predictions = model1.predict(test_vectorized_data_padded)\n",
        "\n",
        "\n",
        "pred_labels = numpy.argmax(m1_predictions, axis=-1)    #\n",
        "pprint(pred_labels[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx2oFySvS1E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#return to tags for the test data ### DOESN*T WORK!! LOOK FROM TATJANA'S?\n",
        "pred_labels.tolist()\n",
        "predicted_tags_test = []\n",
        "class_numbers = list(label_map.values())\n",
        "class_keys = list(label_map.keys())\n",
        "for item in pred_labels:\n",
        "    pred_tags_sent =[]\n",
        "    #print(item)\n",
        "    for i in range(len(item)):\n",
        "        pred_tags_sent.append(class_keys[class_numbers.index(item[i])])\n",
        "    predicted_tags_test.append(pred_tags_sent)\n",
        "#test\n",
        "print(predicted_tags_test[0])\n",
        "print(test_texts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZsm9armCG4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 2 with sigmoid activation and Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed, LSTM, Bidirectional\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings)\n",
        "    outp = Dense(class_count, activation=\"softmax\")(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model2 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)\n",
        "\n",
        "print(model2.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csFaCX7eCDGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train another model\n",
        "optimizer=Adam(lr=0.001) # define the learning rate\n",
        "model2.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "evaluation_function2=EvaluateTags()\n",
        "\n",
        "# train\n",
        "model2_hist=model2.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function1, evaluation_function2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwI5fOALIuwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model and init weights\n",
        "model2.save('model2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg-s7U7eCQXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the evaluation \n",
        "\n",
        "plot_history(evaluation_function1.fscore, evaluation_function1.precision, evaluation_function2.accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMB5SeuXC2j2",
        "colab_type": "code",
        "outputId": "ae3fa826-1449-4671-bba0-a9e0ff5ae28a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# model 3 with exponential activation and Adamax\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, Adamax, Adadelta\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model3 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)\n",
        "\n",
        "print(model3.summary())"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        (None, 145)               0         \n",
            "_________________________________________________________________\n",
            "embedding_12 (Embedding)     (None, 145, 300)          15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, 145, 100)          30100     \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 145, 37)           3737      \n",
            "=================================================================\n",
            "Total params: 15,034,437\n",
            "Trainable params: 33,837\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "321odNcHGPWs",
        "colab_type": "code",
        "outputId": "1c1e8adb-6ffe-4b92-8289-0e2c703082bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# train third model\n",
        "optimizer=Adadelta(0.07) # define the learning rate, if applicable\n",
        "model3.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function1=EvaluateEntities()\n",
        "evaluation_function2=EvaluateTags()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model3.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function1, evaluation_function2])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 2s - loss: 0.2352\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 2/10\n",
            " - 2s - loss: 0.0570\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.0454\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.0445\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.0440\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.0437\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.0435\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 8/10\n",
            " - 2s - loss: 0.0434\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 9/10\n",
            " - 2s - loss: 0.0432\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n",
            "Epoch 10/10\n",
            " - 2s - loss: 0.0432\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Accuracy: 0.8602711772174859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1G8Ikm4Ix_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model and init weights\n",
        "model3.save('model3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y22OKBRGR5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the evaluation scores\n",
        "\n",
        "\n",
        "plot_history(evaluation_function1.fscore, evaluation_function1.precision, evaluation_function2.accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwMwwo9-MPv4"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9pXRewbyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model for context with rnn\n",
        "\n",
        "from keras.layers import LSTM, Dropout, Conv1D\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "rnn_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    rnn = Bidirectional(LSTM(rnn_size, return_sequences=True, recurrent_dropout=0.1))(embeddings)\n",
        "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP-kwoNXUMb",
        "colab_type": "code",
        "outputId": "9127043d-dced-482e-9fa6-7ab77928f2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "\n",
        "print(rnn_model.summary())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        (None, 145)               0         \n",
            "_________________________________________________________________\n",
            "embedding_11 (Embedding)     (None, 145, 300)          15000600  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 145, 200)          320800    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 145, 37)           7437      \n",
            "=================================================================\n",
            "Total params: 15,328,837\n",
            "Trainable params: 328,237\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIal9meVXnN_",
        "colab_type": "code",
        "outputId": "4459ed33-e1cf-4ea0-ccea-fa667c4772f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "\n",
        "optimizer=Adam(lr=0.01) # define the learning rate\n",
        "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "\n",
        "evaluation_function=EvaluateEntities()\n",
        "#evaluation_function2=EvaluateTags()\n",
        "\n",
        "# train\n",
        "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=6, callbacks=[evaluation_function])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            " - 461s - loss: 0.0064\n",
            "\n",
            "Precision/Recall/F-score: 0.6615701415701416 / 0.5318021932547072 / 0.5896306492314751\n",
            "Epoch 2/6\n",
            " - 452s - loss: 0.0058\n",
            "\n",
            "Precision/Recall/F-score: 0.693318991275248 / 0.4801158700600041 / 0.5673488349348395\n",
            "Epoch 3/6\n",
            " - 449s - loss: 0.0053\n",
            "\n",
            "Precision/Recall/F-score: 0.6667663924208427 / 0.5533623008483344 / 0.60479421076436\n",
            "Epoch 4/6\n",
            " - 450s - loss: 0.0049\n",
            "\n",
            "Precision/Recall/F-score: 0.695542373876784 / 0.49008897165321746 / 0.5750145659351331\n",
            "Epoch 5/6\n",
            " - 457s - loss: 0.0045\n",
            "\n",
            "Precision/Recall/F-score: 0.6784404150508333 / 0.5357334988619905 / 0.5987004878951141\n",
            "Epoch 6/6\n",
            " - 456s - loss: 0.0041\n",
            "\n",
            "Precision/Recall/F-score: 0.6851471224578105 / 0.5241878750258638 / 0.5939558764916887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKNs4t8X3Ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "2b30ea8d-2cb6-424e-94ae-c1cfb94fff94"
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_history(evaluation_function.fscore, evaluation_function.precision, evaluation_function.recall) \n",
        "## recall doesn't show, it's just a filler param so I didn't have to write the function again\n",
        "## also, ignore the red axis"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HISTORY:\n",
            "Entity level\n",
            "F-scores, entity level:  [0.5896306492314751, 0.5673488349348395, 0.60479421076436, 0.5750145659351331, 0.5987004878951141, 0.5939558764916887]\n",
            "Highest f-score: 0.60479421076436\n",
            "Precision:  [0.6615701415701416, 0.693318991275248, 0.6667663924208427, 0.695542373876784, 0.6784404150508333, 0.6851471224578105]\n",
            "Highest precision: 0.695542373876784\n",
            "-----------------------------\n",
            "Token level\n",
            "Accuracy:  [0.5318021932547072, 0.4801158700600041, 0.5533623008483344, 0.49008897165321746, 0.5357334988619905, 0.5241878750258638]\n",
            "Highest precision: 0.5533623008483344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAFZCAYAAAASMsFjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzde3xV1Zn4/8/KjSQQSIAESAKES5K9gygBBFEQpV6wqXihrWirtdP5MtJa+7Nj2zjjzFjUNnV02mpvUEvrtFbGVmvR6FidAFZEJNxE2BsEPEJCuOYCJORykvX7Y+9zPAlJSGCHnJM879crr5x9Xyfoec6z9rPXUlprhBBCiEgW1dsNEEIIIc6XBDMhhBARL2bTpk1pMTExzwAXIcGtu1qAD/1+/z9OmzbtSG83Rggh+quYmJiYZ0aOHGmmpqZWRUVFyQ20bmhpaVFHjx7NO3To0DPAgt5ujxBC9FdRwEWpqaknJJB1X1RUlE5NTa3ByWqFEEL0kiggSgLZuXP/dtI9K4QQvUg+hIUQQkS8sAhmjz76aNr48eMnLViwYFxvt0UIIUTkientBgD85je/SX3rrbd2T5gwoamnrtHU1ERsbGxPnV4IIUQv6vXM7I477hhTVlY24IYbbsj+/ve/nxZYX1paGj958mTTMIy8nJycvO3btw8A+NnPfjYsJycnLzc3N+/mm28eB7Br1664yy67LCcnJydv1qxZOR999FEcwMKFC7PuuOOOMRdffLGxZMmSzB07dgyYM2dO9qRJk8xp06blbtmyJb533rUQQggvtcrMvvPnbaN3HzqZ6OUFckYm1f3n5y850NH2P/7xj/vXrl07ZO3atbtHjRrlD6x/+umnU7/+9a8fXrJkSWV9fb3y+/2UlpbGP/HEE6PWr19vjxo1yn/48OFogCVLloz50pe+dPyb3/zm8Z/85CfDlixZMvqtt97aC1BRURG3efNmOyYmhlmzZuUsX778k8mTJzeUlJQMXLJkyZj33ntvt5fvVwghxIUXFt2M7Zk1a1btE088MaqsrCxu0aJFVZMnT2544403Bt94441VgaA3YsSIZoAtW7YMfP311/cCLFmypPL73/9+ZuA8t956a1VMTAw1NTVRW7ZsGfSFL3xhQmBbY2OjutDvSwghhPdaBbPOMqie9vrrryc9/vjjowCWL1/uu+eeeyrnzJlT+5e//GXI5z73ueynn376k3M576BBg1oAmpubSUpK8tu2vdPLdgshhOh9vX7PLOCGG244adv2Ttu2d1555ZV1O3fujDNNs+Ghhx46cv3111dv3bo14frrrz/xyiuvpBw6dCgaINDNmJ+fX/vMM8+kACxbtmzo9OnTT7U9/9ChQ1syMzMbV6xYkQLQ0tLC+vXrEy7kexRCCNEzwiaYtfWHP/xhaE5OziTDMPIsy0r4p3/6p+PTp0+v/+d//ueKOXPmGLm5uXlf//rXRwP86le/2v/73/9+eE5OTt7zzz8/7Be/+EW7Gebzzz+/77e//e3w3NzcvOzs7Ekvvvhi8oV9V0IIIXqC2rp1q++SSy451tsNiWTbtm0bfskll2T1djuEEKK/CtvMTAghhOgqCWZCCCEingQzIYQQEU+CmRBCiIgnwUwIIUTEk2AmhBAi4vXZYPb2228n3n333aM72u7z+WLnz58//kK2SQghRM+ImOfM/H4/MTHhOZSkPGcmhBC9Kywys127dsWNGzdu0oIFC8aNHz9+0vz588efPHkyKiMjY/KSJUsy8vLyzBUrVqS89NJLg6dMmWLk5eWZN9xww/iampoogLVr1ybm5+cbubm5eZMnTzarqqqiXn311aSrr756IkBxcfEgwzDyDMPIM00zr6qqKmrXrl1x2dnZkwDq6urU5z//+aycnJw80zTzXnnllSSAp556ath11103Yc6cOdljx4696J577sns+F0IIYToLa1SnX9b92+j91Tt8XQKmIkpE+seueKRsw5g7PP54pctW+a77rrrar/whS9k/ed//mcqwLBhw/w7d+60KioqYm688cYJb7/99u7Bgwe3/Ou//uvIRx55ZMSjjz566Etf+tKE5557bu/cuXPrKisrowKDCwc8+eSTI5966qlPrrvuutqampqoxMTEliNHjgS3/+hHP0pTSrF79+6dW7Zsif/sZz+bvXfv3g8Bdu7cmbht27adCQkJLRMnTrzogQceODxx4sQem0RUCCFE94VFZgYwcuTIxuuuu64W4M477zz+7rvvDgK46667qgDWrFkzcO/evfEzZswwDMPIW7ly5bD9+/fHffDBB/FpaWlNc+fOrQNnQOG2M0pfdtllpx544IHRjz76aNqxY8ei225/9913B915553HAfLz8+vT09Mbt2/fHg8we/bsE8OGDWtOTEzUEydOrN+7d++Anv5bCCGE6J5WmVlXMqieopRqdzkpKakFQGvN7NmzT7zyyisfh+73/vvvn3Xk+x/84AeHbr755pq//vWvQ+bMmWMUFxd/lJiY2HK24wDi4uJ04HV0dLRuamqSOdCEECLMhE1mVlFREffWW28NBHjuueeGXn755a2mcbnqqqtqS0tLB3344YcDAE6cOBH1wQcfDLj44ovrjxw5Ert27dpEgKqqqqimpta9gDt27BgwY8aM04899tihiy++uPbDDz+MD91+xRVXnPrDH/4wFOCDDz4YUFFREXfxxRfX9+DbFUII4aGwCWZZWVn1Tz/9dNr48eMnVVdXxzzwwANHQ7enp6f7ly1b5lu0aNH4nJycvOnTpxvbt2+Pj4+P188999ze++67b0xubm7eVVddlVNXV9fqfT3++ONp2dnZk3JycvJiY2P15z//+ZrQ7d/97nePtLS0qJycnLzbbrttwrJly3wJCQkaIYQQESEsSvN37doV97nPfS77o48+2tGb7ThXUpovhBC9K2wyMyGEEOJchUUwy83NbYzUrEwIIUTvC4tgJoQQQpwPCWZCCCEingQzIYQQEU+CmRBCiIgnwUwIIUTE61fBrO3IIEIIIfqGsAlm11xzzYRJkyaZEydOnPTEE08MB/jzn/88OC8vz8zNzc2bNWtWDkBNTU1UYLqWnJycvN/97nfJAImJifmBc/32t79NWbhwYRbAwoULs+64444xF198sbFkyZLM1atXJ06ZMsUwTTMvPz/f2LZt2wBw5ktbvHhxZmCkkMceeyxt1apVSddcc82EwHn/8pe/DL722msnIIQQIqycMdvlx1/4Ym7bdUnXXlM5fPHioy21tVGf3P3V7Lbbhyy48djQO+887j96NObA17/R6sN+3J9e2NWVhjz33HO+ESNGNJ86dUrl5+fn3XbbbdX33ntv1po1a2zDMBoPHz4cDVBYWDhq8ODBzbt3794JcPTo0eiznbuioiJu8+bNdkxMDJWVlVEbN260Y2Njefnll5O++93vZr7xxht7n3zyydT9+/fH7dy5c0dsbCyHDx+OTk1Nbf7Wt7415uDBgzHp6en+FStWDPvqV78a9hOZCiFEfxM2Uzf/6Ec/GlFcXJwMcOjQodinnnoqdcaMGScNw2gEGDFiRDPA22+/PXjlypX7AselpqY2n+3ct956a1VglurKysro2267bZzP54tXSgVHwS8pKRl8zz33HA1MDxO43he/+MXjv/71r4d+4xvfOL558+ZBL7300scdXUcIIUTvOCOYdZZJRQ0c2NLZ9pjUVH9XM7FQr776atLatWuTSktL7aSkpJYZM2bk5ufn1+3atSv+7Ec7QqeQOX36dKtpWkIn6/ze976XMXfu3JNvvvnm3l27dsXNmzfvjEw01JIlS44XFBRMjI+P1zfeeGNV27nQhBBC9L6wuGdWXV0dPWTIkOakpKSWLVu2xG/btm1gfX191Pvvv59k23YcQKCbce7cuSd+/OMfpwWODXQzDhs2rGnz5s3xzc3N/PWvf03p6FonTpyIzszMbARYtmzZ8MD6z3zmMyeWLVs2PFAkErheVlZW04gRI5qefPLJUYsXL5YuRiGECENhEcwWLlxY4/f71fjx4yd95zvfybjkkktq09LS/E899ZTvlltumZibm5t3yy23jAf44Q9/WFFdXR2dnZ09KTc3N++1115LAvj+979fftNNN02cOnWqMWLEiA7LFr/3ve8devjhhzNN08zz+/3B9ffff//RzMzMRsMwJuXm5ub95je/GRrYtmjRouOjRo1qnDp1qsxxJoQQYSgspoAJd3fdddeY/Pz8uvvvv7/dv5NMASOEEL0rbApAwtWkSZPMhISElmXLlh3o7bYIIYRonwSzs9ixY4fV220QQgjRuSigpaWlRZ11T9Eu92/XctYdhRBC9Jgo4MOjR48OkYDWfS0tLero0aNDgA97uy1CCNGfxfj9/n88dOjQM4cOHbqIMKlujCAtwId+v/8fe7shQgjRnymtdW+3QQghhDgvkokJIYSIeBLMhBBCRDwJZkIIISKeBDMhhBART4KZEEKIiCfBTAghRMSTYCaEECLiSTATQggR8SSYCSGEiHgSzIQQQkQ8CWZCCCEingQzIYQQEU+CmRBCiIgnwUwIIUTEk2AmhBAi4kkwE0IIEfEkmAkhhIh4EsyEEEJEPAlmQgghIl5MbzcgQCk1H/gpEA08o7UuarP9x8DV7mIikKa1Tna3fQV4yN32qNb62QvTaiGE6Nssw1wBfA44YtrWRe1sVzif3Z8F6oC7Tdva7G5r9dls2laPfTaHRWamlIoGfg7cAOQBtyul8kL30Vrfr7WeorWeAjwNvOQeOxT4D2AmMAP4D6VUyoVsvxBC9GG/A+Z3sv0GINv9WQz8EsAyzDM+my3D7LHP5rAIZjhvdI/Wep/WuhFYCdzUyf63A8+7r68H3tRaV2qtq4A36fwPL4QQootM23obqOxkl5uA/zZtS5u29R6QbBnmKNzPZtO2Kk3b6vHP5nDpZswADoQsl+FE8zMopcYC44CSTo7NaOe4xTjfGgCmJSYmnmeThRAi8tXV1Wlgc8iq5Vrr5d04RUefwV36bPZKuASz7lgE/Flr3dydg9x/nOUAAwcO1LW1tT3RNiGEiChKqdNa6+m93Y7zFS7BrBwYHbKc6a5rzyLgG22OvarNsWs8bJsQEa1Ft2BX2rxT/g6bDm8ieUAyE5MnBn8ykjKIUuFyx0FEoI4+vy/oZ3O4BLONQLZSahzOH2ARcEfbnZRSBpACrA9Z/Qbwg5Cij+uAB3u2uX3bkboj/L3s7yTEJDBj1AyGJwzv7SaJbqqqr2L9wfW8U/4O6w6uo7LeueWRnZLNvpp9vPbxa8F946PjGTdkHBOTJzIheULwd/qgdAlyoitWAfdahrkS5/ZQjWlbFZZhvgH8IKToo0c/m8MimGmt/Uqpe3ECUzSwQmu9Qym1FCjVWq9yd10ErNRa65BjK5VSj+AERIClWuvOblaKNrTWfFzzMSUHSijZX8L2Y9tbbZ+YPJGZo2Yyc+RMpo+cTlJcUi+1VHSkuaWZD49/yLrydawrX8f2Y9vRaJIHJHN5+uXMzpjNrPRZwS8mpxpPsbdmL3ur97Kneg97q/ey4dAGXtn3SvCcCTEJjB8yvlWAm5g8kVEDR6GU6q23Ki4wyzCfx8mwhluGWYZToRgLYNrWr4DXcMry9+CU5n/V3VZpGWarz2bTtnrss1mFxIV+Q+6ZOV1PHxz9gJIDJazevxrfCR8Ak4dPZt6YeVyVeRUNzQ28V/EeGyo2sOXIFuqb64lSUVw07CJmjJrBzFEzmZI6hfiY+N59M/3UsdPHgsHr3Yp3qWmocf59hl/E7IzZzE6fTd6wPKKjort8zhONJ9hXvS8Y4AK/j54+GtwnMSaRCckTzghyIxJHSJCLQEqpOq31wN5ux/mSYNaPNDQ3sKFiAyX7S1hzYA3H648TExXDzJEznQA2+irSEtPaPbaxuZFtR7exoWIDGyo2sP3Ydpp1M3FRceSn5QeD26Rhk4iJCouEv89pamnig6MfOF2H5euwKi0AhsUP44qMK5iTMYfLRl1Gcnyy59euaahpFdwCr4/XHw/uMyh2EOOTxzsBbsingS4tMU2CXBiTYBbB+lMwq2mo4e/lf6dkfwnrytdR569jYOxA5mTMYd6YeczOmH1O3Ya1TbVsOrwpGNx2Ve0CnA+06SOmM3PUTGaMmkF2crZ8kJ2HQ7WHWFe+jnfK3+G9ivc41XSKaBXNlLQpzM6YzRXpV5A7NLfX7m1V11e3zuLcrsvAPTqApNikdjO54QnD5b+NMCDBLIL19WB2qPYQqw+spmR/CaWHSvFrP6kJqVw9+mrmjZnHpSMvJS46ztNrVtZX8v6h93m/4n02VGxg/8n9AAyNH8rMkTODwW100uiznKl/a2xuZPORzcEAtqd6DwAjEkc4XYcZs5k5ambY37c8fvp4q0wuEOhqGmqC+wyOGxwMbqGBblj8sH4f5LTWnPaf5kTjCeen4QQnG09+uuyuC7w+2XiSojlFpA9K7/a1JJhFsL4WzLTW7KneEwxgO47vAGDckHHMGz2PeWPmcdHwiy7ot/eKUxXO/bZDG3i/4v3gPZeMQRnBYhKplHSUnSwLdh1uOLSB0/7TxEbFMnXEVOZkzOGK9CuYkDwh4j/gtdYcrz9+xv24PdV7ONl4Mrhf8oDkM7K4CckTGBo/tBdb331aa2qbas8IPoGgVNNQ0yo4nWxoHaz8Lf5Oz58Um8TgAYMZHOf8/Pusf2fM4DHdbqcEswjWF4JZc0sz245uo2R/CSUHSjhw0nnQ/pLUS5g3Zh5Xj76acUPG9XIrHYFqyUAxycbDG4MfXv2xUrLeX0/p4dJgAAsU32QMyghmXzNGziAxtn+MUqO15ujpo2fcj9tbvZdTTaeC+w2NH+pkcUNaB7qeuEcY0KJbWmdEbQJSaHbUNlM62XiS5k7GdohSUSTFJQWD0eC4wcHgFFw/4Mxtg+MGMyh2ULcKezojwSyCRWowq/fX817Fe6w+sJo1B9ZQWV9JbFQsM0fNDFYgpiam9nYzz6q5pRm70m63UnLSsElOcOtDlZJaa3wnfMGuw9LDpTQ0NzAgegCXjrw0eO9r7OCxEZ99eUlrzeG6w2cUnuyt2Utt06f//w6LH9Zud+WQAUMA8Lf4Oww+J5tOdhiQTjSe4FTjKTQdf0bGqJhWQSZpQJvg1ElASoxNDIvn+CSYRbBICmY1DTW8Xfa2U8BxcB2n/adJik1iTuanBRwDYyP7v8POKiWnpE0JBrdIqpSsa6pjQ8UG1h10Alj5KWdAm6zBWcHsa9qIaX0iWF9oWmsO1R46o7tyb81eTvtPB/dLGZBCQ3MDdf66Ts8XFxXXYcA5W4aUEJMQ8V9AJJhFsHAPZhWnKoLPf5UeLqVZN5OWmPZpAceIS4mNju3tZvaYs1VKBh4DCKdKycB9y0DX4aYjm/C3+EmISWDmqJnMyZjD5emXk5mU2dtN7bNadAsVtRXBAHfg5AHio+M7DUZJcUn9/guFBLMIFm7BTGvN7qrdwQAWeH5oYvLEYADLG5YXFl0SveFslZKB4HahKyVPNJ5gQ8UG3il/h3fK3+FI3RHAGTJqdvpsrsi4gqlpU/v0Fw8R+SSYRbBwCGb+Fj9bjmwJViCWnypHoZiSNoV5o+dx9ZirGTt4bK+2MVydrVJyxkgnuHldKRkYsDdw72vb0W0062aSYpO4LP0yZmfM5vL0yxk5cKSn1xWiJ0kwi2C9FcxO+0+z/uB6SvaXsLZsLdUN1cRFxTErfRbzxszjyswrpVS9m3q6UrK6vpp3D77LuoPOsFGBES/MoWbw3tfk1MnERkn2JSKTBLMIdiGDWVV9FWvL1lKyv4T1B9dT31xPUlwSczPnMm/MPK5Iv6LflGBfCOdbKdnc0syO4zuC974CA/YOGTAkOGDv5emXy5cO0WdIMItgPR3Myk6WBbsPNx/ZTItuYeTAkcHuw2kjpsk3+QukK5WS00ZMo/xUOe+UvRMcsFehmJw6OXjva9KwSZ491yNEOJFgFsG8DmZaa+xKO1jAEai+y07JDo7AYQ41w6byrj/rqFISPh2wd3bGbGaNmtWjD+MKES4kmEUwL4KZv8XP5sObg3OAVdRWEKWiyE/LD2ZgMg5h+Kusr2Trka2MGjiqVwfsFaK3SDCLYOcazOqa6nj34LusPrCatWVrqWmoYUD0AKeAY/Q85o6eG3Hjxwkh+re+EswiYziFMPHl17/MR1UfMWTAkGABx6xRs6SAQwgheplkZt1Qsr+EpLgk8tPyI2ZYJSGE6ExfycwkmAkhRD/WV4KZ3O0WQggR8cImmCml5iuldiml9iilCjvY54tKqZ1KqR1KqT+GrG9WSm11f1ZduFYLIYQIB2Fx40cpFQ38HLgWKAM2KqVWaa13huyTDTwIXKG1rlJKpYWc4rTWesoFbbQQQoiwES6Z2Qxgj9Z6n9a6EVgJ3NRmn/8H/FxrXQWgtT5ygdsohBAiTIVLMMsADoQsl7nrQuUAOUqpdUqp95RS80O2xSulSt31N7d3AaXUYnefUr/f723rhRBC9Kqw6GbsohggG7gKyATeVkpN1lpXA2O11uVKqfFAiVJqu9Z6b+jBWuvlwHJwqhkvbNOFEEL0pHDJzMqB0LGfMt11ocqAVVrrJq31x8BunOCG1rrc/b0PWAPk93SDhRBChI9wCWYbgWyl1DilVBywCGhblfgyTlaGUmo4TrfjPqVUilJqQMj6K4CdCCGE6DfCoptRa+1XSt0LvAFEAyu01juUUkuBUq31KnfbdUqpnUAz8B2t9XGl1OXAMqVUC05wLgqtghRCCNH3yQggQgjRj/WVEUDCIjMTQggRvizDnA/8FKfn7BnTtorabB8LrABSgUrgy6ZtlbnbmoHt7q77Tdta0BNtlGAmhBCiQ5ZhnjGohWWYq0zbCr2d8wTw36ZtPWsZ5jzgh8Cd7rbTpm31+KAW4VIAIoQQIjzNAPaYtrXPtK2OBrXIA0rc16vb2d7jJJgJIYToTFcGtdgG3Oq+vgVIsgxzmLscbxlmqWWY71mG2e6gFl6QYCaEEP1bTGB0JPdn8Tmc4wFgrmWYW4C5OM8JN7vbxpq2NR24A/iJZZgTvGl2a3LPTAgh+je/1np6J9vPOqiFaVsHcTMzyzAHAQtN26p2t5W7v/dZhrkGZ1CLViM0eUEyMyGEEJ3ZCGRbhjnOMsx2B7WwDHO4ZZiBePIgTmUjlmGmWIY5ILAPPTiohQQzIYQQHTJtyw8EBrWwgBdM29phGeZSyzADZfZXAbssw9wNjAAeCxwOlFqGuQ2nMKSoTRWkZ+ShaSGE6Mf6ykPTkpkJIYSIeBLMhBBCRDwJZkIIISKeBDMhhBART4KZEEKIiCfBTAghRMSTYCaEECLiSTATQggR8SSYCSGEiHgSzIQQQkQ8CWaiX9Nas+NgDUdO1vd2U4QQ5yFspoBRSs0HfgpEA89orYva2eeLwMOABrZpre9w138FeMjd7VGt9bMXpNEiYu0/XsdLW8r4y5ZyPjleB8DkjCFcbaQxz0jj4owhREWpXm6lEKKrwmKgYaVUNLAbuBZnFtONwO1a650h+2QDLwDztNZVSqk0rfURpdRQoBSYjhPkNgHTtNZVHV1PBhrun07UN/HaBxW8tLmc932VKAWXTxjGgkvSOXaqkRL7CFv2V9GiYfigOObmpHG1kcqc7FSGJMT2dvOF6BF9ZaDhcAlms4CHtdbXu8sPAmitfxiyz+PAbq31M22OvR24Smv9T+7yMmCN1vr5jq4nwaz/8De38PePjvHi5jLe3HmYBn8LE1IHsnBaJjdPySA9OaHV/lW1jazdfZQS+whrdx+l5nQTMVGKaWNTmOdmbRPTBqGUZG2ib+grwSxcuhkzgAMhy2XAzDb75AAopdbhdEU+rLX+3w6OzWh7AXcq8MUAcXFxnjVchKedB0/w0uYyXt56kGOnGkhJjGXRpaO5dWomF2cO6TAYpQyM4+b8DG7Oz8Df3MLWA9WU2EcosY/ww9dtfvi6TWZKAvOMNK420pg1fhjxsdEX+N2JjlTVNrLpkypKP6lix8EaYqOjSIiLZmBcNIlxMQwc4P6OiyZxQAwD42JIHBDt/I6LZuAAZ1uCu3+0dDVHjHAJZl0RA2TjTAKXCbytlJrc1YO11suB5eBkZj3RQNG7jpyo569bD/Li5jLsQyeJjVbMM9JYODWTq3LTiIvpXr1TTHQU07OGMj1rKN+db3Cw+jSrdx1htX2EP5WW8d/rPyE+NoorJgznKjdry2iT6Ymeo7Vmf2Udpb4qSj+ppNRXxUdHTgEQG63IHZlElFLUNvipa2ymtsFPbWMzzS1d/98/Pjaq3YCXGBfdZr2zrt392uwfFx0lmX0PCJdgVg6MDlnOdNeFKgM2aK2bgI+VUrtxgls5ToALPXZNj7VUhJX6pmb+tvMwL24q4+8fHaVFw5TRyTxy0yQ+d3E6KQO9y8LTkxP40syxfGnmWOqbmnlv33FW20co2XWE/7OP8G9A7oikYBHJ1DHJxERLwbBXmppbsCpOsNFXRamvktJPqjh6sgGApPgYpo9N4eb8DKaPTeGS0cntZsxaaxqbW6hraKa28dMgF/rb+fFT2+D+bvSfsf/Rkw2t1tc3tXT5fcREqWCQS4g7S5BsExzb7h/IMBNjo/t9wVK43DOLwSkA+QxOcNoI3KG13hGyz3ycopCvKKWGA1uAKXxa9DHV3XUzTgFIZUfXk3tmka2lRbPRV8lLm8t5bXsFJxv8ZCQncEt+BrdMzWBC6qAL2h6tNXuP1jqBzT7CRl8l/hbNkIRYrsxJZZ6RytycNIZ6GFj7g5P1TWzZXx0MXFv2V3O6qRmAzJQELs0ayrSxKVyaNZTstEG9+mHe3KKpazwzKLYKhG5m2DpQNrda/+m+zu9uJJG8dt8c8tIHd7vtfeWemafBLKuw+FbgR0AaoNwf7SsqOOtfWCn1WeAnOPfDVmitH1NKLQVKtdarlJOXPwnMB5qBx7TWK91j/wH4F/dUj2mtf9vZtSSYRaaPj9Xyl81lvLSlnLKq0wyMi+aGyaO4dWoGl40bFjbfTE/UN/HOR8cosY+wZtcRjp1qRCknY5yX69xrm5Q+WLqa2qioOc1GXxWbfJVs9FVhH+e6f+AAACAASURBVDpBi4YoBXnpg5k+dijTs1KYPnYoI4fE93Zze5zWmgZ/S+vssVWWGBIUG5q5a9ZYhg0a0O3rSDBrR1Zh8R7gRl9RgeXZSXvAuQaz59/fT3OLJn9MMrkjkqQL6QKoqWvi1e0HeWlzOZs+qUIpmD1xOAunZnLdpBEkxoVLT3n7Wlo028trgoFtW1kNACMGD+BqN7DNnjicgQPC+314rblFs/vwSUo/cbsMfVWUV58GIDEumvwxyUwfO5RLs4YyZUwyg/rZ3+dCCpdgZhlmu8mQaVtdSje9DmbrfEUFV3h2wh5yrsHsll+sY8v+agASYqO5OHMI+WNSyB+TTP6YZNKS+v63xQuhqbmFtbuO8tKWMt7aeYTG5hay0wYFy+kj+Vv50ZMNrNl1hNW7jvD33cc42eAnLjqKmeOHclWuc69t3PBe/1zx3OnGZrYeqGbTJ07WtXl/FSfr/QCkJQ3g0qxPsy5zlHxRvJDCKJjtAW40beuckiGvg9lPgZHAy0BDYL2vqOAlzy7igXMNZlpryqpOs3m/03+/5UA1Ow/W0NTs/A0zkhPcwJbC1DHJ5KUPZkCMlG13hTOs1Ale3FzGqq0HOV7byLCBcSyYks7CqZl9sluuqbmFjb7K4L22vUed/ybHDR/I1W5gmzFuaLerMMPBsVMNTpWhe7/rw/Ia/O4NoJwRg5wqUfd+V2ZKQp/7t40kYRTM1pm2dc7JkNfBrL17VdpXVPAPnl3EA17eM6tvambHwRNsCQS4/VUcrHHG+YuLjmJSxmDyR3+avWUky/+4oQ7V1PPy1nJe2lzG7sOniIuO4po8p5z+ypxUYvvRN/T9x+tYvcsJbOv3HafR38LAuGhmZw93nmvLTSNtcPhlpYECmEDWtemTKj4+5vz/FRcTxZTMZCfrykph6pgUkhOlECachFEwazcZMm2rS8lQWFQzXmg9XQByqKaerQcCwa2aD8qrg6W7qUkDyB+dHOyevDhzSNjf9/FaXaOfv+04zIuby3hnzzG0hmljU7h1agafm5zOkEQZOqqu0c+7e45T4j7XVuF+QbooY3DwXtslmcm98lBvg7+ZD8tPBLOuTZ9UUVnbCEBKYmww65qeNZSLMqR3ItyFUTBrNxkybatLyZDXmVkm8DQQSBX/DnzLV1RQ5tlFPHChqxmbmlvYdejkp9nbgergN9foKEXuiKRg92T+mGTGDRsYNpV5Xmlp0bz38XFe2lzO69srqG1sJjMlgVunZnJLfkafvE/kFa019qGTlNhOYNvsjh85dGAcV+WkcrWRxpU5PTd+ZE1dE5v2u1mXr4qtZdU0+p0vZ+OGD3QDlxO8xg8fKD0PESZcgtn58jqYvQn8Efi9u+rLwJd8RQXXenYRD4RDaX5lbSPbDjjdklsOVLN1fzUnG5wb4kMSYpkyOjkY4KZkJkdstrL36ClnWKktBymvPs2gATEUuOX0l2YN7XNB+0Koqm3k7Y+Osto+wprdR6muayLaHT8ycK8tZ8S5jR8ZuC+80c26Sn2V7D7sjKoRE6W4KGNIMOuaNjaF1KTul4KL8BIuwcwyzHaTIdO2upQMeR3MtvqKCqacbV1vC4dg1lZLi2bv0VNu5uZkcLsOnyTwzzMhdeCnlZOjU8gZMShsK76qaht59YODvLi5nK0HqolScGVOKrdOzeRacwQJcdLt5JXmFs3WA1Xu+JFHsSpOAE4x0tVGKvOMNGaNH97h39zf3IJVcTI4HFTpJ5UcPuGOqjEghqljU7jUzbouyUyWf7s+KIyCWbvJkGlbXUqGvA5m/wf8FgiMWH878FVfUcFnPLuIB8IxmLXnZH0T28tq2OJmcJv3VwfvTSTGffpowNQxKUwZndyr35Ib/S2s3nWElzaXUWIfoalZY4xMYuHUTG6akh6WhQt9UUXNaVbbzqj/6/Yc43RTMwNiorh8wjDmGWlcPnE4FdX1buZVyZb91dQ1OqNqZCQnBLsLp49NIWdEkgy02w+EUTDbatrWlLOt64jXwWwsTpo4C2eYqXeB+3xFBfs9u4gHIiWYtaW15kDl6WDmtnl/FTsPngiWPI8emhBSOZlC3qjBPVrWrbXmg7IaXtpcxqptB6mqa2L4oAHcPCWdW6dmntPQOsI79U3NvP9xpXOvbdeR4CSk4IyqYYwczKVZKUxzg1fb6XBE/xBGwazdZMi0rS4lQ1LNGOHqm5r5sLymVfdkoPItLiaKi9IHhzzYnUL6kPjzvkF/sPo0f9nilNPvPVpLXEwU1+WNYOHUTOZkDw/b7s/+TGvNvmO1bNhXSWaK8zxkUnxk3ocV3gqjYNZuMmTaVpeSIU+CWVZh8Xd9RQWPZxUWP+02ohVfUcF9530RD/WlYNaeiprTbHWrJrfsr+KDshoa3OqztKQBn1ZOjk5mchcfDaht8PO/Hx7ixc1lrN93HK3h0qwUFk7N5IbJo2QmZiEiVFeCmWWY84Gf4oyd+4xpW0Vtto8FVgCpQCXw5UDhhmWYXwEecnd91LStZz1+C4B3U8AEhh8p9eh84jyMGpLAqMkJ3DB5FOA8GmBXnAxmblv2V/HGjsOA82iAMTIpWFiSPyaZcW55dXOLZv3e47y0uYzXPzzE6aZmxgxN5FufyeaW/AzGDuv1L3NCiB5mGWY08HPgWpypuDZahrnKtK2dIbs9Afy3aVvPWoY5D/ghcKdlmEOB/wCm485w4h5bFXL+75q29bhlmO0mQ6ZtdSkZ8iSY+YoKXnFf1vmKCv4Uui2rsPgLXlxDnLvY6CgmZw5hcuYQ7prlrKusbWz1YPfLWw7yh/ecbD45MZbJGUP46PApDp2oJyk+hpvzM1g4NYNpY1PkOSIh+pcZwB7TtvYBWIa5ErgJCA1mecC33dercUbxALgeeNO0rUr32DdxZj55PuRYT5Ihr4eeeBD4UxfWiV42dGAc84wRzDNGAE6Jt/NogBPgth6oJi99MA99zuQac0S7Ex0KIfqFDOBAyHIZMLPNPtuAW3G6Im8BkizDHNbBsRmhB5q2FUyGTNtqFSssw+xyMuRJMMsqLL4B+CyQkVVY/FTIpsGA34triJ4VHaXIGZFEzogkbrt0TG83Rwhx4cQopUKzouVa6+XdPMcDwM8sw7wbeBtnkuXmbp7jvJIhrzKzgzgp4gKcWZ8DTgL3e3QNIYQQ3vNrrad3sr0cGB2ynOmuCzJt6yBOZoZlmIOAhaZtVVuGWQ5c1ebYNaHHWoYZTIYswzznZMire2bbgG1ZhcV/9BUVNHlxTiGEEGFhI5BtGeY4nCC2CLgjdAfLMIcDlaZtteBkUyvcTW8AP7AMM8Vdvs7dHsqTZMjrh6azcapY8oDgkA++ooLxnl3EA329NF8IIbqqi6X5nwV+glOav8K0rccsw1wKlJq2tcoyzM/jfPZrnG7Gb5i21eAe+w/Av7inesy0rfZGx8cyzFjTts45GfI6mL2DU4b5Y+BG4KtAlK+o4N89u4gHJJgJIYQjjB6abjcZMm2rS8mQ10M1JPiKCv4PUL6igk98RQUPAwUeX0MIIUTf81vglzj3ya4G/hv4Q1cP9jqYNWQVFkcBH2UVFt+bVVh8CzCoKwcqpeYrpXYppfYopQrb2X63UuqoUmqr+/OPIduaQ9av8u7tCCGEuEASTNv6P0CZtvWJaVsP041kyOvnzL4FJAL3AY8A84CvnO0gpdQZT5grpVZprXe22fV/tNb3tnOK01rrsJpmRgghRLc0WIYZBXxkGea9OMUmXUqGwONg5isq2Oi+PIVzv6yrZgB7tNb7AJRS7T1hLoQQou86p2QowNNgllVYvJr2Bxqed5ZDu/KEOcBCpdSVwG7gfq114Jh496E/P1CktX657YFKqcXAYoC4uLizvRUhhBAXkGlb55oMAd53Mz4Q8joeWIh3I4C8AjyvtW5QSv0T8CxO5AYYq7UuV0qNB0qUUtu11ntDD3afaF8OTjWjR20SQgjhAcsw202GTNs6WzIEeN/NuKnNqnVZhcXvd+HQsz5hrrU+HrL4DPB4yLZy9/c+pdQaIB9oFcyEEEKEtfNKhrzuZhwashgFTAOGdOHQjUC2UqrDJ8yVUqO01hXu4gLckZaVUilAnZuxDQeuICTQCSGECH+mbZ2RDFmG2ZVkCPC+m3ETTpqocCLqx8DXznaQ1tqvlLoXZ+iTaGCF1nqHUmopUKq1XgXcp5Ra4J63ErjbPdwElimlWnACaFE7VZBCCCHCmDv3WUB3kiHA4xFAIoWMACKEEI4wGgHkY85MhpaatvVOV473JJhlFRbf2tl2X1HBS+d9EQ9JMBNCCEe4BLPz5VU3442dbNNAWAUzIYQQ4cEyzE6TIdO2uhQ/vJoCptvPBAghhBB4lAx5fs8sq7C4AJhE6ylglnp6kfMk3YxCCOGQbsZ2ZBUW/wpnOJKrcZ4F+zzQ5dJKIYQQ/ZdlmGckQ6ZtdSkZ8nrU/Mt9RQV3AVW+ooLvA7OAHI+vIYQQoo+xDPNXwG3AN3EqGr8AjO3q8V4Hs9Pu77qswuJ0oAkY5fE1hBBC9D2Xm7Z1F1Bl2la3kyGvH5p+NauwOBn4T2Azzs27X3t8DSGEEH1PMBmyDDMdOE43kqEee2g6q7B4ABDvKyqo6ZELnAcpABFCCEe4FIBYhvlvwNPAZ3Dmt9TAr03b+veuHO9pMMsqLP4AWAn8j6+oIGwH+pVgJoQQjnAJZqEswxwAxJu21eVkyOtuxhtxbuC9kFVY3AL8D/CCr6hgv8fXEUII0YdYhhlMhkzb2gs0dOf4nuxmzAb+DfiSr6ggukcuco4kMxNCCEe4ZGaWYY7FSYZuA4LJkGlbXUqGvM7MyCosDm1QM/Bdr68hhBCibzFt6xOc6bsetwwzkAz9CGcmlbPy+qHpDUAs8CfgC76ign1enl8IIUTf1SY761Yy5HVmdpevqGCXx+cUQgjRx1mG2SoZMm2rW8mQ18GsOquw+DdAuq+o4IaswuI8YJavqOA3Hl9HCCFE33KXaVvnnAx5PQLI73Bmi053l3cD/5/H1xBCCNH3VFuG+RvLMF8HsAwzzzLMr3X1YK+D2XBfUcELOJUo+IoK/Dj9nkIIIURnfsd5JENeB7ParMLiYThPbpNVWHwZEHYjgAghhAg7w03bCiZDpm11Kxny+p7Zt4FVwISswuJ1QCrONDBnpZSaD/wUpwzzGa11UZvtd+OM+VjurvqZ1voZd9tXgIfc9Y9qrZ89z/chhBDCZRlmq89n07aK2mwfAzwLJLv7FJq29ZplmFmABQTuhb1n2tY9HVym1jLMYDJkGWa3kqGemJwzBsjFGcJ/l6+ooOmsjVAqGielvBYoAzYCt2utd4bsczcwXWt9b5tjhwKlwHScP8ImYJrWuqqj68lD00II4TjbQ9OWYbb7+Wza1s6QfZYDW0zb+qVlmHnAa6ZtZbnB7FXTti46Wzssw5yKMzbjRcCHuMmQaVsfdOV9eP7QtHufbEc3D5sB7NFa7wNQSq0EbgJ2dnqU43rgTa11pXvsm8B84PlutkEIIcSZZgB7AqXylmG29/msgcHu6yHAwe5exLStzZZhziUkGTJt66zJUIDnwewcZQAHQpbLgJnt7LdQKXUlzreE+7XWBzo4NqOnGiqEEP1MVz6fHwb+ZhnmN4GBwDUh28ZZhrkFOAE8ZNrW3zu6kHufrLvJEOB9AUhPegXI0lpfDLyJ0z/bZUqpxUqpUqVUqd/v75EGCiFEBIoJfDa6P4vP4Ry3A78zbSsT+Czwe8swo4AKYIxpW/k4NRV/tAxzcCfnOWeeZGZZhcVTO9vuKyrYfJZTlAOjQ5Yz+bTQAwCt9fGQxWdwxvAKHHtVm2PXtL2A1no5sByce2ZnaY8QQvQXfq319E62n/XzGfgazu0dTNtabxlmPE514hHc0e9N29pkGeZenNmjS71qfIBX3YxPdrJNA/POcvxGIFspNQ7nj7QIuCN0B6XUKK11hbu4AKdCBpznEn6glEpxl68DHuxG24UQQnRsI5BtGWaHn8/AfpxJNX9nGaYJxANHLcNMBSpN22q2DHM8kA20GqbKLfzokGlbZ0uGAI+Cma+o4OrzOV5r7VdK3YsTmKKBFVrrHUqppUCp1noVcJ9SagHgByqBu91jK5VSj+D8wQGWBopBhBBCnB/TtvyWYbb6fDZta4dlmEuBUtO2VgH/DPzaMsz7cRKYu03b0pZhXgkstQyzCef5sXtM22r7+Xy+yRDg/UzTiTj9omN8RQWL3TnNcn1FBa96dhEPSGm+EEI4wmU+s/PldQHIb4FG4HJ3uRx41ONrCCGE6GMsw0y0DPMh95k1LMPMtgzzc1093utgNsFXVPA40ATgKyqow3leQAghhOjMeSVDXgezxqzC4gQ+HZtxAm4lixBCCNGJCaZtBZMh07a6lQx5/dD0w8D/AqOzCoufA67ALdQQQgghOtFoGWYwGbIMs1vJUE+MzTgMuAwnor7nKyo45ukFPCAFIEII4QiXAhDLMK8D/hXIA/6GmwyZtrWmK8d7Xc34CvBHYJWvqCBso4UEMyGEcIRLMANwR80PJkOmbXU5GfI6mM0FbgMKcJ77Wgm86isqqPfsIh6QYCaEEI5wCWaWYQaTIdO2uv0B7Xk3I0BWYXE0zoNu/w+Y7ysq6JGxuM6VBDMhhHCEUTBrNxkybatLyZDno+a71Yw3uo2aSjcHBBZCCNH/mLa1Fljrzp8WSIZW8OnUMp3yNJhlFRa/gDP3zf8CPwPW+ooKWry8hhBCiL7JrWY8p2TI68zsN8DtvqKCZo/PK4QQog+zDPOMZMi0rS4nQ14Hs78DD2YVFof12IxCCCHCzm+A203bOqdkSMZmFEIIEQ7+DjwoYzMKIYSIZDI2oxBCiIgXVmMz/gcyNqMQQojuk7EZu0semhZCCEcYPTR9LfAQvTk2Y1Zh8dTOtvuKCjaf90U8JMFMCCEc4RLM4PzGZvSqm/HJTrZpnKe5hRBCiFYsw2ybDFW4v8dYhjnGtK0uJUM9MjZjuJPMTAghHL2dmVmGubqTzdq0rS4lQ2ETzJRS84GfAtHAM1rrog72Wwj8GbhUa12qlMoCLGCXu8t7Wut7OruWBDMhhHD0djDziucDDZ8LpVQ08HPgWqAM2KiUWqW13tlmvyTgW8CGNqfYq7WeckEaK4QQIux4/ZzZuZoB7NFa79NaN+IM/X9TO/s9AvwICKv50YQQQvQuT4NZVmHxS1mFxQVZhcXdPW8GcCBkucxdF6SUmgqM1loXt3P8OKXUFqXUWqXUnG5eWwghRITzupvxF8BXgaeyCov/BPzWV1Sw6yzHnJVSKgr4L9p/ALsCGKO1Pq6Umga8rJSapLU+0eYci4HFAHFxcefbJCGEEB6zDDMDGEtIbDJt6+2uHNtTM00PAW4H/hUn4/o18AdfUUFTu41QahbwsNb6enf5QQCt9Q/d5SHAXuCUe8hIoBJYoLUubXOuNcADbdeHkgIQIYRwhEsBiGWYP8KZx2wnEBg5X5u2taArx/fETNPDgC8DdwJbgOeA2cBXgKs6OGwjkK2UGoczuOQi4I7ARq11DTA8sBwasJRSqUCl1rpZKTUeyAb2efy2hBBC9KybgVzTts5pPF+vZ5r+C5AL/B640VdUEHj47X+yCos7zJS01n6l1L3AGzil+Su01juUUkuBUq31qk4ueyWwVCnVBLQA92itK714P0IIIS6YfUAs5zg4vafdjFmFxZ/1FRW81mbdAF9RQViNnC/djEII4QijbsYXgUuA/yMkoJm2dV9Xjve6m/FR4LU269YDnY7dKIQQot9b5f6cE0+CWVZh8UicUvqErMLifD6dg2YwkOjFNYQQQvQOyzBbjdBk2lZRm+1jgGeBZHefQtO2XnO3PQh8Daeo4z7Ttt5o7xqmbT3rTgEzxrStblfBe/Wc2fXAE0AmTgn9k+7Pt4F/8egaQgghLjDLMAMjNN2AMz3L7ZZh5rXZ7SHgBdO28nEK+H7hHpvnLk8C5gO/cM/X3nVuBLbizImJZZhTLMPscqbmSWbmKyp4Fng2q7B4oa+o4EUvzimEECIszAD2mLa1D8AyzMAITaHDDWqcnjiAIcBB9/VNwEq3QvFjyzD3uOdb3851Hna3rQEwbWurZZjju9pIr7oZv+wrKvgDkJVVWPztttt9RQX/5cV1hBBCXHDtjdA0s80+DwN/swzzm8BA4JqQY99rc2wG7WsybavGMszQdS1dbaRX3YyBSphBQFKbn0EeXUMIIYT3YpRSpSE/i8/hHLcDvzNtKxP4LPB7yzC7G192WIZ5BxBtGWa2ZZhPA+929WCvuhmXuS/f8hUVrAvdllVYfIUX1xBCCNEj/Frr6Z1sLwdGhyxnuutCfQ3nnhimba23DDMeZ6CLrhwb8E2cUaMagD8CfwOWdvE9eF6a/zRnluG3t04IIURk2AhkW4bZ7ghNrv3AZ4DfWYZpAvHAUZxS+z9ahvlfQDrOCE3vd3Cd203b+lecgAaAZZhFQGFXGunVPbNZwOVAapt7ZoNxyjSFEEJEINO2/JZhthqhybStHZZhLgVKTdtaBfwz8GvLMO/HKQa527QtjdN1+AJOsYgf+IZpW83tX4mFlmHWm7b1HIBlmD8DErraTk9GAMkqLJ6LM+7iPcCvQjadBF7xFRV8dN4X8ZCMACKEEI4wGgEkASeTW4HTZVlt2ta3unq818NZjfUVFXzi2Ql7iAQzIYRw9HYwswxzaMhiEvAysA74dwDTtro01q7X98wGZBUWLweyQs/tKyqY5/F1hBBC9A2bcLomVcjvAvdHA1161szrYPYnnG7GZ/h0PhohhBCiXaZtjfPiPF4HM7+vqOCXHp9TCCFEH2cZZiywBGdaL3BGAllm2la7kzq35XUweyWrsPjrwF8IGcLfV1Qg84sJIYTozC9x5jP7hbt8p7vuH7tysNfB7Cvu7++ErOtyn6cQQoj+xTLMGNO2/MClpm1dErKpxDLMbV09j6fBzFdU4EnfpxBCiH7jfZyBNZotw5xg2tZeAHeQ4S7XXngyNmNWYfF3Q15/oc22H3hxDSGEEH1SYP7LB4DVlmGusQxzDVCC8zB2l3g10PCikNcPttk236NrCCGE6HtSLcP8NjAFWIYTxEqAXwP5XT2JV92MqoPX7S0LIYQQAdE4s6u0jRUxOA9Rd4lXwUx38Lq95XYppVpNy621Lupgv4XAn4FLtdal7rpW03JrrdudllsIIUTYqTBtq8uj43fEq2B2SVZh8QmcyJrgvsZdjj/bwUqpwLTc1+JM3rZRKbVKa72zzX5JwLeADSHrQqflTgfeUkrlaK3loW0hhAh/nvTeeTWf2fmOjD8D2KO13geglGpvWm6AR4Af0br0/yZgpda6AfhYKdXZtNxCCCHCy2e8OIlXBSDnq71puVtNra2UmgqM1loXd/dYIYQQ4amrAwmfjdcPTfcIpVQU8F/A3edxjsXAYoC4uDhvGiaEECIshEswO9vU2knARcAapRTASGCVUmpBF44FQGu9HFgOzhQwXjZeCCFE7wqXbsaNQLZSapxSKg6noGNVYKPWukZrPVxrnaW1zgLeAxa41YyrgEVKqQFKqXF0Pi23EEKIPigsMjOttV8p1Wpabq31DqXUUqBUa72qk2N3KKVaTcstlYxCCNG/eDrTdKSQmaaFEMLR2zNNeyVcuhmFEEKIcybBTAghRMSTYCaEECLiSTATQggR8SSYCSGEiHgSzIQQQkQ8CWZCCCEingQzIYQQEU+CmRBCiIgnwUwIIUTEk2AmhBAi4oXFQMNCCCHCl2WY84Gf4gwE/4xpW0Vttv8YuNpdTATSTNtKdrc1A9vdbftN21rQE22UgYaFEKIfO9tAw5ZhRgO7gWuBMpwpu243bWtnB/t/E8g3besf3OVTpm0N8r7lrUk3oxBCiM7MAPaYtrXPtK1GYCVwUyf73w48f0FaFkK6GYUQon+LUUqVhiwv11ovD1nOAA6ELJcBM9s7kWWYY4FxQEnI6njLMEtx5pssMm3rZW+a3ZoEMyGE6N/8WuvpHp1rEfBn07ZCJ0gea9pWuWWY44ESyzC3m7a116PrBUk3oxBCiM6UA6NDljPdde1ZRJsuRtO2yt3f+4A1QL73TZTMTAghROc2AtmWYY7DCWKLgDva7mQZpgGkAOtD1qUAdaZtNViGORy4Ani8JxopmZkQQogOmbblB+4F3gAs4AXTtnZYhrnUMszQMvtFwErTtkJL5E2g1DLMbcBqnHtm7VZBni8pzRdCiH7sbKX5kSJsMjOl1Hyl1C6l1B6lVGE72+9RSm1XSm1VSr2jlMpz12cppU6767cqpX514VsvhBCiN4VFZqaUavehPK31zpB9BmutT7ivFwBf11rPV0plAa9qrS/q6vUkMxNCCIdkZt6aAezRWu/TWrf7UF4gkLkGAr0fhYUQQoSFcAlm7T2Ul9F2J6XUN5RSe3GqYe4L2TROKbVFKbVWKTWnZ5sqhBAi3IRLMOsSrfXPtdYTgO8BD7mrK4AxWut84NvAH5VSg9seq5RarJQqVUqV+v3+C9doIYQQPS5cgll3HsoDpxvyZgCtdYPW+rj7ehOwF8hpe4DWernWerrWenpMjDxeJ4QQfUm4BLONQLZSapxSKg7neYVVoTsopbJDFguAj9z1qW4BCUqp8UA2sO+CtFoIIURYCIsURWvtV0oFHsqLBlZorXcopZYCpVrrVcC9SqlrgCagCviKe/iVwFKlVBPQAtyjta688O9CCCFEbwmL0vwLTUrzhRDCIaX5QgghRJiQYCaEECLiSTATQggR8SSYCSGEiHgSzIQQQkQ8CWZCCCEingQzIYQQEU+CmRBCiIgnwUwIIUTEk2AmhBAi4kkwE0IIEfEkmAkhhIh4EsyEEEJEPAlmQgghIp4EMyGEEBFPgpkQQoiIJ8FMCCFExJNgJoQQIuJJMBNCCBHxJJgJIYSIeDG93YAApdR84KdANPCM1rqozfZ7nM16TgAABrhJREFUgG8AzcApYLHWeqe77UHga+62+7TWb1zItgshRF9mGWarz2fTtorabP8xcLW7mAikmbaV7G77CvCQu+1R07ae7Yk2Kq11T5y3e41QKhrYDVwLlAEbgdsDwcrdZ7DW+oT7egHwda31fKVUHvA8MANIB94CcrTWzR1db+DAgbq2trbH3o8QQkQKpVSd1npgR9stw2z389m0rZ0d7P9NIN+0rX+wDHMoUApMBzSwCZhm2laVx28jbLoZZwB7tNb7tNaNwErgptAdAoHMNRDnD4O730qtdYPW+mNgj3s+IYQQ528GsMe0rX2mbbX7+dzG7TgJBsD1wJumbVW6AexNYH5PNDJcglkGcCBkucxd14pS6htKqb3A48B93TlWCCHEOenyZ6xlmGOBcUBJd489X2Fzz6wrtNY/B36ulLoDpw/2K109Vim1GFgcOJVS6vQ5NiMG8J/jsZFG3mvfJO+17zmf95mglCoNWV6utV5+judaBPzZtK0Ob/P0lHAJZuXA6JDlTHddR1YCv+zOse4/zrn+AwUppUq11tPP9zyRQN5r3yTvte/p4ffZnc/nRTiFeqHHXtXm2DUeti0oXLoZNwLZSqlxSqk4nD/IqtAdlFLZIYsFwEfu61XAIqXUAKXUOCAbeP8CtFkIIfqDjUC2ZZjjLMNs9/MZwDJMA0gB1oesfgO4zjLMFMswU4Dr3HWeC4tgprX2A/fivEkLeEFrvUMptdStXAS4Vym1Qym1Ffg2bhej1noH8AKwE/hf4BudVTIKIYToOtO2zvh8Nm1rh2WYSy3DXBCy6yJgpWlbOuTYSuARnIC4EVjqrvNcWJTmRxKl1OLz6E+OKPJe+yZ5r31Pf3mfnZFgJoQQIuKFRTejEEIIcT4kmHWRUmq+UmqXUmqP+v/bu58Xq8o4juPvTyVlY2WBhWQkFUQ/KK1okRVRq0LChSGUEtHShdKiEoyoP6DaBAkVKA4VpW4KIjMxZmGWNmqYq3BhCENQ0QRF6KfFeRbThHhrPPPMOffzgmHufTj38D0Ml+/58cznkV6sXU+bJL0raULSd7VraZOk6yTtlXSsPI/dULumtki6RNIBSYfLsb5Su6a2SbpQ0reSPq5dS5sknZB0VNL4tCn2QyW3GQcwSNxWn0h6kCb/cpvt22vX0xZJi4HFtg9JuowmamdVH/+ukgSM2J6UNA8YAzbY3l+5tNZIeo4mRuly2ytr19MWSSeAe2z/VLuWmnJlNphzxm31ie0vgVZmHM0ltk/ZPlRe/0YzU6uX6TFuTJa388pPb89kJS2h+Reet2vXErMjzWwwiczqOUlLgeXAV3UraU+57TYOTAC7bff2WIE3gOeBM7ULmQUGPpN0sCQdDaU0sxh6khYAO4CN0wKte8X2advLaFIY7pXUy1vIklYCE7YP1q5lltxv+y7gUWB9eUwwdNLMBvNf47aiI8rzox3AqO2dteuZDbZ/AfbSUnr5HLACeLw8S3ofeFjS9roltcf2j+X3BLCLIV01JM1sMOeM24ruKZMi3gG+t/1a7XraJGmRpIXl9XyayUzH61bVDtubbC+xvZTmu/qF7bWVy2qFpJEyeQlJIzRxUb2ehXw2aWYDOFvcVt2q2iPpPZp8tZslnZT0bO2aWrICWEdz5j5efh6rXVRLFgN7JR2hOTnbbbvXU9aHxDXAmKTDNJm0n9j+tHJNVWRqfkREdF6uzCIiovPSzCIiovPSzCIiovPSzCIiovPSzCIiovPSzCLmMEkP9T31PeJ8SDOLiIjOSzOLOA8krS3rhY1L2lJCfSclvV7WD9sjaVHZdpmk/ZKOSNol6coyfpOkz8uaY4ck3Vh2v0DSR5KOSxotySURMUWaWcQMSboFWAOsKEG+p4GngBHgG9u3AfuAl8tHtgEv2L4DODplfBR40/adwH3AqTK+HNgI3ArcQJNcEhFTXFS7gIgeeAS4G/i6XDTNp1lm5QzwQdlmO7BT0hXAQtv7yvhW4MOSr3et7V0Atv8AKPs7YPtkeT8OLKVZXDMiijSziJkTsNX2pn8MSi9N2+7/Zsf9OeX1afK9jfiX3GaMmLk9wGpJVwNIukrS9TTfr9VlmyeBMdu/Aj9LeqCMrwP2lZWuT0paVfZxsaRLZ/UoIjosZ3gRM2T7mKTNNKv9XgD8BawHfqdZBHMzzW3HNeUjTwNvlWb1A/BMGV8HbJH0atnHE7N4GBGdltT8iJZImrS9oHYdEcMgtxkjIqLzcmUWERGdlyuziIjovDSziIjovDSziIjovDSziIjovDSziIjovDSziIjovL8BZNUqKZDGxGIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jECXLrLxUF6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model and init weights\n",
        "rnn_model.save('rnn_model_BDAdam001_hid100.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryD1GICX3nyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4eb10ee3-9df7-43ff-ce37-9d68d8a974a9"
      },
      "source": [
        "#prediction on test data\n",
        "m2_predictions = rnn_model.predict(test_vectorized_data_padded)\n",
        "\n",
        "pred_labels = numpy.argmax(m2_predictions, axis=-1)    #\n",
        "pprint(pred_labels[2])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([25,  1, 25, 25,  2,  7, 19, 35, 35, 35, 35, 25, 19, 35, 35, 35, 25,\n",
            "       19, 35, 35, 35, 35, 25, 25, 25, 25, 25, 19, 25, 25, 25, 25, 25, 25,\n",
            "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
            "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
            "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
            "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
            "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
            "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
            "       25, 25, 25, 25, 25, 25, 25, 25, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCo0xF5kMMbH"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgSYNcerMI9R"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRDxKgLSL_uf"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlG6ZWkIL-HY"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}