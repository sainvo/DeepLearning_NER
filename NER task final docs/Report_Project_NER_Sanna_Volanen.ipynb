{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "We got a bit of a late start to this project, but here we go!\n",
    "\n",
    "We opted for the English data, because I thought it would be more interesting for my teammate Tatjana, being non-Finnish speaking student. At first we focused on getting the data in the correct form by following the example given in the NER notebook from the course materials. This was my main focus in the beginning of the project as the quality of the data fed to the model is kind of crucial. Thus, I dedicated a lot of hours to print out the data examples in various stages of the preprocessing to identify the mistakes and to make a better data segmentation part to produce the highest quality data input for the model as possible. \n",
    "\n",
    "It looked already good at one point, but then Tatjana questioned the fact that the longest supposed sentence in the prepocessed, vectorized data, was 17k tokens long. Even with all the punctiation in the world, it would be hard to have such a long sentence, so I dived back into debugging the data. Finally, I managed to clean the data to a point where the longest sentence was 168 tokens. I manually checked the longest sentences to make sure that this time they really were one sentence in one list, without nested sentences or lists. \n",
    "\n",
    "I handed this code to Tatjana, so she could use it to train her already existing models built and trained on the bad formed data. Then I kept working on the first model whereas Tatjana took upon herself to move on and focus on BERT. That is to say, that I have not had basically any input on Milestones beyond 1.2 - that's all Tatjana, so please take that into consideration when grading this project.\n",
    "\n",
    "Next I will take a look at the Milestone 1, and in the end summarize on the first two models and the project as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 1.1\n",
    "\n",
    "After I manged to get the data wrangled to an acceptable shape, I focused on training the first model on various hyperparameters. In the notebook, you can see three variations of the model 1 with different parameter pairings. \n",
    "\n",
    "The general result was that all the variations that produced real numbers, produced result that were in the same ballpark.\n",
    "On token level, the model reached around 90 % accuracy with most parameter selections. On entity level, the f-scores were at the highest around 0.4 and precision around 0.5. Changing the learning rate did not have a big effect on the model runs where the hidden layer activation was relu. With sigmoid, the results were slightly under the one's with relu or produced useless, small numbers. \n",
    "\n",
    "Here is a compilation of some of my runs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Optimizer | lrate | Activation | Maximum values on evals(ent)|\n",
    "|-----------|-------|------------|-----------------------------|\n",
    "|-----------|-------|------------|F-score | Recall | Precision | \n",
    "|-----------|-------|------------|--------|--------|-----------|\n",
    "|  Adamax   | 0.009 | relu       |  0.52  |    -   |    40.5   |\n",
    "|-----------|-------|------------|--------|--------|-----------|\n",
    "|  Adam     | 0.007 | relu       |  0.5   |  0.29  |    41.3   |\n",
    "|-----------|-------|------------|--------|--------|-----------|\n",
    "|  Adam     | 0.006 | relu       | 0.51   |  0.33  |    39.4   |\n",
    "|-----------|-------|------------|--------|--------|-----------|\n",
    "|  Adam     | 0.005 |  sigmoid   | 0.53   |  0.34  |    40.8   |\n",
    "|-----------|-------|------------|--------|--------|-----------|\n",
    "|  Adam     | 0.007 | sigmoid    | 0.53   |  0.3   |    38     |\n",
    "|-----------|-------|------------|--------|--------|-----------|\n",
    "|  Adadelta | ?     |  relu      | 0.53   |   -    |    35     |\n",
    "|-----------|-------|------------|--------|--------|-----------|\n",
    "\n",
    " Token level accuracy was around 0.86 to 0.91 with all above variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis on model 1\n",
    "\n",
    "As already stated, the learning rate did not have much of an effect. The above optimizers were the ones that mainly produced best numbers, others just produced useless, around o.oox values. On a token level, this model is a moderately good tagger based on the accuracy, but it has also a lot of noise coming from a skewed data. At one point, I was trying to implement class weights on the model to counter this, unsuccessfully, so I calculated the amounts of each label. Evidence of this can be found from the notebook *DL_NER_token_model_WOP1* in the repo folder Raw versions and notes. The ratio of all the other tokens to the O was around 6:100, so the impact was significant and affects the maximum evaluation values we can get from this model. Below, you can see the full list of the frequences. Based on this info, I would estimate that predicting the least frequent labels is really dependent on the model handling the skewed data.\n",
    "\n",
    "**Calculated counts per tag**\n",
    "\n",
    "Counter({'O': 770141,\n",
    "         'B-GPE': 5766,\n",
    "         'B-PERSON': 5643,\n",
    "         'I-ORG': 4818,\n",
    "         'I-PERSON': 4518,\n",
    "         'I-DATE': 3402,\n",
    "         'B-ORG': 3230,\n",
    "         'B-NORP': 3186,\n",
    "         'B-DATE': 2794,\n",
    "         'B-CARDINAL': 2176,\n",
    "         'I-GPE': 1222,\n",
    "         'I-WORK_OF_ART': 839,\n",
    "         'I-EVENT': 839,\n",
    "         'I-FAC': 693,\n",
    "         'B-LOC': 616,\n",
    "         'B-ORDINAL': 608,\n",
    "         'I-LOC': 597,\n",
    "         'I-MONEY': 563,\n",
    "         'I-CARDINAL': 548,\n",
    "         'I-TIME': 516,\n",
    "         'I-QUANTITY': 402,\n",
    "         'B-TIME': 398,\n",
    "         'B-FAC': 396,\n",
    "         'B-QUANTITY': 384,\n",
    "         'B-EVENT': 356,\n",
    "         'B-WORK_OF_ART': 338,\n",
    "         'I-PERCENT': 302,\n",
    "         'B-MONEY': 282,\n",
    "         'I-LAW': 229,\n",
    "         'B-PERCENT': 207,\n",
    "         'B-PRODUCT': 181,\n",
    "         'I-NORP': 172,\n",
    "         'B-LANGUAGE': 126,\n",
    "         'I-PRODUCT': 123,\n",
    "         'B-LAW': 65,\n",
    "         'I-LANGUAGE': 12,\n",
    "         'I-ORDINAL': 2})\n",
    "\n",
    "**So, does it make sense to evaluate the data on a token level?**\n",
    "Not really, as Named Entities are usually more than one token (\" word) lengthwise. Thus, having an okish BIO tagger may get us started with untagged data, but it doesn't give us much value on entity level sequences. To summarize, on a token level, this model is ok but on an entity level, it's crap.\n",
    "\n",
    "Note: I ran the predictions once and got values out, but I didn't finish the value2class transferring. So, my analysis lacks on this area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 1.2\n",
    "\n",
    "Both of us struggled to get this to work in the beginning, and my understanding is that Tatjana dit not manage to make a working second model. This task was left to me as Tatjana moved to BERT.\n",
    "\n",
    "I first tried to copy the model in the example NER notebook, but got nowhere with that RNN model.  The evaluation values were all zeros. As I got to this phase on the last day (extended deadline 20.5.), I decided to scrap the example model and try LSTM instead. Not until I tried bidirectional LSTM, did I start to get better results. This model's versions are saved as rnn_model_X.h5 in the repo (if fits or add link to Gdrive). \n",
    "\n",
    "From the first runs, I could already see an improvement in the precision scores which jumped 10 points or more from the best f-score from Model 1. Thus, also the f-scores became higher, landing now north of 0.43, generally. The \n",
    "\n",
    "Here are a few of the evaluations sets from this model's dev variations. I had time only to run a few rounds as th model took about 10min per epoch to train, so I ran out of time to tweak it. This means that it may have potential for even better numbers!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Optimizer |  lr   | Max values on evals(ent)| Other info               |\n",
    "|-----------|-------|-------------------------|------------|-------------|\n",
    "|-----------|-------|Prec  | Recall | F-score |------------|-------------|\n",
    "|-----------|-------|----- |--------|---------|------------|-------------|\n",
    "|  Adam     | 0.001 | 0.66 |  0.41  |  0.50   | 10 epochs  | hidden 100  |\n",
    "|-----------|-------|----- |--------|---------|------------|-------------|\n",
    "|  Adam     | 0.005 | 0.69 |   -    |  0.53   |   6 epochs |     \"\"      |\n",
    "|-----------|-------|----- |--------|---------|------------|-------------|\n",
    "|  Adam     | 0.01  | 0.70 |  0.55  |  0.60   |   6 epochs |     100     |*\n",
    "|-----------|-------|----- |--------|---------|------------|-------------|\n",
    "|  Adamax   | 0.005 | 0.67 |  0.45  |  0.54   |  - \" \" -   |    \"\"  |\n",
    "|-----------|-------|----- |--------|---------|------------|-------------|\n",
    "|  Adamax   | 0.01  | 0.68 |  0.51  |  0.59   |   - \" \" -  |-------------|\n",
    "|-----------|-------|----- |--------|---------|------------|-------------|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis on Model 2\n",
    "\n",
    "I did not use the token level evaluation function for this model as it is for bigger sequences, so it seemed unnecessary. It also made the model train even slower, so I made this decision to produce at least some results rather than strive for perfection. However, I can report as a curiosity that the accuracy on token level dropped to about 0.45. I also took away a few epochs to get more train runs, and I would've also tried to make the hidden layer 50 with more than one pairing, if I had time. I would also run the model more than 6 to 10 epochs.\n",
    "\n",
    "\n",
    "Adam0.01 gave the smallest loss of all the versions I run, 0.0041. This model was so far the best I came up with, reaching precision of 70, recall or 50 and F-score of 60 per cent. Above, we can see that in this model, the learning rate has an effect on the model, where as the difference between use of Adam or Adamax optimizers is not that big. Still, th     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teamwork\n",
    "\n",
    "We both are very tenacious on problem solving, that I can say. With all the technical issues and other projects we had, I think we worked well together. We both took initiative and allocated time to solving the problems we faced and helped each other. We also shared responsibilities, so we were both contributing while working on different parts/issues. Communication was mostly good and frequent (Whatsapp), although more than one live meeting would have been ideal but corona. All in all, I'm pretty happy with what we managed to get done. Mostly I'm proud that I managed to sort out the issue with the data as without well-formed input, the models can't do much. Thus, that step was very important for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project post mortem\n",
    "\n",
    "The project was very interesting, challenging and educational as it was not a basic problem anymore. Sometimes I felt very confused with all the terminology and, the course being advanced level, with the requirements and specs of the project, but I did my best under the circumstances and googled, a lot. I'm looking forward to have more time to dig into the topics and learn more, which I have to do this summer at IKITIK anyway. So, this was a nice warm-up for that! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
