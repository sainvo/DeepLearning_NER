{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTBsYI1tLeVk"
   },
   "source": [
    "# Deep Learning NER task\n",
    "\n",
    "Tatjana Cucic and Sanna Volanen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9T2GevEzfPP2"
   },
   "source": [
    "https://spacy.io/api/annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5MwAmUALZ4V"
   },
   "source": [
    "# Milestones\n",
    "\n",
    "## 1.1 Predicting word labels independently\n",
    "\n",
    "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
    "* Evaluate the results on token level and entity level. \n",
    "* Report your results with different network hyperparameters. \n",
    "* Also discuss whether the token level accuracy is a reasonable metric.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "0Q3HiGQgMU5L",
    "outputId": "6aa4cfdc-f214-455a-d41a-0dde30e4ed71"
   },
   "outputs": [],
   "source": [
    "# Training data: Used for training the model\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
    "\n",
    "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
    "\n",
    "# Test data: Never touched during training / model development, used for evaluating the final model\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
    "\n",
    "#saved model\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FZhc7b6VFWYX",
    "outputId": "862727bf-32a4-432a-a348-69baba3fcef2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "csv.field_size_limit(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Tanja\\\\Desktop\\\\NER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOOHEYpiMzFp"
   },
   "outputs": [],
   "source": [
    "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
    "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
    "\n",
    "def read_ontonotes(tsv_file): # \n",
    "    current_sent = [] # list of (word,label) lists\n",
    "    #with open(tsv_file) as f:\n",
    "    with open(tsv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
    "        for line in tsvreader:\n",
    "            #print(line)\n",
    "            if not line:\n",
    "                if current_sent:\n",
    "                    yield current_sent\n",
    "                    current_sent=[]\n",
    "                continue\n",
    "            current_sent.append(line[0]) \n",
    "        else:\n",
    "            if current_sent:\n",
    "                yield current_sent\n",
    "\n",
    "train_data_full = list(read_ontonotes('train.tsv'))\n",
    "dev_data_full = list(read_ontonotes('dev.tsv'))\n",
    "test_data_full = list(read_ontonotes('test.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "q32R9o_mJZAt",
    "outputId": "456f0e4d-bd31-4f39-e61d-63c2cbf65d29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50252\n",
      "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O'], ['.', 'O']]\n",
      "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O'], ['.', 'O']]\n",
      "[['In', 'O'], ['order', 'O'], ['to', 'O'], ['attract', 'O'], ['this', 'O'], ['group', 'O'], ['of', 'O'], ['seasoned', 'O'], ['adults', 'O'], ['pulling', 'O'], ['in', 'O'], ['over', 'O'], ['NT$', 'B-MONEY'], ['1', 'I-MONEY'], ['million', 'I-MONEY'], ['a', 'O'], ['year', 'O'], ['back', 'O'], ['to', 'O'], ['the', 'O'], ['ivory', 'O'], ['tower', 'O'], [',', 'O'], ['universities', 'O'], ['have', 'O'], ['begun', 'O'], ['to', 'O'], ['establish', 'O'], ['executive', 'O'], ['MBA', 'B-WORK_OF_ART'], ['(', 'O'], ['EMBA', 'B-WORK_OF_ART'], [')', 'O'], ['programs', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "#regex for empty space chars, \\t \\n\n",
    "tab = re.compile('[\\t]')\n",
    "line = re.compile('[\\n]')\n",
    "punct = re.compile('[.?!:;]')\n",
    "\n",
    "def splitter(sent):\n",
    "    #print('----------------------------------------')\n",
    "    #print(\"one sentence in raw data:\", sent)\n",
    "    split_list = []\n",
    "    # loop over tokens items inside sentence, supposedly item= token+ \\t +tag\n",
    "    for item in sent: \n",
    "        #print(\"Item in sentence: \", item)\n",
    "        if item != None:\n",
    "            match1 = item.count('\\n')\n",
    "            #print(match1)\n",
    "            match2 = item.count('\\t')\n",
    "            #print(match2)\n",
    "            if match1 ==0: # no new lines nested\n",
    "                if match2 == 1: #just one tab inside token\n",
    "                    item_pair = item.split('\\t')\n",
    "                if item_pair[0] =='': # replacing empty string with missing quote marks\n",
    "                    item_pair[0] = '\\\"'\n",
    "                split_list.append(item_pair) \n",
    "            else:\n",
    "                subitems_list = item.split('\\n') ## check if token has \\n -> bundled, quotes\n",
    "                if len(subitems_list) > 1:  ## item string has more than one sentence nested in it\n",
    "                    #print(\"Found nested sentences: \", subitems_list)\n",
    "                    #print(\"subseq start\")\n",
    "                    for j in range(len(subitems_list)):  \n",
    "                        token = subitems_list[j]  \n",
    "                        #print(token)\n",
    "                        subtoken_listed_again = token.split('\\n') \n",
    "                        for token in subtoken_listed_again:\n",
    "                            match1=token.count('\\n')\n",
    "                            match2=token.count('\\t')\n",
    "                            if  match1 == 0: # no new lines nested\n",
    "                               if  match2 == 1: #just one tab inside token\n",
    "                                    token = token.split('\\t')\n",
    "                            if token =='': # replacing empty string with missing quote marks\n",
    "                                token = '\\\"'\n",
    "                            if token == '.':\n",
    "                                split_list.append(token)\n",
    "                                continue\n",
    "                                split_list=[]\n",
    "                            else:\n",
    "                                split_list.append(token)\n",
    "                    #print(\"subseq end\")\n",
    "    for item in split_list:\n",
    "        #print(\"Item in split list: \",item)\n",
    "        if type(item) != list:\n",
    "            split_list.remove(item)\n",
    "        if item[0] =='': # replacing empty string with missing quote marks\n",
    "            item[0] = '\\\"'\n",
    "    #print(\"Resplitted sentence :\", split_list)\n",
    "    return split_list\n",
    "\n",
    "def clean(raw_data): ## input list is list of lists of strings \n",
    "    clean_data =[]  #list of lists that have one clean sentence per list\n",
    "    for sent in raw_data: # split by [] lines, supposedly a sentence line\n",
    "        one_sentence = [] #collects the new sentence if there has been need to resplit items\n",
    "        splitted= splitter(sent)\n",
    "        for item in splitted:\n",
    "            #print(item)\n",
    "            matchi = re.match(punct, item[0])\n",
    "            if matchi:\n",
    "                #print(\"collected sentence\")\n",
    "                one_sentence.append(item)\n",
    "                clean_data.append(one_sentence)\n",
    "                one_sentence=[]\n",
    "                break\n",
    "            else:\n",
    "                one_sentence.append(item)\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "train_data_clean = clean(train_data_full)\n",
    "print(len(train_data_clean))\n",
    "for item in train_data_clean[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "jQrs8MSroPfa",
    "outputId": "6fa023af-ea39-4540-ac3e-083a85e6f1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence: 168 index:  8041\n",
      "[168, 145, 123, 121, 120, 108, 106, 106, 102, 101, 97, 96, 94, 94, 93, 93, 93, 92, 91, 91, 91, 90, 90, 90, 90, 90, 90, 89, 88, 88, 88, 87, 87, 86, 85, 85, 85, 85, 84, 84, 84, 84, 83, 83, 83, 83, 82, 82, 82, 82, 81, 81, 80, 80, 80, 79, 79, 79, 79, 79, 79, 78, 78, 78, 78, 78, 77, 77, 77, 77, 77, 77, 77, 77, 76, 76, 76, 76, 76, 76, 76, 76, 76, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n"
     ]
    }
   ],
   "source": [
    "# final check on the sentences\n",
    "item_lengths = []\n",
    "max_text = 0\n",
    "for item in train_data_clean:\n",
    "    item_lengths.append(len(item))\n",
    "    if len(item) > max_text:\n",
    "        max_text = len(item)\n",
    "        ind = train_data_clean.index(item)\n",
    "print(\"Longest sentence:\", max_text, \"index: \",ind)\n",
    "\n",
    "lengths_sorted = sorted(item_lengths, reverse=True)\n",
    "max = item_lengths.index(max_text)\n",
    "#print(items_sorted[0])\n",
    "#pprint(train_data_clean[max])\n",
    "print(lengths_sorted[:300]) # longest sentences\n",
    "# checking long items\n",
    "#for item in train_data_clean:\n",
    "    #if len(item) == 123:\n",
    "        #pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'O'],\n",
       " ['rational', 'O'],\n",
       " ['approach', 'O'],\n",
       " ['to', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['would', 'O'],\n",
       " ['entail', 'O'],\n",
       " ['(', 'O'],\n",
       " ['1', 'O'],\n",
       " [')', 'O'],\n",
       " ['creating', 'O'],\n",
       " ['communities', 'O'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['mix', 'O'],\n",
       " ['of', 'O'],\n",
       " ['housing', 'O'],\n",
       " ['/', 'O'],\n",
       " ['office', 'O'],\n",
       " ['/', 'O'],\n",
       " ['retail', 'O'],\n",
       " ['/', 'O'],\n",
       " ['amenities', 'O'],\n",
       " ['to', 'O'],\n",
       " ['reduce', 'O'],\n",
       " ['the', 'O'],\n",
       " ['number', 'O'],\n",
       " ['and', 'O'],\n",
       " ['length', 'O'],\n",
       " ['of', 'O'],\n",
       " ['automobile', 'O'],\n",
       " ['trips', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['2', 'O'],\n",
       " [')', 'O'],\n",
       " ['reforming', 'O'],\n",
       " ['counterproductive', 'O'],\n",
       " ['zoning', 'O'],\n",
       " ['codes', 'O'],\n",
       " [',', 'O'],\n",
       " ['subdivision', 'O'],\n",
       " ['ordinances', 'O'],\n",
       " ['and', 'O'],\n",
       " ['comprehensive', 'O'],\n",
       " ['plans', 'O'],\n",
       " ['in', 'O'],\n",
       " ['order', 'O'],\n",
       " ['to', 'O'],\n",
       " ['create', 'O'],\n",
       " ['more', 'O'],\n",
       " ['transit', 'O'],\n",
       " ['-', 'O'],\n",
       " ['friendly', 'O'],\n",
       " ['and', 'O'],\n",
       " ['pedestrian', 'O'],\n",
       " ['-', 'O'],\n",
       " ['friendly', 'O'],\n",
       " ['communities', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['3', 'O'],\n",
       " [')', 'O'],\n",
       " ['creating', 'O'],\n",
       " ['funding', 'O'],\n",
       " ['sources', 'O'],\n",
       " ['for', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['that', 'O'],\n",
       " ['establishes', 'O'],\n",
       " ['a', 'O'],\n",
       " ['rational', 'O'],\n",
       " ['nexus', 'O'],\n",
       " ['between', 'O'],\n",
       " ['those', 'O'],\n",
       " ['who', 'O'],\n",
       " ['use', 'O'],\n",
       " ['/', 'O'],\n",
       " ['benefit', 'O'],\n",
       " ['from', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['facilities', 'O'],\n",
       " ['and', 'O'],\n",
       " ['those', 'O'],\n",
       " ['who', 'O'],\n",
       " ['pay', 'O'],\n",
       " ['for', 'O'],\n",
       " ['them', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['4', 'O'],\n",
       " [')', 'O'],\n",
       " ['promoting', 'O'],\n",
       " ['telework', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['5', 'O'],\n",
       " [')', 'O'],\n",
       " ['liberating', 'O'],\n",
       " ['mass', 'O'],\n",
       " ['transit', 'O'],\n",
       " ['from', 'O'],\n",
       " ['government', 'O'],\n",
       " ['monopolies', 'O'],\n",
       " ['and', 'O'],\n",
       " ['encouraging', 'O'],\n",
       " ['private', 'O'],\n",
       " ['sector', 'O'],\n",
       " ['innovation', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['6', 'O'],\n",
       " [')', 'O'],\n",
       " ['using', 'O'],\n",
       " ['a', 'O'],\n",
       " ['wide', 'O'],\n",
       " ['range', 'O'],\n",
       " ['of', 'O'],\n",
       " ['tools', 'O'],\n",
       " [',', 'O'],\n",
       " ['from', 'O'],\n",
       " ['corridor', 'O'],\n",
       " ['management', 'O'],\n",
       " ['to', 'O'],\n",
       " ['intelligent', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['systems', 'O'],\n",
       " [',', 'O'],\n",
       " ['to', 'O'],\n",
       " ['increase', 'O'],\n",
       " ['the', 'O'],\n",
       " ['capacity', 'O'],\n",
       " ['of', 'O'],\n",
       " ['existing', 'O'],\n",
       " ['thoroughfares', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['7', 'O'],\n",
       " [')', 'O'],\n",
       " ['stretching', 'O'],\n",
       " ['the', 'O'],\n",
       " ['value', 'O'],\n",
       " ['of', 'O'],\n",
       " ['road', 'O'],\n",
       " ['construction', 'O'],\n",
       " ['/', 'O'],\n",
       " ['maintenance', 'O'],\n",
       " ['dollars', 'O'],\n",
       " ['through', 'O'],\n",
       " ['outsourcing', 'O'],\n",
       " [',', 'O'],\n",
       " ['and', 'O'],\n",
       " ['(', 'O'],\n",
       " ['8', 'O'],\n",
       " [')', 'O'],\n",
       " ['prioritizing', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['projects', 'O'],\n",
       " ['on', 'O'],\n",
       " ['a', 'O'],\n",
       " ['congestion', 'O'],\n",
       " ['-', 'O'],\n",
       " ['mitigation', 'O'],\n",
       " ['Return', 'O'],\n",
       " ['on', 'O'],\n",
       " ['Investment', 'O'],\n",
       " ['basis', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np   #want to see the longest sentence\n",
    "\n",
    "lengths = np.array(item_lengths)\n",
    "train_data_clean[np.argmax(lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In', 'O'],\n",
       " ['recent', 'B-DATE'],\n",
       " ['years', 'I-DATE'],\n",
       " [',', 'O'],\n",
       " ['advanced', 'O'],\n",
       " ['education', 'O'],\n",
       " ['for', 'O'],\n",
       " ['professionals', 'O'],\n",
       " ['has', 'O'],\n",
       " ['become', 'O'],\n",
       " ['a', 'O'],\n",
       " ['hot', 'O'],\n",
       " ['topic', 'O'],\n",
       " ['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['business', 'O'],\n",
       " ['community', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "tk8Brh3sd6dl",
    "outputId": "21341a1a-f969-4fca-dda8-046c2d75306c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "9954\n",
      "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON'], ['.', 'O']]\n",
      "[['On', 'O'], ['August', 'B-DATE'], ['25', 'I-DATE'], ['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['wrapped', 'O'], ['up', 'O'], ['his', 'O'], ['first', 'B-ORDINAL'], ['overseas', 'O'], ['trip', 'O'], ['since', 'O'], ['taking', 'O'], ['office', 'O'], [',', 'O'], ['swinging', 'O'], ['through', 'O'], ['three', 'B-CARDINAL'], ['countries', 'O'], ['in', 'O'], ['Latin', 'B-LOC'], ['America', 'I-LOC'], ['and', 'O'], ['another', 'O'], ['three', 'B-CARDINAL'], ['in', 'O'], ['Africa', 'B-LOC'], ['.', 'O']]\n",
      "[['While', 'O'], ['in', 'O'], ['the', 'B-GPE'], ['Dominican', 'I-GPE'], ['Republic', 'I-GPE'], ['to', 'O'], ['attend', 'O'], ['the', 'O'], ['inauguration', 'O'], ['of', 'O'], ['President', 'O'], ['Hipolito', 'B-PERSON'], ['Mejia', 'I-PERSON'], [',', 'O'], ['Chen', 'B-PERSON'], ['had', 'O'], ['a', 'O'], ['chance', 'O'], ['to', 'O'], ['meet', 'O'], ['with', 'O'], ['the', 'O'], ['leaders', 'O'], ['of', 'O'], ['many', 'O'], ['different', 'O'], ['nations', 'O'], ['.', 'O']]\n",
      "------------------------------------------\n",
      "7920\n",
      "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O'], ['.', 'O']]\n",
      "[['Worldwide', 'O'], [',', 'O'], ['biotechnology', 'O'], ['is', 'O'], ['a', 'O'], ['rising', 'O'], ['star', 'O'], ['of', 'O'], ['the', 'O'], ['industrial', 'O'], ['stage', 'O'], ['.', 'O']]\n",
      "[['In', 'O'], ['Taiwan', 'B-GPE'], [',', 'O'], ['in', 'O'], ['recent', 'O'], ['years', 'O'], ['the', 'B-ORG'], ['Ministry', 'I-ORG'], ['of', 'I-ORG'], ['Economic', 'I-ORG'], ['Affairs', 'I-ORG'], [',', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Science', 'I-ORG'], ['Council', 'I-ORG'], ['and', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Health', 'I-ORG'], ['Research', 'I-ORG'], ['Institutes', 'I-ORG'], ['have', 'O'], ['been', 'O'], ['strongly', 'O'], ['pursuing', 'O'], ['\"', 'O'], ['biochip', 'O'], ['\"', 'O'], ['research', 'O'], ['programs', 'O'], ['.', 'O']]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('------------------------------------------')\n",
    "dev_data_clean = clean(dev_data_full)\n",
    "print(len(dev_data_clean))\n",
    "for item in dev_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')\n",
    "test_data_clean = clean(test_data_full)\n",
    "print(len(test_data_clean))\n",
    "for item in test_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "cZHhXzVTQA_P",
    "outputId": "aa4715b9-a966-490a-d09e-6ff8c947ba92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
      "\n",
      "[{'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']}, {'text': ['On', 'August', '25', 'President', 'Chen', 'Shui', '-', 'bian', 'wrapped', 'up', 'his', 'first', 'overseas', 'trip', 'since', 'taking', 'office', ',', 'swinging', 'through', 'three', 'countries', 'in', 'Latin', 'America', 'and', 'another', 'three', 'in', 'Africa', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-CARDINAL', 'O', 'B-LOC', 'O']}, {'text': ['While', 'in', 'the', 'Dominican', 'Republic', 'to', 'attend', 'the', 'inauguration', 'of', 'President', 'Hipolito', 'Mejia', ',', 'Chen', 'had', 'a', 'chance', 'to', 'meet', 'with', 'the', 'leaders', 'of', 'many', 'different', 'nations', '.'], 'tags': ['O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n"
     ]
    }
   ],
   "source": [
    "# shape into dicts per sentence\n",
    "\n",
    "def reshape_sent2dicts(f):\n",
    "    data_dict = []\n",
    "    for item in f: # list of lists (tokens)\n",
    "        #print(item)\n",
    "        sent_text= [] \n",
    "        sent_tags = []\n",
    "        for token in item:\n",
    "            if len(token) ==2:\n",
    "                sent_text.append(token[0])\n",
    "                sent_tags.append(token[1])\n",
    "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
    "        #print(sent_dict['text'])\n",
    "        #print(sent_dict['tags'])\n",
    "        data_dict.append(sent_dict)\n",
    "    return data_dict\n",
    "\n",
    "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))\n",
    "samp = train_data_sent[:2]\n",
    "print(samp)\n",
    "print()\n",
    "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
    "samp2 = dev_data_sent[:3]\n",
    "print(samp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "VpYBQMbcQGBi",
    "outputId": "91c6e6a3-b808-43b6-8e20-9ecad77fbc1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'text': ['Does', 'it', 'wake', 'you', 'up', '?'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O']}\n",
      "\n",
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "------------\n",
      "0\n",
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----------------------------\n",
      "30000\n",
      "-----------------------\n",
      "Text:  ['Does', 'it', 'wake', 'you', 'up', '?']\n",
      " Texts length:  30000\n",
      "Label:  ['O', 'O', 'O', 'O', 'O', 'O']\n",
      " Labels length:  30000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(train_data_sent)\n",
    "#max_sent = [max(len(i[\"text\"])) for i in train_data_sent]\n",
    "#print(max_sent)\n",
    "print(type(train_data_sent))\n",
    "print(train_data_sent[0]) ##one dict\n",
    "print()\n",
    "print(train_data_sent[0][\"text\"])\n",
    "print()\n",
    "print(train_data_sent[0][\"tags\"])\n",
    "print('------------')\n",
    "\n",
    "def typed_listing(data, key):\n",
    "    listed = []\n",
    "    max_length = 0\n",
    "    for item in data: # dictionary {text:\"\", tags:\"\"}\n",
    "        #print('Item: ', item)\n",
    "        #print('Key: ', key, ' content: ', item[key], 'length: ',len(item[key]))\n",
    "        if len(item[key]) > max_length:\n",
    "            max = len(item[key])\n",
    "        listed.append(item[key])\n",
    "    return listed, max_length\n",
    "\n",
    "listed_texts= typed_listing(train_data_sent, \"text\")\n",
    "train_texts = listed_texts[0]\n",
    "train_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(train_data_sent, \"tags\")\n",
    "train_labels= listed_labels[0]\n",
    "train_lbl_max = listed_labels[1]\n",
    "print(train_txt_max)\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "\n",
    "\n",
    "print('-----------------------------')\n",
    "print(len(train_texts))\n",
    "print('-----------------------')\n",
    "print('Text: ', train_texts[0])\n",
    "print(' Texts length: ',len(train_texts))\n",
    "print('Label: ', train_labels[0])\n",
    "print(' Labels length: ',len(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "DNQQRw0YO-Ng",
    "outputId": "474f0076-ea05-457c-aad4-13c726b7eb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.']\n",
      " Texts length:  9954\n",
      "Label:  ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']\n",
      " Labels length:  9954\n"
     ]
    }
   ],
   "source": [
    "## same for validation/dev data\n",
    "listed_texts= typed_listing(dev_data_sent, \"text\")\n",
    "dev_texts = listed_texts[0]\n",
    "dev_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(dev_data_sent, \"tags\")\n",
    "dev_labels= listed_labels[0]\n",
    "dev_lbl_max = listed_labels[1]\n",
    "print('Text: ', dev_texts[0])\n",
    "print(' Texts length: ',len(dev_texts))\n",
    "print('Label: ', dev_labels[0])\n",
    "print(' Labels length: ',len(dev_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ICn08fOgbyXl",
    "outputId": "f0c0a0e9-1dca-4d7b-9fa9-264dea736c5a"
   },
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "#!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bFv2qclTbyXp",
    "outputId": "06369298-d917-4d82-b580-04a09bef3870"
   },
   "outputs": [],
   "source": [
    "# Give -n argument so that a possible existing file isn't overwritten \n",
    "#!unzip -n wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "kx_C5ii8byXt",
    "outputId": "9f9fb2d2-1c77-417d-cbd7-638b0022ef3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
    "print(\"Words from embedding model:\", len(words))\n",
    "print(\"First 50 words:\", words[:50])\n",
    "\n",
    "# Normalize the vectors to unit length\n",
    "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkdgjgOlbyXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary mappings\n",
    "\n",
    "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
    "# Also reserve an index for out-of-vocabulary items.\n",
    "vocabulary={\n",
    "    \"<PAD>\": 0,\n",
    "    \"<OOV>\": 1\n",
    "}\n",
    "\n",
    "for word in words: # These are words from the word2vec model\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
    "\n",
    "\n",
    "# Embedding matrix\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGaojUBhbyX2"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_M9Ox5_ObyX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-CARDINAL': 26,\n",
      " 'B-DATE': 23,\n",
      " 'B-EVENT': 35,\n",
      " 'B-FAC': 17,\n",
      " 'B-GPE': 36,\n",
      " 'B-LANGUAGE': 15,\n",
      " 'B-LAW': 1,\n",
      " 'B-LOC': 14,\n",
      " 'B-MONEY': 6,\n",
      " 'B-NORP': 3,\n",
      " 'B-ORDINAL': 31,\n",
      " 'B-ORG': 21,\n",
      " 'B-PERCENT': 28,\n",
      " 'B-PERSON': 13,\n",
      " 'B-PRODUCT': 33,\n",
      " 'B-QUANTITY': 2,\n",
      " 'B-TIME': 29,\n",
      " 'B-WORK_OF_ART': 32,\n",
      " 'I-CARDINAL': 24,\n",
      " 'I-DATE': 5,\n",
      " 'I-EVENT': 11,\n",
      " 'I-FAC': 18,\n",
      " 'I-GPE': 10,\n",
      " 'I-LANGUAGE': 27,\n",
      " 'I-LAW': 16,\n",
      " 'I-LOC': 0,\n",
      " 'I-MONEY': 34,\n",
      " 'I-NORP': 7,\n",
      " 'I-ORDINAL': 22,\n",
      " 'I-ORG': 9,\n",
      " 'I-PERCENT': 30,\n",
      " 'I-PERSON': 19,\n",
      " 'I-PRODUCT': 20,\n",
      " 'I-QUANTITY': 12,\n",
      " 'I-TIME': 4,\n",
      " 'I-WORK_OF_ART': 8,\n",
      " 'O': 25}\n"
     ]
    }
   ],
   "source": [
    "#Labels\n",
    "\n",
    "\n",
    "not_letter = re.compile(r'[^a-zA-Z]')\n",
    "# Label mappings\n",
    "# 1) gather a set of unique labels\n",
    "label_set = set()\n",
    "for sentence_labels in train_labels: #loops over sentences \n",
    "    #print(sentence_labels)\n",
    "    for label in sentence_labels: #loops over labels in one sentence\n",
    "       # match = not_letter.match(label)\n",
    "        #if match or label== 'O':\n",
    "        #    break\n",
    "        #else:    \n",
    "        label_set.add(label)\n",
    "\n",
    "# 2) index these\n",
    "label_map = {}\n",
    "for index, label in enumerate(label_set):\n",
    "    label_map[label]=index\n",
    "    \n",
    "pprint(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8k8DshceEaI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 25, 25, 25, 25, 25]\n"
     ]
    }
   ],
   "source": [
    "# vectorize the labels\n",
    "def label_vectorizer(train_labels,label_map):\n",
    "    vectorized_labels = []\n",
    "    for label in train_labels:\n",
    "        vectorized_example_label = []\n",
    "        for token in label:\n",
    "            if token in label_map:\n",
    "                vectorized_example_label.append(label_map[token])\n",
    "        vectorized_labels.append(vectorized_example_label)\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    return vectorized_labels\n",
    "        \n",
    "\n",
    "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
    "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
    "\n",
    "print(vectorized_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUtqLdCMPf3X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "[3328, 21, 8846, 32, 91, 39]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## vectorization of the texts\n",
    "def text_vectorizer(vocab, train_texts):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(train_texts):\n",
    "        vectorized_example = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "\n",
    "        vectorized_data.append(vectorized_example)     \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
    "    \n",
    "    return vectorized_data, sentence_lengths\n",
    "\n",
    "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
    "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
    "\n",
    "print(train_texts[0])\n",
    "print(vectorized_data[0])\n",
    "pprint(type(lengths))\n",
    "#max = lengths.index(17040)\n",
    "#print(max)\n",
    "#pprint(train_texts[11103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9e6FH5F1QGrq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (30000,)\n",
      "New shape: (30000, 168)\n",
      "First example:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 3328   21 8846   32   91   39]\n",
      "<class 'numpy.ndarray'>\n",
      "Padded labels shape: (30000, 168, 1)\n",
      "First example labels:\n",
      "First weight vector:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# padding for tensor\n",
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "max_len = np.max(np.array(lengths))\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max_len)\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\")\n",
    "print( vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=np.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max_len), -1)\n",
    "\n",
    "print(type(vectorized_labels_padded))\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "#pprint(label_map)\n",
    "print(\"First example labels:\")\n",
    "#pprint(vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\")\n",
    "print( weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max_len)\n",
    "validation_vectorized_labels_padded=np.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max_len), -1)\n",
    "validation_weights = np.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhAOVAbBTRAv"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "import keras\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    return entity_offsets\n",
    "\n",
    "def _entity_level_PRF(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    \n",
    "    try:\n",
    "        precision = tp / pred_count # tp / (tp+np)\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "    return precision, recall, fscore             \n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.fscore = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
    "        self.precision.append(evaluation_parameters[0])\n",
    "        self.recall.append(evaluation_parameters[1])\n",
    "        self.fscore.append(evaluation_parameters[2])\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVS-GKkQUpue"
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed, SimpleRNN\n",
    "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = Dense(hidden_size, activation=\"relu\")(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "model1 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gj3x13FELJLt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 168)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 168, 50)           15050     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 168, 37)           1887      \n",
      "=================================================================\n",
      "Total params: 15,017,537\n",
      "Trainable params: 16,937\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuWSrGPVUw9Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 80s - loss: 0.0249\n",
      "\n",
      "Precision/Recall/F-score: 0.4444444444444444 / 0.0004007212983370066 / 0.0008007206485837253\n",
      "Epoch 2/10\n",
      " - 80s - loss: 0.0247\n",
      "\n",
      "Precision/Recall/F-score: 0.5 / 0.0005009016229212583 / 0.00100080064051241\n",
      "Epoch 3/10\n",
      " - 82s - loss: 0.0246\n",
      "\n",
      "Precision/Recall/F-score: 0.5 / 0.0005009016229212583 / 0.00100080064051241\n",
      "Epoch 4/10\n",
      " - 89s - loss: 0.0245\n",
      "\n",
      "Precision/Recall/F-score: 0.5 / 0.0005009016229212583 / 0.00100080064051241\n",
      "Epoch 5/10\n",
      " - 93s - loss: 0.0245\n",
      "\n",
      "Precision/Recall/F-score: 0.5 / 0.0005009016229212583 / 0.00100080064051241\n",
      "Epoch 6/10\n",
      " - 95s - loss: 0.0246\n",
      "\n",
      "Precision/Recall/F-score: 0.45454545454545453 / 0.0005009016229212583 / 0.0010007004903432402\n",
      "Epoch 7/10\n",
      " - 94s - loss: 0.0246\n",
      "\n",
      "Precision/Recall/F-score: 0.5 / 0.0005009016229212583 / 0.00100080064051241\n",
      "Epoch 8/10\n",
      " - 94s - loss: 0.0245\n",
      "\n",
      "Precision/Recall/F-score: 0.45454545454545453 / 0.0005009016229212583 / 0.0010007004903432402\n",
      "Epoch 9/10\n",
      " - 88s - loss: 0.0245\n",
      "\n",
      "Precision/Recall/F-score: 0.5 / 0.0005009016229212583 / 0.00100080064051241\n",
      "Epoch 10/10\n",
      " - 95s - loss: 0.0245\n",
      "\n",
      "Precision/Recall/F-score: 0.5 / 0.0005009016229212583 / 0.00100080064051241\n"
     ]
    }
   ],
   "source": [
    "# train model 1\n",
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "model1.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "vanilla_hist=model1.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=1,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU and Adam gave bad results with different learning rates and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecFC_-OhC1lN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: [0.0008007206485837253, 0.00100080064051241, 0.00100080064051241, 0.00100080064051241, 0.00100080064051241, 0.0010007004903432402, 0.00100080064051241, 0.0010007004903432402, 0.00100080064051241, 0.00100080064051241]\n",
      "Highest f-score: 0.00100080064051241\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the f scores\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "def plot_history(fscores):\n",
    "    max_f = np.max(np.array(fscores))\n",
    "    print(\"History:\", fscores)\n",
    "    print(\"Highest f-score:\", max_f)\n",
    "    plt.plot(fscores)\n",
    "    plt.legend(loc='lower center', borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\tanja\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from h5py) (1.17.4)\n",
      "Requirement already satisfied: six in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from h5py) (1.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"Model1_Adam.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwMwwo9-MPv4"
   },
   "source": [
    "## 1.2 Expand context\n",
    "\n",
    "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lw9pXRewbyX6"
   },
   "outputs": [],
   "source": [
    "#expanding to RNN model with context\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "rnn_size = 100\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
    "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPP-kwoNXUMb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 168)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 168, 100)          160400    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 168, 37)           3737      \n",
      "=================================================================\n",
      "Total params: 15,164,737\n",
      "Trainable params: 164,137\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIal9meVXnN_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1260s - loss: nan\n",
      "\n",
      "Precision/Recall/F-score: 0.9987944544906571 / 0.99599278701663 / 0.9973916532905296\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-5c34cba9567d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrnn_hist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized_data_padded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvectorized_labels_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevaluation_function\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3740\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3742\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m     \"\"\"\n\u001b[1;32m-> 1081\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1121\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer=Adam(lr=1e-10) # define the learning rate\n",
    "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRKNs4t8X3Ed"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCo0xF5kMMbH"
   },
   "source": [
    "## 2.1 Use deep contextual representations\n",
    "\n",
    "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgSYNcerMI9R"
   },
   "source": [
    "## 2.2 Error analysis\n",
    "\n",
    "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to see how the model predicts classes\n",
    "\n",
    "\n",
    "# Recreate the exact same bi_lstm, including its weights and the optimizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "simple_rnn = tf.keras.models.load_model('Model1_Adam.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_predictions = bi_lstm.predict(vectorized_data_padded)  #save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_predictions[0][0]   #see what the output looks like\n",
    "#The outputs are probabilities for each class (tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "list_of_tag_args = []    #create a list of saved positions for each entity label\n",
    "for i in simple_rnn_predictions:\n",
    "    for j in i:\n",
    "        temp = np.argmax(j)\n",
    "        list_of_tag_args.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tag_args  #Now that we have this list, we can use our label map dictionary to identify the actual tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRDxKgLSL_uf"
   },
   "source": [
    "## 3.1 Predictions on unannotated text\n",
    "\n",
    "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlG6ZWkIL-HY"
   },
   "source": [
    "## 3.2 Statistically analyze the results\n",
    "\n",
    "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DL_NER_debug2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
