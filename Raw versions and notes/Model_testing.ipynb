{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O26whGApdc74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "e7771500-e93f-4ef5-b2b1-9d424032f09c"
      },
      "source": [
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
        "\n",
        "#saved model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
        "\n",
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "\n",
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-12 16:29:19--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1788466 (1.7M) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "\rtest.tsv              0%[                    ]       0  --.-KB/s               \rtest.tsv            100%[===================>]   1.71M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-05-12 16:29:19 (23.3 MB/s) - ‘test.tsv’ saved [1788466/1788466]\n",
            "\n",
            "--2020-05-12 16:29:20--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60228624 (57M) [application/octet-stream]\n",
            "Saving to: ‘Adamax90.h5’\n",
            "\n",
            "Adamax90.h5         100%[===================>]  57.44M   200MB/s    in 0.3s    \n",
            "\n",
            "2020-05-12 16:29:22 (200 MB/s) - ‘Adamax90.h5’ saved [60228624/60228624]\n",
            "\n",
            "--2020-05-12 16:29:23--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  24.0MB/s    in 28s     \n",
            "\n",
            "2020-05-12 16:29:52 (23.3 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n",
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MA3N6YJdoW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4452e407-20f5-417f-8337-92fc76ab77de"
      },
      "source": [
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mgqPSmmdptW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
        "\n",
        "def read_ontonotes(tsv_file): # \n",
        "    current_sent = [] # list of (word,label) lists\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line:\n",
        "                if current_sent:\n",
        "                    yield current_sent\n",
        "                    current_sent=[]\n",
        "                continue\n",
        "            current_sent.append(line) \n",
        "        else:\n",
        "            if current_sent:\n",
        "                yield current_sent\n",
        "\n",
        "full_test_data = list(read_ontonotes('test.tsv'))\n",
        "#size_ts = int(len(full_test_data)/2)\n",
        "#print(size_ts)\n",
        "test_data_sample = full_test_data[:500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_xrpyV1d2Tt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f619bfd4-aea3-4a11-b215-d0b9ec5f7aa2"
      },
      "source": [
        "import re\n",
        "#regex for empty space chars, \\t \\n\n",
        "tab = re.compile('[\\t]')\n",
        "\n",
        "def clean(list):\n",
        "    clean_data =[]\n",
        "    for sent in list:\n",
        "        clean_list = []\n",
        "        for item in sent:\n",
        "            str = ''.join(item)\n",
        "            #match_nl = re.match(r\"\\n\", str)\n",
        "            #print(match_nl)\n",
        "            count_tab =  re.findall(r\"\\t\", str)\n",
        "            #print(count_tab)\n",
        "            if len(count_tab) == 1: \n",
        "                item = re.split(\"\\t\", str)\n",
        "                if item[0] != '.':\n",
        "                    clean_list.append(item)\n",
        "            elif len(count_tab) > 1:\n",
        "                item = re.split(\"\\n\", str)\n",
        "                #print(item)\n",
        "                for i in range(len(item)):\n",
        "                    #print(item[i])\n",
        "                    if i == 0 or i == len(item)-1:\n",
        "                        item[i] = '\"'+item[i]\n",
        "                        item[i] = re.split(\"\\t\", item[i])\n",
        "                        #print(item[i])\n",
        "                    else:\n",
        "                        item[i] = re.split(\"\\t\", item[i])\n",
        "                        #print(item[i])\n",
        "                    clean_list.append(item[i])\n",
        "        clean_data.append(clean_list)        \n",
        "    return clean_data\n",
        "\n",
        "test_data_clean = clean(test_data_sample)\n",
        "print(len(test_data_clean))\n",
        "for item in test_data_clean[:3]:\n",
        "    print(item)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "[['Powerful', 'B-WORK_OF_ART'], ['Tools', 'I-WORK_OF_ART'], ['for', 'I-WORK_OF_ART'], ['Biotechnology', 'I-WORK_OF_ART'], ['-', 'I-WORK_OF_ART'], ['Biochips', 'I-WORK_OF_ART']]\n",
            "[['(', 'O'], ['Chang', 'B-PERSON'], ['Chiung', 'I-PERSON'], ['-', 'I-PERSON'], ['fang', 'I-PERSON'], ['/', 'O'], ['photos', 'O'], ['by', 'O'], ['Hsueh', 'B-PERSON'], ['Chi', 'I-PERSON'], ['-', 'I-PERSON'], ['kuang', 'I-PERSON'], ['/', 'O'], ['tr.', 'O'], ['by', 'O'], ['Robert', 'B-PERSON'], ['Taylor', 'I-PERSON'], [')', 'O']]\n",
            "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qogWYHhFeBby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c5d3298b-5b1b-4a60-a277-09a78ca9b7ef"
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for item in f: # list of lists (tokens)\n",
        "        #print(item)\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for token in item:\n",
        "            if len(token) ==2:\n",
        "                sent_text.append(token[0])\n",
        "                sent_tags.append(token[1])\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict['text'])\n",
        "        #print(sent_dict['tags'])\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "test_data_sent = list(reshape_sent2dicts(test_data_clean[:30000]))\n",
        "\n",
        "print(test_data_sent[:3])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': ['Powerful', 'Tools', 'for', 'Biotechnology', '-', 'Biochips'], 'tags': ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']}, {'text': ['(', 'Chang', 'Chiung', '-', 'fang', '/', 'photos', 'by', 'Hsueh', 'Chi', '-', 'kuang', '/', 'tr.', 'by', 'Robert', 'Taylor', ')'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O']}, {'text': ['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7St12VBeDQD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a3fe5bd1-dc21-4c76-a237-f32debae6ba2"
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "random.seed(123)\n",
        "random.shuffle(test_data_sent)\n",
        "print(type(test_data_sent))\n",
        "print(type(test_data_sent[0]))\n",
        "\n",
        "test_texts=[i[\"text\"] for i in test_data_sent]\n",
        "test_labels=[i[\"tags\"] for i in test_data_sent]\n",
        "\n",
        "#print(type(train_texts))\n",
        "#print(type(train_texts[0]))\n",
        "\n",
        "print('Text: ', test_texts[:4])\n",
        "print('Labels: ', test_labels[:4])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "Text:  [['Do', 'I', 'like', 'the', 'person', 'I', 'see', 'in', 'the', 'mirror', '?'], ['Ugly', 'duckling', 'no', 'more'], ['His', 'wife', 'was', 'a', 'universally', 'admired', 'beauty', ',', 'the', 'envy', 'of', 'all'], ['The', 'enterovirus', 'detection', 'biochip', 'developed', 'by', 'DR.', 'Chip', 'Biotechnology', 'takes', 'only', 'six', 'hours', 'to', 'give', 'hospitals', 'the', 'answer', 'to', 'whether', 'a', 'sample', 'contains', 'enterovirus', ',', 'and', 'if', 'it', 'is', 'the', 'deadly', 'strain', 'Entero', '71']]\n",
            "Labels:  [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMf-ISgaeRYZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "0f258ab1-ae05-4d8b-bfbd-55e83d72ce59"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fPUHHMseSHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef0502fa-591f-4eb7-bde8-86104fe2841e"
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kth-bcYvfE2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "6dc3f27c-aca0-4c61-ea9f-58a2cbf3cd29"
      },
      "source": [
        "#Labels\n",
        "from pprint import pprint\n",
        "\n",
        "not_letter = re.compile(r'[^a-zA-Z]')\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in test_labels: #loops over sentences \n",
        "    #print(sentence_labels)\n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "       # match = not_letter.match(label)\n",
        "        #if match or label== 'O':\n",
        "        #    break\n",
        "        #else:    \n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 14,\n",
            " 'B-DATE': 8,\n",
            " 'B-EVENT': 16,\n",
            " 'B-FAC': 13,\n",
            " 'B-GPE': 19,\n",
            " 'B-LANGUAGE': 21,\n",
            " 'B-LOC': 7,\n",
            " 'B-MONEY': 22,\n",
            " 'B-NORP': 28,\n",
            " 'B-ORDINAL': 5,\n",
            " 'B-ORG': 3,\n",
            " 'B-PERCENT': 23,\n",
            " 'B-PERSON': 10,\n",
            " 'B-QUANTITY': 17,\n",
            " 'B-TIME': 26,\n",
            " 'B-WORK_OF_ART': 18,\n",
            " 'I-CARDINAL': 11,\n",
            " 'I-DATE': 4,\n",
            " 'I-EVENT': 2,\n",
            " 'I-FAC': 9,\n",
            " 'I-GPE': 12,\n",
            " 'I-LOC': 6,\n",
            " 'I-MONEY': 27,\n",
            " 'I-NORP': 15,\n",
            " 'I-ORG': 20,\n",
            " 'I-PERCENT': 24,\n",
            " 'I-PERSON': 1,\n",
            " 'I-QUANTITY': 30,\n",
            " 'I-TIME': 29,\n",
            " 'I-WORK_OF_ART': 25,\n",
            " 'O': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgXn15G4fJAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8550092f-cb89-4948-e917-ebdd114cb14c"
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in test_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            if token in label_map:\n",
        "                vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(test_labels,label_map)\n",
        "#validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "pprint(vectorized_labels[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjUqBVFTe079",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1ae13744-b9e2-4aa3-b441-210cb61d1305"
      },
      "source": [
        "## vectorization of the text\n",
        "def text_vectorizer(vocab, test_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(test_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, test_texts)\n",
        "#validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "\n",
        "pprint(test_texts[0])\n",
        "pprint(vectorized_data[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Do', 'I', 'like', 'the', 'person', 'I', 'see', 'in', 'the', 'mirror', '?']\n",
            "[1955, 30, 68, 3, 363, 30, 138, 8, 3, 5563, 39]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62LxLsa1yCuM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ab9f432-2757-49eb-e0f1-43b9de9d2c08"
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max(lengths))\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max(lengths)), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Old shape: (500,)\n",
            "New shape: (500, 76)\n",
            "First example:\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0 1955   30   68    3  363\n",
            "   30  138    8    3 5563   39]\n",
            "Padded labels shape: (500, 76, 1)\n",
            "{'B-CARDINAL': 14,\n",
            " 'B-DATE': 8,\n",
            " 'B-EVENT': 16,\n",
            " 'B-FAC': 13,\n",
            " 'B-GPE': 19,\n",
            " 'B-LANGUAGE': 21,\n",
            " 'B-LOC': 7,\n",
            " 'B-MONEY': 22,\n",
            " 'B-NORP': 28,\n",
            " 'B-ORDINAL': 5,\n",
            " 'B-ORG': 3,\n",
            " 'B-PERCENT': 23,\n",
            " 'B-PERSON': 10,\n",
            " 'B-QUANTITY': 17,\n",
            " 'B-TIME': 26,\n",
            " 'B-WORK_OF_ART': 18,\n",
            " 'I-CARDINAL': 11,\n",
            " 'I-DATE': 4,\n",
            " 'I-EVENT': 2,\n",
            " 'I-FAC': 9,\n",
            " 'I-GPE': 12,\n",
            " 'I-LOC': 6,\n",
            " 'I-MONEY': 27,\n",
            " 'I-NORP': 15,\n",
            " 'I-ORG': 20,\n",
            " 'I-PERCENT': 24,\n",
            " 'I-PERSON': 1,\n",
            " 'I-QUANTITY': 30,\n",
            " 'I-TIME': 29,\n",
            " 'I-WORK_OF_ART': 25,\n",
            " 'O': 0}\n",
            "First example labels:\n",
            "array([[0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0],\n",
            "       [0]], dtype=int32)\n",
            "First weight vector:\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH1CNZPZt20e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_EL = load_model('Adamax90.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhAgPfI_xPBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_EL.predict_classes(test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}