{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainvo/DeepLearning_NER/blob/master/DL_NER_Adadelta70.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTBsYI1tLeVk"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2GevEzfPP2",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/api/annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O5MwAmUALZ4V"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q3HiGQgMU5L",
        "outputId": "f8887991-249d-43d4-e1e2-82d06fa88e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# Training data: Used for training the model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
        "\n",
        "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
        "\n",
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-11 17:09:40--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17252156 (16M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "\rtrain.tsv             0%[                    ]       0  --.-KB/s               \rtrain.tsv           100%[===================>]  16.45M   101MB/s    in 0.2s    \n",
            "\n",
            "2020-05-11 17:09:41 (101 MB/s) - ‘train.tsv’ saved [17252156/17252156]\n",
            "\n",
            "--2020-05-11 17:09:42--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2419425 (2.3M) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>]   2.31M  14.3MB/s    in 0.2s    \n",
            "\n",
            "2020-05-11 17:09:43 (14.3 MB/s) - ‘dev.tsv’ saved [2419425/2419425]\n",
            "\n",
            "--2020-05-11 17:09:44--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1788466 (1.7M) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>]   1.71M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-11 17:09:44 (13.2 MB/s) - ‘test.tsv’ saved [1788466/1788466]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZhc7b6VFWYX",
        "colab_type": "code",
        "outputId": "e3e21561-95bb-48a0-e2ee-d1c76a3c381e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOOHEYpiMzFp",
        "outputId": "8ecd59f2-7afa-4164-9701-b2afb0b8fbb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
        "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
        "\n",
        "def read_ontonotes(tsv_file): # \n",
        "    current_sent = [] # list of (word,label) lists\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line:\n",
        "                if current_sent:\n",
        "                    yield current_sent\n",
        "                    current_sent=[]\n",
        "                continue\n",
        "            current_sent.append(line) \n",
        "        else:\n",
        "            if current_sent:\n",
        "                yield current_sent\n",
        "\n",
        "full_train_data = list(read_ontonotes('train.tsv'))\n",
        "size_tr = int(len(full_train_data)/2)\n",
        "print(size_tr)\n",
        "##slice train\n",
        "train_data_sample = full_train_data[:size_tr]\n",
        "print(train_data_sample[:5])\n",
        "print()\n",
        "full_dev_data = list(read_ontonotes('dev.tsv'))\n",
        "size_dv = int(len(full_dev_data)/2)\n",
        "print(size_dv)\n",
        "#slice dev\n",
        "dev_data_sample = full_dev_data[:size_dv]\n",
        "print(dev_data_sample[:5])\n",
        "print()\n",
        "full_test_data = list(read_ontonotes('test.tsv'))\n",
        "size_ts = int(len(full_test_data)/2)\n",
        "print(size_ts)\n",
        "test_data_sample = full_test_data[:size_ts]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33409\n",
            "[[['Big\\tO'], ['Managers\\tO'], ['on\\tO'], ['Campus\\tO']], [['In\\tO'], ['recent\\tB-DATE'], ['years\\tI-DATE'], [',\\tO'], ['advanced\\tO'], ['education\\tO'], ['for\\tO'], ['professionals\\tO'], ['has\\tO'], ['become\\tO'], ['a\\tO'], ['hot\\tO'], ['topic\\tO'], ['in\\tO'], ['the\\tO'], ['business\\tO'], ['community\\tO'], ['.\\tO']], [['With\\tO'], ['this\\tO'], ['trend\\tO'], [',\\tO'], ['suddenly\\tO'], ['the\\tO'], ['mature\\tO'], ['faces\\tO'], ['of\\tO'], ['managers\\tO'], ['boasting\\tO'], ['an\\tO'], ['average\\tO'], ['of\\tO'], ['over\\tO'], ['ten\\tB-DATE'], ['years\\tI-DATE'], ['of\\tO'], ['professional\\tO'], ['experience\\tO'], ['have\\tO'], ['flooded\\tO'], ['in\\tO'], ['among\\tO'], ['the\\tO'], ['young\\tO'], ['people\\tO'], ['populating\\tO'], ['university\\tO'], ['campuses\\tO'], ['.\\tO']], [['In\\tO'], ['order\\tO'], ['to\\tO'], ['attract\\tO'], ['this\\tO'], ['group\\tO'], ['of\\tO'], ['seasoned\\tO'], ['adults\\tO'], ['pulling\\tO'], ['in\\tO'], ['over\\tO'], ['NT$\\tB-MONEY'], ['1\\tI-MONEY'], ['million\\tI-MONEY'], ['a\\tO'], ['year\\tO'], ['back\\tO'], ['to\\tO'], ['the\\tO'], ['ivory\\tO'], ['tower\\tO'], [',\\tO'], ['universities\\tO'], ['have\\tO'], ['begun\\tO'], ['to\\tO'], ['establish\\tO'], ['executive\\tO'], ['MBA\\tB-WORK_OF_ART'], ['(\\tO'], ['EMBA\\tB-WORK_OF_ART'], [')\\tO'], ['programs\\tO'], ['.\\tO']], [['In\\tO'], ['response\\tO'], [',\\tO'], ['each\\tO'], ['year\\tO'], ['over\\tO'], ['1000\\tB-CARDINAL'], ['mature\\tO'], ['professionals\\tO'], ['looking\\tO'], ['to\\tO'], ['recharge\\tO'], ['their\\tO'], ['minds\\tO'], ['and\\tO'], ['retool\\tO'], ['their\\tO'], ['know\\tO'], ['-\\tO'], ['how\\tO'], ['compete\\tO'], ['for\\tO'], ['a\\tO'], ['precious\\tO'], ['few\\tO'], ['openings\\tO'], ['in\\tO'], ['executive\\tO'], ['degree\\tO'], ['programs\\tO'], ['at\\tO'], ['top\\tO'], ['institutions\\tO'], ['such\\tO'], ['as\\tO'], ['National\\tB-ORG'], ['Taiwan\\tI-ORG'], ['University\\tI-ORG'], ['(\\tO'], ['NTU\\tB-ORG'], [')\\tO'], ['and\\tO'], ['National\\tB-ORG'], ['Chengchi\\tI-ORG'], ['University\\tI-ORG'], ['.\\tO']]]\n",
            "\n",
            "5806\n",
            "[[['President\\tB-WORK_OF_ART'], ['Chen\\tI-WORK_OF_ART'], ['Travels\\tI-WORK_OF_ART'], ['Abroad\\tI-WORK_OF_ART']], [['(\\tO'], ['Chang\\tB-PERSON'], ['Chiung\\tI-PERSON'], ['-\\tI-PERSON'], ['fang\\tI-PERSON'], ['/\\tO'], ['tr.\\tO'], ['by\\tO'], ['David\\tB-PERSON'], ['Mayer\\tI-PERSON'], [')\\tO']], [['President\\tO'], ['Chen\\tB-PERSON'], ['Shui\\tI-PERSON'], ['-\\tI-PERSON'], ['bian\\tI-PERSON'], ['visited\\tO'], ['the\\tB-FAC'], ['Nicaraguan\\tI-FAC'], ['National\\tI-FAC'], ['Assembly\\tI-FAC'], ['on\\tO'], ['August\\tB-DATE'], ['17\\tI-DATE'], [',\\tO'], ['where\\tO'], ['he\\tO'], ['received\\tO'], ['a\\tO'], ['medal\\tO'], ['from\\tO'], ['the\\tO'], ['president\\tO'], ['of\\tO'], ['the\\tO'], ['assembly\\tO'], [',\\tO'], ['Ivan\\tB-PERSON'], ['Escobar\\tI-PERSON'], ['Fornos\\tI-PERSON'], ['.\\tO']], [['(\\tO'], ['photo\\tO'], ['by\\tO'], ['Wu\\tB-PERSON'], ['Chi\\tI-PERSON'], ['-\\tI-PERSON'], ['chang\\tI-PERSON'], [',\\tO'], ['Central\\tB-ORG'], ['News\\tI-ORG'], ['Agency\\tI-ORG'], [')\\tO']], [['On\\tO'], ['August\\tB-DATE'], ['25\\tI-DATE'], ['President\\tO'], ['Chen\\tB-PERSON'], ['Shui\\tI-PERSON'], ['-\\tI-PERSON'], ['bian\\tI-PERSON'], ['wrapped\\tO'], ['up\\tO'], ['his\\tO'], ['first\\tB-ORDINAL'], ['overseas\\tO'], ['trip\\tO'], ['since\\tO'], ['taking\\tO'], ['office\\tO'], [',\\tO'], ['swinging\\tO'], ['through\\tO'], ['three\\tB-CARDINAL'], ['countries\\tO'], ['in\\tO'], ['Latin\\tB-LOC'], ['America\\tI-LOC'], ['and\\tO'], ['another\\tO'], ['three\\tB-CARDINAL'], ['in\\tO'], ['Africa\\tB-LOC'], ['.\\tO']]]\n",
            "\n",
            "4875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q32R9o_mJZAt",
        "colab_type": "code",
        "outputId": "90185b9d-d7c0-4726-ff64-4f48d88845c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "import re\n",
        "#regex for empty space chars, \\t \\n\n",
        "tab = re.compile('[\\t]')\n",
        "\n",
        "def clean(list):\n",
        "    clean_data =[]\n",
        "    for sent in list:\n",
        "        clean_list = []\n",
        "        for item in sent:\n",
        "            str = ''.join(item)\n",
        "            #match_nl = re.match(r\"\\n\", str)\n",
        "            #print(match_nl)\n",
        "            count_tab =  re.findall(r\"\\t\", str)\n",
        "            #print(count_tab)\n",
        "            if len(count_tab) == 1: \n",
        "                item = re.split(\"\\t\", str)\n",
        "                if item[0] != '.':\n",
        "                    clean_list.append(item)\n",
        "            elif len(count_tab) > 1:\n",
        "                item = re.split(\"\\n\", str)\n",
        "                #print(item)\n",
        "                for i in range(len(item)):\n",
        "                    #print(item[i])\n",
        "                    if i == 0 or i == len(item)-1:\n",
        "                        item[i] = '\"'+item[i]\n",
        "                        item[i] = re.split(\"\\t\", item[i])\n",
        "                        #print(item[i])\n",
        "                    else:\n",
        "                        item[i] = re.split(\"\\t\", item[i])\n",
        "                        #print(item[i])\n",
        "                    clean_list.append(item[i])\n",
        "        clean_data.append(clean_list)        \n",
        "    return clean_data\n",
        "\n",
        "train_data_clean = clean(train_data_sample)\n",
        "print(len(train_data_clean))\n",
        "for item in train_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')\n",
        "dev_data_clean = clean(dev_data_sample)\n",
        "print(len(dev_data_clean))\n",
        "for item in dev_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')\n",
        "test_data_clean = clean(test_data_sample)\n",
        "print(len(test_data_clean))\n",
        "for item in test_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')          "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33409\n",
            "[['Big', 'O'], ['Managers', 'O'], ['on', 'O'], ['Campus', 'O']]\n",
            "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O']]\n",
            "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O']]\n",
            "------------------------------------------\n",
            "5806\n",
            "[['President', 'B-WORK_OF_ART'], ['Chen', 'I-WORK_OF_ART'], ['Travels', 'I-WORK_OF_ART'], ['Abroad', 'I-WORK_OF_ART']]\n",
            "[['(', 'O'], ['Chang', 'B-PERSON'], ['Chiung', 'I-PERSON'], ['-', 'I-PERSON'], ['fang', 'I-PERSON'], ['/', 'O'], ['tr.', 'O'], ['by', 'O'], ['David', 'B-PERSON'], ['Mayer', 'I-PERSON'], [')', 'O']]\n",
            "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON']]\n",
            "------------------------------------------\n",
            "4875\n",
            "[['Powerful', 'B-WORK_OF_ART'], ['Tools', 'I-WORK_OF_ART'], ['for', 'I-WORK_OF_ART'], ['Biotechnology', 'I-WORK_OF_ART'], ['-', 'I-WORK_OF_ART'], ['Biochips', 'I-WORK_OF_ART']]\n",
            "[['(', 'O'], ['Chang', 'B-PERSON'], ['Chiung', 'I-PERSON'], ['-', 'I-PERSON'], ['fang', 'I-PERSON'], ['/', 'O'], ['photos', 'O'], ['by', 'O'], ['Hsueh', 'B-PERSON'], ['Chi', 'I-PERSON'], ['-', 'I-PERSON'], ['kuang', 'I-PERSON'], ['/', 'O'], ['tr.', 'O'], ['by', 'O'], ['Robert', 'B-PERSON'], ['Taylor', 'I-PERSON'], [')', 'O']]\n",
            "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O']]\n",
            "------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZHhXzVTQA_P",
        "outputId": "61915d82-b534-4ab3-9ebb-9b06f9a47551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for item in f: # list of lists (tokens)\n",
        "        #print(item)\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for token in item:\n",
        "            if len(token) ==2:\n",
        "                sent_text.append(token[0])\n",
        "                sent_tags.append(token[1])\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict['text'])\n",
        "        #print(sent_dict['tags'])\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))\n",
        "samp = train_data_sent[:3]\n",
        "print(samp)\n",
        "print()\n",
        "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
        "samp2 = dev_data_sent[:3]\n",
        "print(samp2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': ['Big', 'Managers', 'on', 'Campus'], 'tags': ['O', 'O', 'O', 'O']}, {'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "\n",
            "[{'text': ['President', 'Chen', 'Travels', 'Abroad'], 'tags': ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']}, {'text': ['(', 'Chang', 'Chiung', '-', 'fang', '/', 'tr.', 'by', 'David', 'Mayer', ')'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O']}, {'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpYBQMbcQGBi",
        "outputId": "b0f28077-f547-4786-e8ed-a1e3a79b67d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "random.seed(123)\n",
        "random.shuffle(train_data_sent)\n",
        "print(type(train_data_sent))\n",
        "print(type(train_data_sent[0]))\n",
        "\n",
        "train_texts=[i[\"text\"] for i in train_data_sent]\n",
        "train_labels=[i[\"tags\"] for i in train_data_sent]\n",
        "\n",
        "#print(type(train_texts))\n",
        "#print(type(train_texts[0]))\n",
        "\n",
        "print('Text: ', train_texts[:4])\n",
        "print('Labels: ', train_labels[:4])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "Text:  [['Sharon', 'Repudiates', 'the', 'Road', 'Map'], ['AppleScript', 'is', 'just', 'one', 'of', 'several', 'client', 'languages', 'for', 'these', 'services', '(', 'it', 'was', \"n't\", 'even', 'the', 'first', ',', 'btw', ';', 'UserTalk', 'predates', 'it', ')'], ['Your', 'qualifications', 'quote', 'end', 'quote', 'meaningless', '/.'], ['[dasanicool]', 'Young', 'people', 'like', 'to', 'race', 'high', '-', 'power', 'motorcycles', 'late', 'at', 'night', 'and', 'revel', 'in', 'bars', 'well', 'after', 'dark']]\n",
            "Labels:  [['O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQQRw0YO-Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## same for validation/dev data\n",
        "dev_texts=[i[\"text\"] for i in dev_data_sent]\n",
        "dev_labels=[i[\"tags\"] for i in dev_data_sent]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNKOr6YY28eC",
        "colab_type": "text"
      },
      "source": [
        "Let's add POS tags to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlIltk1uXIXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT RUN\n",
        "#import nltk\n",
        "#nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "#def sent_parser4pos(texts, labels):\n",
        "  #  for i in range(len(texts) -1):\n",
        "#        #print(texts[i])\n",
        "#        tokens_with_pos = nltk.pos_tag(texts[i])\n",
        "#        #getting the corresponding labels list\n",
        " #       labels_curr= labels[i]\n",
        "  #      print()\n",
        " #       print()\n",
        "        #print('current labels:',labels_curr)\n",
        "       #updating labels with pos label\n",
        "        #print('possed tokens:', tokens_with_pos)\n",
        "  #      for j in range(len(labels_curr) -1):\n",
        "   #         bio = labels_curr[j]\n",
        "  #          pos = tokens_with_pos[j][1]\n",
        "            #print('bio: ',bio)\n",
        "            #print('pos: ', pos)\n",
        "            #print('current labels: ',labels[i])\n",
        "   #         new_label = pos+' '+bio ## new kind of combination label\n",
        "   #         labels[i][j] = new_label\n",
        "        #print('No of Bio Labels: ',len(labels_curr), ' No of pos tags: ', len(tokens_with_pos), ' No of updated labels: ', len(labels[i]), labels[i])\n",
        "  #  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCqK8iji9Nur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DO NOT RUN\n",
        "#train_labels_posbio = sent_parser4pos(train_texts, train_labels)\n",
        "#print(train_texts[0])\n",
        "#print(train_labels[0]\n",
        "#print(train_labels_posbio[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICn08fOgbyXl",
        "colab_type": "code",
        "outputId": "4019782c-2cd6-488a-8a30-85dfc37189d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-11 17:09:49--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  21.9MB/s    in 30s     \n",
            "\n",
            "2020-05-11 17:10:20 (21.5 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFv2qclTbyXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0e085d9f-271b-458c-8893-140a1adbd227"
      },
      "source": [
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_C5ii8byXt",
        "colab_type": "code",
        "outputId": "a840f04e-b6dc-4977-f7e8-c00392cf619d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdgjgOlbyXx",
        "colab_type": "code",
        "outputId": "3740c1ec-523f-4ac2-b12e-d304f8833959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGaojUBhbyX2",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Ox5_ObyX3",
        "colab_type": "code",
        "outputId": "36177f59-cde2-4e5d-e466-e32b9af57bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#Labels\n",
        "from pprint import pprint\n",
        "\n",
        "not_letter = re.compile(r'[^a-zA-Z]')\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in train_labels: #loops over sentences \n",
        "    #print(sentence_labels)\n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "       # match = not_letter.match(label)\n",
        "        #if match or label== 'O':\n",
        "        #    break\n",
        "        #else:    \n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 12,\n",
            " 'B-DATE': 1,\n",
            " 'B-EVENT': 21,\n",
            " 'B-FAC': 25,\n",
            " 'B-GPE': 29,\n",
            " 'B-LANGUAGE': 27,\n",
            " 'B-LAW': 23,\n",
            " 'B-LOC': 17,\n",
            " 'B-MONEY': 10,\n",
            " 'B-NORP': 15,\n",
            " 'B-ORDINAL': 18,\n",
            " 'B-ORG': 8,\n",
            " 'B-PERCENT': 33,\n",
            " 'B-PERSON': 19,\n",
            " 'B-PRODUCT': 7,\n",
            " 'B-QUANTITY': 30,\n",
            " 'B-TIME': 20,\n",
            " 'B-WORK_OF_ART': 34,\n",
            " 'I-CARDINAL': 28,\n",
            " 'I-DATE': 31,\n",
            " 'I-EVENT': 14,\n",
            " 'I-FAC': 13,\n",
            " 'I-GPE': 26,\n",
            " 'I-LANGUAGE': 5,\n",
            " 'I-LAW': 9,\n",
            " 'I-LOC': 22,\n",
            " 'I-MONEY': 11,\n",
            " 'I-NORP': 0,\n",
            " 'I-ORDINAL': 4,\n",
            " 'I-ORG': 16,\n",
            " 'I-PERCENT': 35,\n",
            " 'I-PERSON': 3,\n",
            " 'I-PRODUCT': 24,\n",
            " 'I-QUANTITY': 2,\n",
            " 'I-TIME': 36,\n",
            " 'I-WORK_OF_ART': 32,\n",
            " 'O': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k8DshceEaI",
        "colab_type": "code",
        "outputId": "cf16ed82-8615-48c9-8119-15dbaa1f66cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in train_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            if token in label_map:\n",
        "                vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
        "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "pprint(vectorized_labels[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 6, 6, 6, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtqLdCMPf3X",
        "colab_type": "code",
        "outputId": "38be2fd5-5f93-4737-f7f8-ecbe132b919d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## vectorization of the texts\n",
        "def text_vectorizer(vocab, train_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(train_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
        "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "\n",
        "pprint(train_texts[0])\n",
        "pprint(vectorized_data[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sharon', 'Repudiates', 'the', 'Road', 'Map']\n",
            "[8346, 1, 3, 1685, 8936]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6FH5F1QGrq",
        "colab_type": "code",
        "outputId": "e2a94f3a-a8fc-427c-95b3-8df9072a5348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max(lengths))\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max(lengths)), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])\n",
        "\n",
        "# Same stuff for the validation data\n",
        "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max(lengths))\n",
        "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max(lengths)), -1)\n",
        "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
        "validation_weights[validation_weights > 0] = 1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Old shape: (30000,)\n",
            "New shape: (30000, 17040)\n",
            "First example:\n",
            "[   0    0    0 ...    3 1685 8936]\n",
            "Padded labels shape: (30000, 17040, 1)\n",
            "{'B-CARDINAL': 12,\n",
            " 'B-DATE': 1,\n",
            " 'B-EVENT': 21,\n",
            " 'B-FAC': 25,\n",
            " 'B-GPE': 29,\n",
            " 'B-LANGUAGE': 27,\n",
            " 'B-LAW': 23,\n",
            " 'B-LOC': 17,\n",
            " 'B-MONEY': 10,\n",
            " 'B-NORP': 15,\n",
            " 'B-ORDINAL': 18,\n",
            " 'B-ORG': 8,\n",
            " 'B-PERCENT': 33,\n",
            " 'B-PERSON': 19,\n",
            " 'B-PRODUCT': 7,\n",
            " 'B-QUANTITY': 30,\n",
            " 'B-TIME': 20,\n",
            " 'B-WORK_OF_ART': 34,\n",
            " 'I-CARDINAL': 28,\n",
            " 'I-DATE': 31,\n",
            " 'I-EVENT': 14,\n",
            " 'I-FAC': 13,\n",
            " 'I-GPE': 26,\n",
            " 'I-LANGUAGE': 5,\n",
            " 'I-LAW': 9,\n",
            " 'I-LOC': 22,\n",
            " 'I-MONEY': 11,\n",
            " 'I-NORP': 0,\n",
            " 'I-ORDINAL': 4,\n",
            " 'I-ORG': 16,\n",
            " 'I-PERCENT': 35,\n",
            " 'I-PERSON': 3,\n",
            " 'I-PRODUCT': 24,\n",
            " 'I-QUANTITY': 2,\n",
            " 'I-TIME': 36,\n",
            " 'I-WORK_OF_ART': 32,\n",
            " 'O': 6}\n",
            "First example labels:\n",
            "array([[0],\n",
            "       [0],\n",
            "       [0],\n",
            "       ...,\n",
            "       [6],\n",
            "       [6],\n",
            "       [6]], dtype=int32)\n",
            "First weight vector:\n",
            "[0 0 0 ... 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAOVAbBTRAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function\n",
        "import keras\n",
        "\n",
        "def _convert_to_entities(input_sequence):\n",
        "    \"\"\"\n",
        "    Reads a sequence of tags and converts them into a set of entities.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    previous_tag = label_map['O']\n",
        "    for i, tag in enumerate(input_sequence):\n",
        "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "            current_entity.append((tag, i))\n",
        "        elif tag == label_map['O']: # Entity has ended\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "        elif tag == previous_tag: # Current entity continues\n",
        "            current_entity.append((tag, i))\n",
        "        previous_tag = tag\n",
        "    \n",
        "    # Add the last entity to our entity list if the sentences ends with an entity\n",
        "    if len(current_entity) > 0:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    entity_offsets = set()\n",
        "    \n",
        "    for e in entities:\n",
        "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
        "    return entity_offsets\n",
        "\n",
        "def _entity_level_PRF(predictions, gold, lengths):\n",
        "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
        "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
        "    \n",
        "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
        "    pred_count = sum([len(e) for e in pred_entities])\n",
        "    \n",
        "    try:\n",
        "        precision = tp / pred_count # tp / (tp+np)\n",
        "        recall = tp / sum([len(e) for e in gold_entities])\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    except Exception as e:\n",
        "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
        "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
        "    return precision, recall, fscore             \n",
        "\n",
        "def evaluate(predictions, gold, lengths):\n",
        "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
        "    return precision, recall, fscore\n",
        "\n",
        "class EvaluateEntities(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.precision = []\n",
        "        self.recall = []\n",
        "        self.fscore = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
        "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
        "        self.precision.append(evaluation_parameters[0])\n",
        "        self.recall.append(evaluation_parameters[1])\n",
        "        self.fscore.append(evaluation_parameters[2])\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ydCexfTg5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 1\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 50\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oUO3GLfTrl3",
        "colab_type": "code",
        "outputId": "beddefb3-b8f8-4226-bed3-b9d2c3561318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 17040)             0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 17040, 300)        15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 17040, 50)         15050     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 17040, 37)         1887      \n",
            "=================================================================\n",
            "Total params: 15,017,537\n",
            "Trainable params: 16,937\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIPQrLXUVrWr",
        "colab_type": "code",
        "outputId": "3865fb67-c82e-416f-83c9-9219bd39b628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# train the model 1\n",
        "optimizer=Adam(lr=0.05) # define the learning rate\n",
        "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 48s - loss: 5.2882e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.44780793319415446 / 0.061655648174762864 / 0.10838807478524508\n",
            "Epoch 2/10\n",
            " - 47s - loss: 3.4230e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.452947259565667 / 0.06294912331129635 / 0.11053627760252366\n",
            "Epoch 3/10\n",
            " - 47s - loss: 3.2457e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.4349056603773585 / 0.06625467088243749 / 0.11499126964330257\n",
            "Epoch 4/10\n",
            " - 47s - loss: 3.1840e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.4005708848715509 / 0.06050589249784421 / 0.10513172680734174\n",
            "Epoch 5/10\n",
            " - 47s - loss: 3.1577e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.4223175965665236 / 0.07070997413049727 / 0.12113751077188231\n",
            "Epoch 6/10\n",
            " - 47s - loss: 3.0668e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.420584498094028 / 0.04757114113250934 / 0.08547449967721112\n",
            "Epoch 7/10\n",
            " - 47s - loss: 3.0876e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.43890020366598775 / 0.06194308709399253 / 0.10856423173803527\n",
            "Epoch 8/10\n",
            " - 47s - loss: 3.0973e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.4293193717277487 / 0.058924978442081055 / 0.10362694300518134\n",
            "Epoch 9/10\n",
            " - 47s - loss: 3.0438e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.44471744471744473 / 0.05202644438056913 / 0.09315491507977354\n",
            "Epoch 10/10\n",
            " - 47s - loss: 3.0584e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.4116706634692246 / 0.0740155217016384 / 0.1254720428797661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bah0URSVV5GP",
        "colab_type": "code",
        "outputId": "9e291831-c2eb-40ac-cbed-260c0c8898e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    print(\"History:\", fscores)\n",
        "    print(\"Highest f-score:\", max(fscores))\n",
        "    plt.plot(fscores)\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "History: [0.10838807478524508, 0.11053627760252366, 0.11499126964330257, 0.10513172680734174, 0.12113751077188231, 0.08547449967721112, 0.10856423173803527, 0.10362694300518134, 0.09315491507977354, 0.1254720428797661]\n",
            "Highest f-score: 0.1254720428797661\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKpy2xq0NeSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 2\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 50\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model2 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTYjLB28KfUR",
        "colab_type": "code",
        "outputId": "b151caa5-f9c9-41e8-9813-ce89d217c7c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# train the model 2\n",
        "optimizer=Adamax(lr=0.01) # define the learning rate\n",
        "model2.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model2.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 47s - loss: 7.8312e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Epoch 2/10\n",
            " - 47s - loss: 5.2328e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Epoch 3/10\n",
            " - 47s - loss: 4.7204e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.45390070921985815 / 0.009198045415349238 / 0.01803070855050007\n",
            "Epoch 4/10\n",
            " - 47s - loss: 4.3499e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.32142857142857145 / 0.016815176774935325 / 0.03195848128926523\n",
            "Epoch 5/10\n",
            " - 47s - loss: 4.0892e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.324853228962818 / 0.023857430296062087 / 0.04445039496585889\n",
            "Epoch 6/10\n",
            " - 47s - loss: 3.8950e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.36942675159235666 / 0.03334291463064099 / 0.06116530450830478\n",
            "Epoch 7/10\n",
            " - 47s - loss: 3.7667e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.34375 / 0.03794193733831561 / 0.06834066787470876\n",
            "Epoch 8/10\n",
            " - 47s - loss: 3.6598e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.3524804177545692 / 0.038804254096004596 / 0.0699119627136199\n",
            "Epoch 9/10\n",
            " - 47s - loss: 3.5805e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.3679890560875513 / 0.03866053463638977 / 0.06997008713746912\n",
            "Epoch 10/10\n",
            " - 47s - loss: 3.5149e-04\n",
            "\n",
            "Precision/Recall/F-score: 0.3832077502691066 / 0.05116412762288014 / 0.09027513630024091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIm0vXkjKqYf",
        "colab_type": "code",
        "outputId": "0a4de211-6762-4cba-c9ea-593ef48f1091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    print(\"History:\", fscores)\n",
        "    print(\"Highest f-score:\", max(fscores))\n",
        "    plt.plot(fscores)\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "History: [0.0, 0.0, 0.01803070855050007, 0.03195848128926523, 0.04445039496585889, 0.06116530450830478, 0.06834066787470876, 0.0699119627136199, 0.06997008713746912, 0.09027513630024091]\n",
            "Highest f-score: 0.09027513630024091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVS-GKkQUpue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 3\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 50\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model3 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuWSrGPVUw9Z",
        "colab_type": "code",
        "outputId": "7614f4ae-9d42-446a-b74b-06f651ff1cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# train the model 3\n",
        "optimizer=Adadelta(lr=0.01) # define the learning rate\n",
        "model3.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model3.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 47s - loss: 0.0049\n",
            "\n",
            "Precision/Recall/F-score: 0.6892132431470274 / 0.8347226214429434 / 0.7550211244718881\n",
            "Epoch 2/10\n",
            " - 47s - loss: 0.0049\n",
            "\n",
            "Precision/Recall/F-score: 0.6832137395600517 / 0.8347226214429434 / 0.751406947409276\n",
            "Epoch 3/10\n",
            " - 47s - loss: 0.0048\n",
            "\n",
            "Precision/Recall/F-score: 0.6795366795366795 / 0.8347226214429434 / 0.7491776846178652\n",
            "Epoch 4/10\n",
            " - 47s - loss: 0.0048\n",
            "\n",
            "Precision/Recall/F-score: 0.6730791516977633 / 0.8347226214429434 / 0.7452364149611855\n",
            "Epoch 5/10\n",
            " - 47s - loss: 0.0047\n",
            "\n",
            "Precision/Recall/F-score: 0.6615787675133842 / 0.8347226214429434 / 0.7381330622100781\n",
            "Epoch 6/10\n",
            " - 47s - loss: 0.0047\n",
            "\n",
            "Precision/Recall/F-score: 0.6523643715601483 / 0.8347226214429434 / 0.73236239833554\n",
            "Epoch 7/10\n",
            " - 47s - loss: 0.0046\n",
            "\n",
            "Precision/Recall/F-score: 0.6464106844741235 / 0.8347226214429434 / 0.7285956219030295\n",
            "Epoch 8/10\n",
            " - 47s - loss: 0.0046\n",
            "\n",
            "Precision/Recall/F-score: 0.6283674131775397 / 0.8347226214429434 / 0.7169927782235664\n",
            "Epoch 9/10\n",
            " - 47s - loss: 0.0046\n",
            "\n",
            "Precision/Recall/F-score: 0.6213758425163154 / 0.8347226214429434 / 0.7124195032198711\n",
            "Epoch 10/10\n",
            " - 47s - loss: 0.0045\n",
            "\n",
            "Precision/Recall/F-score: 0.6111754182889614 / 0.8347226214429434 / 0.7056679424093312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwMwwo9-MPv4"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9pXRewbyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#expanding to RNN model with context\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "rnn_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
        "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
        "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP-kwoNXUMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "bbd3adad-3b95-4dd1-ee50-7ab2e3d4d3a3"
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 17040)             0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 17040, 300)        15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 17040, 50)         15050     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 17040, 37)         1887      \n",
            "=================================================================\n",
            "Total params: 15,017,537\n",
            "Trainable params: 16,937\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIal9meVXnN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72a23a6d-66dd-47d6-d436-8da706ddbd3f"
      },
      "source": [
        "\n",
        "optimizer=Adam(lr=0.01) # define the learning rate\n",
        "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKNs4t8X3Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCo0xF5kMMbH"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgSYNcerMI9R"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRDxKgLSL_uf"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlG6ZWkIL-HY"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}