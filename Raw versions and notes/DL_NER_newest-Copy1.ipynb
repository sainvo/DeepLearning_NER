{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTBsYI1tLeVk"
   },
   "source": [
    "# Deep Learning NER task\n",
    "\n",
    "Tatjana Cucic and Sanna Volanen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9T2GevEzfPP2"
   },
   "source": [
    "https://spacy.io/api/annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5MwAmUALZ4V"
   },
   "source": [
    "# Milestones\n",
    "\n",
    "## 1.1 Predicting word labels independently\n",
    "\n",
    "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
    "* Evaluate the results on token level and entity level. \n",
    "* Report your results with different network hyperparameters. \n",
    "* Also discuss whether the token level accuracy is a reasonable metric.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "0Q3HiGQgMU5L",
    "outputId": "6aa4cfdc-f214-455a-d41a-0dde30e4ed71"
   },
   "outputs": [],
   "source": [
    "# Training data: Used for training the model\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
    "\n",
    "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
    "\n",
    "# Test data: Never touched during training / model development, used for evaluating the final model\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
    "\n",
    "#saved model\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FZhc7b6VFWYX",
    "outputId": "862727bf-32a4-432a-a348-69baba3fcef2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "import csv\n",
    "\n",
    "#csv.field_size_limit(sys.maxsize)\n",
    "csv.field_size_limit(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Tanja\\\\Desktop\\\\NER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOOHEYpiMzFp"
   },
   "outputs": [],
   "source": [
    "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
    "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
    "\n",
    "def read_ontonotes(tsv_file): # \n",
    "    current_sent = [] # list of (word,label) lists\n",
    "    #with open(tsv_file) as f:\n",
    "    with open(tsv_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
    "        for line in tsvreader:\n",
    "            #print(line)\n",
    "            if not line:\n",
    "                if current_sent:\n",
    "                    yield current_sent\n",
    "                    current_sent=[]\n",
    "                continue\n",
    "            current_sent.append(line[0]) \n",
    "        else:\n",
    "            if current_sent:\n",
    "                yield current_sent\n",
    "\n",
    "train_data_full = list(read_ontonotes('train.tsv'))\n",
    "dev_data_full = list(read_ontonotes('dev.tsv'))\n",
    "test_data_full = list(read_ontonotes('test.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And\\tO',\n",
       " 'what\\tO',\n",
       " 'effect\\tO',\n",
       " 'does\\tO',\n",
       " 'their\\tO',\n",
       " 'return\\tO',\n",
       " 'have\\tO',\n",
       " 'on\\tO',\n",
       " 'campus\\tO',\n",
       " '?\\tO']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_full[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "q32R9o_mJZAt",
    "outputId": "456f0e4d-bd31-4f39-e61d-63c2cbf65d29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50252\n",
      "[['In', 'O'], ['recent', 'B-DATE'], ['years', 'I-DATE'], [',', 'O'], ['advanced', 'O'], ['education', 'O'], ['for', 'O'], ['professionals', 'O'], ['has', 'O'], ['become', 'O'], ['a', 'O'], ['hot', 'O'], ['topic', 'O'], ['in', 'O'], ['the', 'O'], ['business', 'O'], ['community', 'O'], ['.', 'O']]\n",
      "[['With', 'O'], ['this', 'O'], ['trend', 'O'], [',', 'O'], ['suddenly', 'O'], ['the', 'O'], ['mature', 'O'], ['faces', 'O'], ['of', 'O'], ['managers', 'O'], ['boasting', 'O'], ['an', 'O'], ['average', 'O'], ['of', 'O'], ['over', 'O'], ['ten', 'B-DATE'], ['years', 'I-DATE'], ['of', 'O'], ['professional', 'O'], ['experience', 'O'], ['have', 'O'], ['flooded', 'O'], ['in', 'O'], ['among', 'O'], ['the', 'O'], ['young', 'O'], ['people', 'O'], ['populating', 'O'], ['university', 'O'], ['campuses', 'O'], ['.', 'O']]\n",
      "[['In', 'O'], ['order', 'O'], ['to', 'O'], ['attract', 'O'], ['this', 'O'], ['group', 'O'], ['of', 'O'], ['seasoned', 'O'], ['adults', 'O'], ['pulling', 'O'], ['in', 'O'], ['over', 'O'], ['NT$', 'B-MONEY'], ['1', 'I-MONEY'], ['million', 'I-MONEY'], ['a', 'O'], ['year', 'O'], ['back', 'O'], ['to', 'O'], ['the', 'O'], ['ivory', 'O'], ['tower', 'O'], [',', 'O'], ['universities', 'O'], ['have', 'O'], ['begun', 'O'], ['to', 'O'], ['establish', 'O'], ['executive', 'O'], ['MBA', 'B-WORK_OF_ART'], ['(', 'O'], ['EMBA', 'B-WORK_OF_ART'], [')', 'O'], ['programs', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "#regex for empty space chars, \\t \\n\n",
    "tab = re.compile('[\\t]')\n",
    "line = re.compile('[\\n]')\n",
    "punct = re.compile('[.?!:;]')\n",
    "\n",
    "def splitter(sent):\n",
    "    #print('----------------------------------------')\n",
    "    #print(\"one sentence in raw data:\", sent)\n",
    "    split_list = []\n",
    "    # loop over tokens items inside sentence, supposedly item= token+ \\t +tag\n",
    "    for item in sent: \n",
    "        #print(\"Item in sentence: \", item)\n",
    "        if item != None:\n",
    "            match1 = item.count('\\n')\n",
    "            #print(match1)\n",
    "            match2 = item.count('\\t')\n",
    "            #print(match2)\n",
    "            if match1 ==0: # no new lines nested\n",
    "                if match2 == 1: #just one tab inside token\n",
    "                    item_pair = item.split('\\t')\n",
    "                if item_pair[0] =='': # replacing empty string with missing quote marks\n",
    "                    item_pair[0] = '\\\"'\n",
    "                split_list.append(item_pair) \n",
    "            else:\n",
    "                subitems_list = item.split('\\n') ## check if token has \\n -> bundled, quotes\n",
    "                if len(subitems_list) > 1:  ## item string has more than one sentence nested in it\n",
    "                    #print(\"Found nested sentences: \", subitems_list)\n",
    "                    #print(\"subseq start\")\n",
    "                    for j in range(len(subitems_list)):  \n",
    "                        token = subitems_list[j]  \n",
    "                        #print(token)\n",
    "                        subtoken_listed_again = token.split('\\n') \n",
    "                        for token in subtoken_listed_again:\n",
    "                            match1=token.count('\\n')\n",
    "                            match2=token.count('\\t')\n",
    "                            if  match1 == 0: # no new lines nested\n",
    "                               if  match2 == 1: #just one tab inside token\n",
    "                                    token = token.split('\\t')\n",
    "                            if token =='': # replacing empty string with missing quote marks\n",
    "                                token = '\\\"'\n",
    "                            if token == '.':\n",
    "                                split_list.append(token)\n",
    "                                continue\n",
    "                                split_list=[]\n",
    "                            else:\n",
    "                                split_list.append(token)\n",
    "                    #print(\"subseq end\")\n",
    "    for item in split_list:\n",
    "        #print(\"Item in split list: \",item)\n",
    "        if type(item) != list:\n",
    "            split_list.remove(item)\n",
    "        if item[0] =='': # replacing empty string with missing quote marks\n",
    "            item[0] = '\\\"'\n",
    "    #print(\"Resplitted sentence :\", split_list)\n",
    "    return split_list\n",
    "\n",
    "def clean(raw_data): ## input list is list of lists of strings \n",
    "    clean_data =[]  #list of lists that have one clean sentence per list\n",
    "    for sent in raw_data: # split by [] lines, supposedly a sentence line\n",
    "        one_sentence = [] #collects the new sentence if there has been need to resplit items\n",
    "        splitted= splitter(sent)\n",
    "        for item in splitted:\n",
    "            #print(item)\n",
    "            matchi = re.match(punct, item[0])\n",
    "            if matchi:\n",
    "                #print(\"collected sentence\")\n",
    "                one_sentence.append(item)\n",
    "                clean_data.append(one_sentence)\n",
    "                one_sentence=[]\n",
    "                break\n",
    "            else:\n",
    "                one_sentence.append(item)\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "train_data_clean = clean(train_data_full)\n",
    "print(len(train_data_clean))\n",
    "for item in train_data_clean[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "jQrs8MSroPfa",
    "outputId": "6fa023af-ea39-4540-ac3e-083a85e6f1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence: 168 index:  8041\n",
      "[168, 145, 123, 121, 120, 108, 106, 106, 102, 101, 97, 96, 94, 94, 93, 93, 93, 92, 91, 91, 91, 90, 90, 90, 90, 90, 90, 89, 88, 88, 88, 87, 87, 86, 85, 85, 85, 85, 84, 84, 84, 84, 83, 83, 83, 83, 82, 82, 82, 82, 81, 81, 80, 80, 80, 79, 79, 79, 79, 79, 79, 78, 78, 78, 78, 78, 77, 77, 77, 77, 77, 77, 77, 77, 76, 76, 76, 76, 76, 76, 76, 76, 76, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n"
     ]
    }
   ],
   "source": [
    "# final check on the sentences\n",
    "item_lengths = []\n",
    "max_text = 0\n",
    "for item in train_data_clean:\n",
    "    item_lengths.append(len(item))\n",
    "    if len(item) > max_text:\n",
    "        max_text = len(item)\n",
    "        ind = train_data_clean.index(item)\n",
    "print(\"Longest sentence:\", max_text, \"index: \",ind)\n",
    "\n",
    "lengths_sorted = sorted(item_lengths, reverse=True)\n",
    "max = item_lengths.index(max_text)\n",
    "#print(items_sorted[0])\n",
    "#pprint(train_data_clean[max])\n",
    "print(lengths_sorted[:300]) # longest sentences\n",
    "# checking long items\n",
    "#for item in train_data_clean:\n",
    "    #if len(item) == 123:\n",
    "        #pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', 'O'],\n",
       " ['rational', 'O'],\n",
       " ['approach', 'O'],\n",
       " ['to', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['would', 'O'],\n",
       " ['entail', 'O'],\n",
       " ['(', 'O'],\n",
       " ['1', 'O'],\n",
       " [')', 'O'],\n",
       " ['creating', 'O'],\n",
       " ['communities', 'O'],\n",
       " ['with', 'O'],\n",
       " ['a', 'O'],\n",
       " ['mix', 'O'],\n",
       " ['of', 'O'],\n",
       " ['housing', 'O'],\n",
       " ['/', 'O'],\n",
       " ['office', 'O'],\n",
       " ['/', 'O'],\n",
       " ['retail', 'O'],\n",
       " ['/', 'O'],\n",
       " ['amenities', 'O'],\n",
       " ['to', 'O'],\n",
       " ['reduce', 'O'],\n",
       " ['the', 'O'],\n",
       " ['number', 'O'],\n",
       " ['and', 'O'],\n",
       " ['length', 'O'],\n",
       " ['of', 'O'],\n",
       " ['automobile', 'O'],\n",
       " ['trips', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['2', 'O'],\n",
       " [')', 'O'],\n",
       " ['reforming', 'O'],\n",
       " ['counterproductive', 'O'],\n",
       " ['zoning', 'O'],\n",
       " ['codes', 'O'],\n",
       " [',', 'O'],\n",
       " ['subdivision', 'O'],\n",
       " ['ordinances', 'O'],\n",
       " ['and', 'O'],\n",
       " ['comprehensive', 'O'],\n",
       " ['plans', 'O'],\n",
       " ['in', 'O'],\n",
       " ['order', 'O'],\n",
       " ['to', 'O'],\n",
       " ['create', 'O'],\n",
       " ['more', 'O'],\n",
       " ['transit', 'O'],\n",
       " ['-', 'O'],\n",
       " ['friendly', 'O'],\n",
       " ['and', 'O'],\n",
       " ['pedestrian', 'O'],\n",
       " ['-', 'O'],\n",
       " ['friendly', 'O'],\n",
       " ['communities', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['3', 'O'],\n",
       " [')', 'O'],\n",
       " ['creating', 'O'],\n",
       " ['funding', 'O'],\n",
       " ['sources', 'O'],\n",
       " ['for', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['that', 'O'],\n",
       " ['establishes', 'O'],\n",
       " ['a', 'O'],\n",
       " ['rational', 'O'],\n",
       " ['nexus', 'O'],\n",
       " ['between', 'O'],\n",
       " ['those', 'O'],\n",
       " ['who', 'O'],\n",
       " ['use', 'O'],\n",
       " ['/', 'O'],\n",
       " ['benefit', 'O'],\n",
       " ['from', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['facilities', 'O'],\n",
       " ['and', 'O'],\n",
       " ['those', 'O'],\n",
       " ['who', 'O'],\n",
       " ['pay', 'O'],\n",
       " ['for', 'O'],\n",
       " ['them', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['4', 'O'],\n",
       " [')', 'O'],\n",
       " ['promoting', 'O'],\n",
       " ['telework', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['5', 'O'],\n",
       " [')', 'O'],\n",
       " ['liberating', 'O'],\n",
       " ['mass', 'O'],\n",
       " ['transit', 'O'],\n",
       " ['from', 'O'],\n",
       " ['government', 'O'],\n",
       " ['monopolies', 'O'],\n",
       " ['and', 'O'],\n",
       " ['encouraging', 'O'],\n",
       " ['private', 'O'],\n",
       " ['sector', 'O'],\n",
       " ['innovation', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['6', 'O'],\n",
       " [')', 'O'],\n",
       " ['using', 'O'],\n",
       " ['a', 'O'],\n",
       " ['wide', 'O'],\n",
       " ['range', 'O'],\n",
       " ['of', 'O'],\n",
       " ['tools', 'O'],\n",
       " [',', 'O'],\n",
       " ['from', 'O'],\n",
       " ['corridor', 'O'],\n",
       " ['management', 'O'],\n",
       " ['to', 'O'],\n",
       " ['intelligent', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['systems', 'O'],\n",
       " [',', 'O'],\n",
       " ['to', 'O'],\n",
       " ['increase', 'O'],\n",
       " ['the', 'O'],\n",
       " ['capacity', 'O'],\n",
       " ['of', 'O'],\n",
       " ['existing', 'O'],\n",
       " ['thoroughfares', 'O'],\n",
       " [',', 'O'],\n",
       " ['(', 'O'],\n",
       " ['7', 'O'],\n",
       " [')', 'O'],\n",
       " ['stretching', 'O'],\n",
       " ['the', 'O'],\n",
       " ['value', 'O'],\n",
       " ['of', 'O'],\n",
       " ['road', 'O'],\n",
       " ['construction', 'O'],\n",
       " ['/', 'O'],\n",
       " ['maintenance', 'O'],\n",
       " ['dollars', 'O'],\n",
       " ['through', 'O'],\n",
       " ['outsourcing', 'O'],\n",
       " [',', 'O'],\n",
       " ['and', 'O'],\n",
       " ['(', 'O'],\n",
       " ['8', 'O'],\n",
       " [')', 'O'],\n",
       " ['prioritizing', 'O'],\n",
       " ['transportation', 'O'],\n",
       " ['projects', 'O'],\n",
       " ['on', 'O'],\n",
       " ['a', 'O'],\n",
       " ['congestion', 'O'],\n",
       " ['-', 'O'],\n",
       " ['mitigation', 'O'],\n",
       " ['Return', 'O'],\n",
       " ['on', 'O'],\n",
       " ['Investment', 'O'],\n",
       " ['basis', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np   #want to see the longest sentence\n",
    "\n",
    "lengths = np.array(item_lengths)\n",
    "train_data_clean[np.argmax(lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In', 'O'],\n",
       " ['recent', 'B-DATE'],\n",
       " ['years', 'I-DATE'],\n",
       " [',', 'O'],\n",
       " ['advanced', 'O'],\n",
       " ['education', 'O'],\n",
       " ['for', 'O'],\n",
       " ['professionals', 'O'],\n",
       " ['has', 'O'],\n",
       " ['become', 'O'],\n",
       " ['a', 'O'],\n",
       " ['hot', 'O'],\n",
       " ['topic', 'O'],\n",
       " ['in', 'O'],\n",
       " ['the', 'O'],\n",
       " ['business', 'O'],\n",
       " ['community', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "tk8Brh3sd6dl",
    "outputId": "21341a1a-f969-4fca-dda8-046c2d75306c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "9954\n",
      "[['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['visited', 'O'], ['the', 'B-FAC'], ['Nicaraguan', 'I-FAC'], ['National', 'I-FAC'], ['Assembly', 'I-FAC'], ['on', 'O'], ['August', 'B-DATE'], ['17', 'I-DATE'], [',', 'O'], ['where', 'O'], ['he', 'O'], ['received', 'O'], ['a', 'O'], ['medal', 'O'], ['from', 'O'], ['the', 'O'], ['president', 'O'], ['of', 'O'], ['the', 'O'], ['assembly', 'O'], [',', 'O'], ['Ivan', 'B-PERSON'], ['Escobar', 'I-PERSON'], ['Fornos', 'I-PERSON'], ['.', 'O']]\n",
      "[['On', 'O'], ['August', 'B-DATE'], ['25', 'I-DATE'], ['President', 'O'], ['Chen', 'B-PERSON'], ['Shui', 'I-PERSON'], ['-', 'I-PERSON'], ['bian', 'I-PERSON'], ['wrapped', 'O'], ['up', 'O'], ['his', 'O'], ['first', 'B-ORDINAL'], ['overseas', 'O'], ['trip', 'O'], ['since', 'O'], ['taking', 'O'], ['office', 'O'], [',', 'O'], ['swinging', 'O'], ['through', 'O'], ['three', 'B-CARDINAL'], ['countries', 'O'], ['in', 'O'], ['Latin', 'B-LOC'], ['America', 'I-LOC'], ['and', 'O'], ['another', 'O'], ['three', 'B-CARDINAL'], ['in', 'O'], ['Africa', 'B-LOC'], ['.', 'O']]\n",
      "[['While', 'O'], ['in', 'O'], ['the', 'B-GPE'], ['Dominican', 'I-GPE'], ['Republic', 'I-GPE'], ['to', 'O'], ['attend', 'O'], ['the', 'O'], ['inauguration', 'O'], ['of', 'O'], ['President', 'O'], ['Hipolito', 'B-PERSON'], ['Mejia', 'I-PERSON'], [',', 'O'], ['Chen', 'B-PERSON'], ['had', 'O'], ['a', 'O'], ['chance', 'O'], ['to', 'O'], ['meet', 'O'], ['with', 'O'], ['the', 'O'], ['leaders', 'O'], ['of', 'O'], ['many', 'O'], ['different', 'O'], ['nations', 'O'], ['.', 'O']]\n",
      "------------------------------------------\n",
      "7920\n",
      "[['The', 'O'], ['enterovirus', 'O'], ['detection', 'O'], ['biochip', 'O'], ['developed', 'O'], ['by', 'O'], ['DR.', 'B-ORG'], ['Chip', 'I-ORG'], ['Biotechnology', 'I-ORG'], ['takes', 'O'], ['only', 'B-TIME'], ['six', 'I-TIME'], ['hours', 'I-TIME'], ['to', 'O'], ['give', 'O'], ['hospitals', 'O'], ['the', 'O'], ['answer', 'O'], ['to', 'O'], ['whether', 'O'], ['a', 'O'], ['sample', 'O'], ['contains', 'O'], ['enterovirus', 'O'], [',', 'O'], ['and', 'O'], ['if', 'O'], ['it', 'O'], ['is', 'O'], ['the', 'O'], ['deadly', 'O'], ['strain', 'O'], ['Entero', 'O'], ['71', 'O'], ['.', 'O']]\n",
      "[['Worldwide', 'O'], [',', 'O'], ['biotechnology', 'O'], ['is', 'O'], ['a', 'O'], ['rising', 'O'], ['star', 'O'], ['of', 'O'], ['the', 'O'], ['industrial', 'O'], ['stage', 'O'], ['.', 'O']]\n",
      "[['In', 'O'], ['Taiwan', 'B-GPE'], [',', 'O'], ['in', 'O'], ['recent', 'O'], ['years', 'O'], ['the', 'B-ORG'], ['Ministry', 'I-ORG'], ['of', 'I-ORG'], ['Economic', 'I-ORG'], ['Affairs', 'I-ORG'], [',', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Science', 'I-ORG'], ['Council', 'I-ORG'], ['and', 'O'], ['the', 'B-ORG'], ['National', 'I-ORG'], ['Health', 'I-ORG'], ['Research', 'I-ORG'], ['Institutes', 'I-ORG'], ['have', 'O'], ['been', 'O'], ['strongly', 'O'], ['pursuing', 'O'], ['\"', 'O'], ['biochip', 'O'], ['\"', 'O'], ['research', 'O'], ['programs', 'O'], ['.', 'O']]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('------------------------------------------')\n",
    "dev_data_clean = clean(dev_data_full)\n",
    "print(len(dev_data_clean))\n",
    "for item in dev_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')\n",
    "test_data_clean = clean(test_data_full)\n",
    "print(len(test_data_clean))\n",
    "for item in test_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "cZHhXzVTQA_P",
    "outputId": "aa4715b9-a966-490a-d09e-6ff8c947ba92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'text': ['With', 'this', 'trend', ',', 'suddenly', 'the', 'mature', 'faces', 'of', 'managers', 'boasting', 'an', 'average', 'of', 'over', 'ten', 'years', 'of', 'professional', 'experience', 'have', 'flooded', 'in', 'among', 'the', 'young', 'people', 'populating', 'university', 'campuses', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
      "\n",
      "[{'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']}, {'text': ['On', 'August', '25', 'President', 'Chen', 'Shui', '-', 'bian', 'wrapped', 'up', 'his', 'first', 'overseas', 'trip', 'since', 'taking', 'office', ',', 'swinging', 'through', 'three', 'countries', 'in', 'Latin', 'America', 'and', 'another', 'three', 'in', 'Africa', '.'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'B-CARDINAL', 'O', 'B-LOC', 'O']}, {'text': ['While', 'in', 'the', 'Dominican', 'Republic', 'to', 'attend', 'the', 'inauguration', 'of', 'President', 'Hipolito', 'Mejia', ',', 'Chen', 'had', 'a', 'chance', 'to', 'meet', 'with', 'the', 'leaders', 'of', 'many', 'different', 'nations', '.'], 'tags': ['O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n"
     ]
    }
   ],
   "source": [
    "# shape into dicts per sentence\n",
    "\n",
    "def reshape_sent2dicts(f):\n",
    "    data_dict = []\n",
    "    for item in f: # list of lists (tokens)\n",
    "        #print(item)\n",
    "        sent_text= [] \n",
    "        sent_tags = []\n",
    "        for token in item:\n",
    "            if len(token) ==2:\n",
    "                sent_text.append(token[0])\n",
    "                sent_tags.append(token[1])\n",
    "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
    "        #print(sent_dict['text'])\n",
    "        #print(sent_dict['tags'])\n",
    "        data_dict.append(sent_dict)\n",
    "    return data_dict\n",
    "\n",
    "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))\n",
    "samp = train_data_sent[:2]\n",
    "print(samp)\n",
    "print()\n",
    "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
    "samp2 = dev_data_sent[:3]\n",
    "print(samp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "VpYBQMbcQGBi",
    "outputId": "91c6e6a3-b808-43b6-8e20-9ecad77fbc1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'text': ['Does', 'it', 'wake', 'you', 'up', '?'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O']}\n",
      "\n",
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "------------\n",
      "0\n",
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "-----------------------------\n",
      "30000\n",
      "-----------------------\n",
      "Text:  ['Does', 'it', 'wake', 'you', 'up', '?']\n",
      " Texts length:  30000\n",
      "Label:  ['O', 'O', 'O', 'O', 'O', 'O']\n",
      " Labels length:  30000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "random.seed(123)\n",
    "random.shuffle(train_data_sent)\n",
    "#max_sent = [max(len(i[\"text\"])) for i in train_data_sent]\n",
    "#print(max_sent)\n",
    "print(type(train_data_sent))\n",
    "print(train_data_sent[0]) ##one dict\n",
    "print()\n",
    "print(train_data_sent[0][\"text\"])\n",
    "print()\n",
    "print(train_data_sent[0][\"tags\"])\n",
    "print('------------')\n",
    "\n",
    "def typed_listing(data, key):\n",
    "    listed = []\n",
    "    max_length = 0\n",
    "    for item in data: # dictionary {text:\"\", tags:\"\"}\n",
    "        #print('Item: ', item)\n",
    "        #print('Key: ', key, ' content: ', item[key], 'length: ',len(item[key]))\n",
    "        if len(item[key]) > max_length:\n",
    "            max = len(item[key])\n",
    "        listed.append(item[key])\n",
    "    return listed, max_length\n",
    "\n",
    "listed_texts= typed_listing(train_data_sent, \"text\")\n",
    "train_texts = listed_texts[0]\n",
    "train_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(train_data_sent, \"tags\")\n",
    "train_labels= listed_labels[0]\n",
    "train_lbl_max = listed_labels[1]\n",
    "print(train_txt_max)\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "\n",
    "\n",
    "print('-----------------------------')\n",
    "print(len(train_texts))\n",
    "print('-----------------------')\n",
    "print('Text: ', train_texts[0])\n",
    "print(' Texts length: ',len(train_texts))\n",
    "print('Label: ', train_labels[0])\n",
    "print(' Labels length: ',len(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "DNQQRw0YO-Ng",
    "outputId": "474f0076-ea05-457c-aad4-13c726b7eb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos', '.']\n",
      " Texts length:  9954\n",
      "Label:  ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O']\n",
      " Labels length:  9954\n"
     ]
    }
   ],
   "source": [
    "## same for validation/dev data\n",
    "listed_texts= typed_listing(dev_data_sent, \"text\")\n",
    "dev_texts = listed_texts[0]\n",
    "dev_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(dev_data_sent, \"tags\")\n",
    "dev_labels= listed_labels[0]\n",
    "dev_lbl_max = listed_labels[1]\n",
    "print('Text: ', dev_texts[0])\n",
    "print(' Texts length: ',len(dev_texts))\n",
    "print('Label: ', dev_labels[0])\n",
    "print(' Labels length: ',len(dev_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ICn08fOgbyXl",
    "outputId": "f0c0a0e9-1dca-4d7b-9fa9-264dea736c5a"
   },
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "#!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bFv2qclTbyXp",
    "outputId": "06369298-d917-4d82-b580-04a09bef3870"
   },
   "outputs": [],
   "source": [
    "# Give -n argument so that a possible existing file isn't overwritten \n",
    "#!unzip -n wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "kx_C5ii8byXt",
    "outputId": "9f9fb2d2-1c77-417d-cbd7-638b0022ef3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
    "print(\"Words from embedding model:\", len(words))\n",
    "print(\"First 50 words:\", words[:50])\n",
    "\n",
    "# Normalize the vectors to unit length\n",
    "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkdgjgOlbyXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary mappings\n",
    "\n",
    "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
    "# Also reserve an index for out-of-vocabulary items.\n",
    "vocabulary={\n",
    "    \"<PAD>\": 0,\n",
    "    \"<OOV>\": 1\n",
    "}\n",
    "\n",
    "for word in words: # These are words from the word2vec model\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
    "\n",
    "\n",
    "# Embedding matrix\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGaojUBhbyX2"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_M9Ox5_ObyX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-CARDINAL': 1,\n",
      " 'B-DATE': 16,\n",
      " 'B-EVENT': 31,\n",
      " 'B-FAC': 25,\n",
      " 'B-GPE': 29,\n",
      " 'B-LANGUAGE': 26,\n",
      " 'B-LAW': 6,\n",
      " 'B-LOC': 17,\n",
      " 'B-MONEY': 21,\n",
      " 'B-NORP': 23,\n",
      " 'B-ORDINAL': 19,\n",
      " 'B-ORG': 15,\n",
      " 'B-PERCENT': 13,\n",
      " 'B-PERSON': 5,\n",
      " 'B-PRODUCT': 22,\n",
      " 'B-QUANTITY': 0,\n",
      " 'B-TIME': 35,\n",
      " 'B-WORK_OF_ART': 2,\n",
      " 'I-CARDINAL': 34,\n",
      " 'I-DATE': 7,\n",
      " 'I-EVENT': 11,\n",
      " 'I-FAC': 30,\n",
      " 'I-GPE': 8,\n",
      " 'I-LANGUAGE': 4,\n",
      " 'I-LAW': 12,\n",
      " 'I-LOC': 3,\n",
      " 'I-MONEY': 9,\n",
      " 'I-NORP': 24,\n",
      " 'I-ORDINAL': 36,\n",
      " 'I-ORG': 18,\n",
      " 'I-PERCENT': 28,\n",
      " 'I-PERSON': 20,\n",
      " 'I-PRODUCT': 10,\n",
      " 'I-QUANTITY': 14,\n",
      " 'I-TIME': 27,\n",
      " 'I-WORK_OF_ART': 32,\n",
      " 'O': 33}\n"
     ]
    }
   ],
   "source": [
    "#Labels\n",
    "\n",
    "\n",
    "not_letter = re.compile(r'[^a-zA-Z]')\n",
    "# Label mappings\n",
    "# 1) gather a set of unique labels\n",
    "label_set = set()\n",
    "for sentence_labels in train_labels: #loops over sentences \n",
    "    #print(sentence_labels)\n",
    "    for label in sentence_labels: #loops over labels in one sentence\n",
    "       # match = not_letter.match(label)\n",
    "        #if match or label== 'O':\n",
    "        #    break\n",
    "        #else:    \n",
    "        label_set.add(label)\n",
    "\n",
    "# 2) index these\n",
    "label_map = {}\n",
    "for index, label in enumerate(label_set):\n",
    "    label_map[label]=index\n",
    "    \n",
    "pprint(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8k8DshceEaI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 33, 33, 33, 33, 33]\n"
     ]
    }
   ],
   "source": [
    "# vectorize the labels\n",
    "def label_vectorizer(train_labels,label_map):\n",
    "    vectorized_labels = []\n",
    "    for label in train_labels:\n",
    "        vectorized_example_label = []\n",
    "        for token in label:\n",
    "            if token in label_map:\n",
    "                vectorized_example_label.append(label_map[token])\n",
    "        vectorized_labels.append(vectorized_example_label)\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    return vectorized_labels\n",
    "        \n",
    "\n",
    "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
    "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
    "\n",
    "print(vectorized_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUtqLdCMPf3X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does', 'it', 'wake', 'you', 'up', '?']\n",
      "[3328, 21, 8846, 32, 91, 39]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## vectorization of the texts\n",
    "def text_vectorizer(vocab, train_texts):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(train_texts):\n",
    "        vectorized_example = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "\n",
    "        vectorized_data.append(vectorized_example)     \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
    "    \n",
    "    return vectorized_data, sentence_lengths\n",
    "\n",
    "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
    "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
    "\n",
    "print(train_texts[0])\n",
    "print(vectorized_data[0])\n",
    "pprint(type(lengths))\n",
    "#max = lengths.index(17040)\n",
    "#print(max)\n",
    "#pprint(train_texts[11103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9e6FH5F1QGrq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (30000,)\n",
      "New shape: (30000, 168)\n",
      "First example:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 3328   21 8846   32   91   39]\n",
      "<class 'numpy.ndarray'>\n",
      "Padded labels shape: (30000, 168, 1)\n",
      "First example labels:\n",
      "First weight vector:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# padding for tensor\n",
    "import tensorflow as tf\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "max_len = np.max(np.array(lengths))\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max_len)\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\")\n",
    "print( vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=np.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max_len), -1)\n",
    "\n",
    "print(type(vectorized_labels_padded))\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "#pprint(label_map)\n",
    "print(\"First example labels:\")\n",
    "#pprint(vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\")\n",
    "print( weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max_len)\n",
    "validation_vectorized_labels_padded=np.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max_len), -1)\n",
    "validation_weights = np.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhAOVAbBTRAv"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "import keras\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    return entity_offsets\n",
    "\n",
    "def _entity_level_PRF(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    \n",
    "    try:\n",
    "        precision = tp / pred_count # tp / (tp+np)\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "    return precision, recall, fscore             \n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.fscore = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
    "        self.precision.append(evaluation_parameters[0])\n",
    "        self.recall.append(evaluation_parameters[1])\n",
    "        self.fscore.append(evaluation_parameters[2])\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVS-GKkQUpue"
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed, SimpleRNN\n",
    "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = Dense(hidden_size, activation=\"relu\")(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "model1 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gj3x13FELJLt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 168)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 168, 50)           15050     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 168, 37)           1887      \n",
      "=================================================================\n",
      "Total params: 15,017,537\n",
      "Trainable params: 16,937\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QuWSrGPVUw9Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 111s - loss: 0.0327\n",
      "\n",
      "Precision/Recall/F-score: 0.45454545454545453 / 0.0005009016229212583 / 0.0010007004903432402\n",
      "Epoch 2/3\n",
      " - 105s - loss: 0.0201\n",
      "\n",
      "Precision/Recall/F-score: 0.4666666666666667 / 0.0007012622720897616 / 0.0014004201260378114\n",
      "Epoch 3/3\n",
      " - 108s - loss: 0.0189\n",
      "\n",
      "Precision/Recall/F-score: 0.4375 / 0.0007012622720897616 / 0.0014002800560112022\n"
     ]
    }
   ],
   "source": [
    "# train model 1\n",
    "optimizer=Adam(lr=1e-4) # define the learning rate\n",
    "model1.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "vanilla_hist=model1.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=1,verbose=2,epochs=3, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU and Adam gave bad results with different learning rates and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecFC_-OhC1lN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: [0.0010007004903432402, 0.0014004201260378114, 0.0014002800560112022]\n",
      "Highest f-score: 0.0014004201260378114\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the f scores\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "def plot_history(fscores):\n",
    "    max_f = np.max(np.array(fscores))\n",
    "    print(\"History:\", fscores)\n",
    "    print(\"Highest f-score:\", max_f)\n",
    "    plt.plot(fscores)\n",
    "    plt.legend(loc='lower center', borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\tanja\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from h5py) (1.17.4)\n",
      "Requirement already satisfied: six in c:\\users\\tanja\\anaconda3\\lib\\site-packages (from h5py) (1.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"NEWModel1_Adam0.001.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwMwwo9-MPv4"
   },
   "source": [
    "## 1.2 Expand context\n",
    "\n",
    "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lw9pXRewbyX6"
   },
   "outputs": [],
   "source": [
    "#expanding to RNN model with context\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "rnn_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
    "    outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPP-kwoNXUMb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 168)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 168, 50)           70200     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 168, 37)           1887      \n",
      "=================================================================\n",
      "Total params: 15,072,687\n",
      "Trainable params: 72,087\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIal9meVXnN_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "optimizer=Adam(lr=1e-10) # define the learning rate\n",
    "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer bidirectional_1: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c68cf79f33b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mrnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_rnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-c68cf79f33b3>\u001b[0m in \u001b[0;36mbuild_rnn_model\u001b[1;34m(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0minp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0moutp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer bidirectional_1: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "#expanding to RNN model with context\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "rnn_size = 50\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    #embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "    rnn = Bidirectional(LSTM(rnn_size, activation='relu', return_sequences=True))(inp)\n",
    "    outp=TimeDistributed(Dense(class_count, activation=\"softmax\"))(rnn)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.save(\"CRAP2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRKNs4t8X3Ed"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCo0xF5kMMbH"
   },
   "source": [
    "## 2.1 Use deep contextual representations\n",
    "\n",
    "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgSYNcerMI9R"
   },
   "source": [
    "## 2.2 Error analysis\n",
    "\n",
    "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 168)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 168, 50)           15050     \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 168, 37)           1887      \n",
      "=================================================================\n",
      "Total params: 15,017,537\n",
      "Trainable params: 16,937\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#I want to see how the model predicts classes\n",
    "\n",
    "\n",
    "# Recreate the exact same rnn, including its weights and the optimizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "simple_rnn = tf.keras.models.load_model('Model1_Adam.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_predictions = simple_rnn.predict(vectorized_data_padded)  #save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00923516e-03, 3.70084337e-04, 2.46811518e-03, 1.86059671e-03,\n",
       "       1.80526462e-03, 1.96474921e-02, 1.98890315e-03, 3.88048938e-04,\n",
       "       3.46431532e-03, 8.96527991e-03, 1.16933300e-03, 2.04792549e-03,\n",
       "       2.45094928e-03, 1.23758800e-02, 1.77301303e-03, 8.77117636e-05,\n",
       "       5.58422646e-04, 1.65068393e-03, 2.11346359e-03, 2.18506940e-02,\n",
       "       5.83406836e-05, 1.00549106e-02, 1.00687124e-07, 1.56564042e-02,\n",
       "       2.50884541e-03, 8.49931955e-01, 1.56020680e-02, 2.94749867e-07,\n",
       "       1.32280961e-03, 1.38389622e-03, 1.98811502e-03, 3.84029234e-03,\n",
       "       2.17660354e-03, 4.04416234e-04, 3.41106812e-03, 1.84880418e-03,\n",
       "       1.76567317e-03], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rnn_predictions[0][0]   #see what the output looks like\n",
    "#The outputs are probabilities for each class (tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "list_of_tag_args = []    #create a list of saved positions for each entity label\n",
    "for i in simple_rnn_predictions:\n",
    "    for j in i:\n",
    "        temp = np.argmax(j)\n",
    "        list_of_tag_args.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 13,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tag_args  #Now that we have this list, we can use our label map dictionary to identify the actual tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_of_tag_args[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 168)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 168, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 168, 50)           70200     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 168, 37)           1887      \n",
      "=================================================================\n",
      "Total params: 15,072,687\n",
      "Trainable params: 72,087\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#I want to see how the model predicts classes\n",
    "\n",
    "\n",
    "# Recreate the exact same lstm, including its weights and the optimizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "lstm = tf.keras.models.load_model('CRAP2.h5')   #This model has an f-score of 0.99\n",
    "\n",
    "# Show the model architecture\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions = lstm.predict(vectorized_data_padded)  #save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_predictions[0][0]   #see what the output looks like\n",
    "#The outputs are probabilities for each class (tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lstm_tag_args = []    #create a list of saved positions for each entity label\n",
    "for i in lstm_predictions:\n",
    "    for j in i:\n",
    "        temp = np.argmax(j)\n",
    "        list_of_lstm_tag_args.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lstm_tag_args  #Now that we have this list, we can use our label map dictionary to identify the actual tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRDxKgLSL_uf"
   },
   "source": [
    "## 3.1 Predictions on unannotated text\n",
    "\n",
    "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlG6ZWkIL-HY"
   },
   "source": [
    "## 3.2 Statistically analyze the results\n",
    "\n",
    "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DL_NER_debug2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
