{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainvo/DeepLearning_NER/blob/master/DL_NER_RNN_not_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTBsYI1tLeVk"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2GevEzfPP2",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/api/annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O5MwAmUALZ4V"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q3HiGQgMU5L",
        "outputId": "0dfdddec-a0aa-468f-83e1-bf550215f042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "# Training data: Used for training the model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
        "\n",
        "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
        "\n",
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
        "\n",
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-08 17:32:11--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17252156 (16M) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "\rtrain.tsv             0%[                    ]       0  --.-KB/s               \rtrain.tsv            27%[====>               ]   4.53M  22.6MB/s               \rtrain.tsv           100%[===================>]  16.45M  45.1MB/s    in 0.4s    \n",
            "\n",
            "2020-05-08 17:32:12 (45.1 MB/s) - ‘train.tsv’ saved [17252156/17252156]\n",
            "\n",
            "--2020-05-08 17:32:14--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2419425 (2.3M) [text/plain]\n",
            "Saving to: ‘dev.tsv’\n",
            "\n",
            "dev.tsv             100%[===================>]   2.31M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-08 17:32:14 (17.6 MB/s) - ‘dev.tsv’ saved [2419425/2419425]\n",
            "\n",
            "--2020-05-08 17:32:15--  https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1788466 (1.7M) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>]   1.71M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-08 17:32:16 (16.1 MB/s) - ‘test.tsv’ saved [1788466/1788466]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOOHEYpiMzFp",
        "outputId": "15c6aa2f-f828-425e-cecc-ca97a3347694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "from collections import namedtuple\n",
        "OneWord=namedtuple(\"OneWord\",[\"word\",\"entity_label\"])\n",
        "\n",
        "def read_ontonotes(tsv_file):\n",
        "  #\"\"\"Yield complete sentences\"\"\"\n",
        "    current_sentence=[] # list of (word,label) tuples\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\t')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line: #sentence break\n",
        "                if current_sentence: #if we gathered a sentence, we should yield it, because a new starts\n",
        "                    yield current_sentence #much like return, but continues past this line once the element has been consumed\n",
        "                    current_sentence=[] #...and start a new one\n",
        "                continue\n",
        "            #if we made it here, we are on a normal line\n",
        "            columns=[line[0], line[1]] #an actual word line\n",
        "            assert len(columns)==2 #we should have four columns, looking at the data\n",
        "            current_sentence.append(OneWord(*columns)) #shorthand for looping over columns\n",
        "        else: #for ... else -> the else part is executed once, when \"for\" runs out of elements\n",
        "            if current_sentence: #yield also the last one!\n",
        "                yield current_sentence\n",
        "\n",
        "#read the data in as sentences\n",
        "sentences_train=list(read_ontonotes(\"train.tsv\"))\n",
        "sentences_dev=list(read_ontonotes(\"dev.tsv\"))\n",
        "sentences_test = list(read_ontonotes(\"test.tsv\"))\n",
        "\n",
        "print(type(sentences_test))\n",
        "\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_train[:3]:\n",
        "    print(sent)\n",
        "print(len(sentences_train))\n",
        "print('---------------------------------------------')\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_dev[:3]:\n",
        "    print(sent)\n",
        "print(len(sentences_dev))\n",
        "print('---------------------------------------------')\n",
        "print(\"First three sentences\")\n",
        "for sent in sentences_test[:3]:\n",
        "    print(sent)\n",
        "print(len(sentences_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "First three sentences\n",
            "[OneWord(word='Big', entity_label='O'), OneWord(word='Managers', entity_label='O'), OneWord(word='on', entity_label='O'), OneWord(word='Campus', entity_label='O')]\n",
            "[OneWord(word='In', entity_label='O'), OneWord(word='recent', entity_label='B-DATE'), OneWord(word='years', entity_label='I-DATE'), OneWord(word=',', entity_label='O'), OneWord(word='advanced', entity_label='O'), OneWord(word='education', entity_label='O'), OneWord(word='for', entity_label='O'), OneWord(word='professionals', entity_label='O'), OneWord(word='has', entity_label='O'), OneWord(word='become', entity_label='O'), OneWord(word='a', entity_label='O'), OneWord(word='hot', entity_label='O'), OneWord(word='topic', entity_label='O'), OneWord(word='in', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='business', entity_label='O'), OneWord(word='community', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "[OneWord(word='With', entity_label='O'), OneWord(word='this', entity_label='O'), OneWord(word='trend', entity_label='O'), OneWord(word=',', entity_label='O'), OneWord(word='suddenly', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='mature', entity_label='O'), OneWord(word='faces', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='managers', entity_label='O'), OneWord(word='boasting', entity_label='O'), OneWord(word='an', entity_label='O'), OneWord(word='average', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='over', entity_label='O'), OneWord(word='ten', entity_label='B-DATE'), OneWord(word='years', entity_label='I-DATE'), OneWord(word='of', entity_label='O'), OneWord(word='professional', entity_label='O'), OneWord(word='experience', entity_label='O'), OneWord(word='have', entity_label='O'), OneWord(word='flooded', entity_label='O'), OneWord(word='in', entity_label='O'), OneWord(word='among', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='young', entity_label='O'), OneWord(word='people', entity_label='O'), OneWord(word='populating', entity_label='O'), OneWord(word='university', entity_label='O'), OneWord(word='campuses', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "66818\n",
            "---------------------------------------------\n",
            "First three sentences\n",
            "[OneWord(word='President', entity_label='B-WORK_OF_ART'), OneWord(word='Chen', entity_label='I-WORK_OF_ART'), OneWord(word='Travels', entity_label='I-WORK_OF_ART'), OneWord(word='Abroad', entity_label='I-WORK_OF_ART')]\n",
            "[OneWord(word='(', entity_label='O'), OneWord(word='Chang', entity_label='B-PERSON'), OneWord(word='Chiung', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='fang', entity_label='I-PERSON'), OneWord(word='/', entity_label='O'), OneWord(word='tr.', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='David', entity_label='B-PERSON'), OneWord(word='Mayer', entity_label='I-PERSON'), OneWord(word=')', entity_label='O')]\n",
            "[OneWord(word='President', entity_label='O'), OneWord(word='Chen', entity_label='B-PERSON'), OneWord(word='Shui', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='bian', entity_label='I-PERSON'), OneWord(word='visited', entity_label='O'), OneWord(word='the', entity_label='B-FAC'), OneWord(word='Nicaraguan', entity_label='I-FAC'), OneWord(word='National', entity_label='I-FAC'), OneWord(word='Assembly', entity_label='I-FAC'), OneWord(word='on', entity_label='O'), OneWord(word='August', entity_label='B-DATE'), OneWord(word='17', entity_label='I-DATE'), OneWord(word=',', entity_label='O'), OneWord(word='where', entity_label='O'), OneWord(word='he', entity_label='O'), OneWord(word='received', entity_label='O'), OneWord(word='a', entity_label='O'), OneWord(word='medal', entity_label='O'), OneWord(word='from', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='president', entity_label='O'), OneWord(word='of', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='assembly', entity_label='O'), OneWord(word=',', entity_label='O'), OneWord(word='Ivan', entity_label='B-PERSON'), OneWord(word='Escobar', entity_label='I-PERSON'), OneWord(word='Fornos', entity_label='I-PERSON'), OneWord(word='.', entity_label='O')]\n",
            "11612\n",
            "---------------------------------------------\n",
            "First three sentences\n",
            "[OneWord(word='Powerful', entity_label='B-WORK_OF_ART'), OneWord(word='Tools', entity_label='I-WORK_OF_ART'), OneWord(word='for', entity_label='I-WORK_OF_ART'), OneWord(word='Biotechnology', entity_label='I-WORK_OF_ART'), OneWord(word='-', entity_label='I-WORK_OF_ART'), OneWord(word='Biochips', entity_label='I-WORK_OF_ART')]\n",
            "[OneWord(word='(', entity_label='O'), OneWord(word='Chang', entity_label='B-PERSON'), OneWord(word='Chiung', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='fang', entity_label='I-PERSON'), OneWord(word='/', entity_label='O'), OneWord(word='photos', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='Hsueh', entity_label='B-PERSON'), OneWord(word='Chi', entity_label='I-PERSON'), OneWord(word='-', entity_label='I-PERSON'), OneWord(word='kuang', entity_label='I-PERSON'), OneWord(word='/', entity_label='O'), OneWord(word='tr.', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='Robert', entity_label='B-PERSON'), OneWord(word='Taylor', entity_label='I-PERSON'), OneWord(word=')', entity_label='O')]\n",
            "[OneWord(word='The', entity_label='O'), OneWord(word='enterovirus', entity_label='O'), OneWord(word='detection', entity_label='O'), OneWord(word='biochip', entity_label='O'), OneWord(word='developed', entity_label='O'), OneWord(word='by', entity_label='O'), OneWord(word='DR.', entity_label='B-ORG'), OneWord(word='Chip', entity_label='I-ORG'), OneWord(word='Biotechnology', entity_label='I-ORG'), OneWord(word='takes', entity_label='O'), OneWord(word='only', entity_label='B-TIME'), OneWord(word='six', entity_label='I-TIME'), OneWord(word='hours', entity_label='I-TIME'), OneWord(word='to', entity_label='O'), OneWord(word='give', entity_label='O'), OneWord(word='hospitals', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='answer', entity_label='O'), OneWord(word='to', entity_label='O'), OneWord(word='whether', entity_label='O'), OneWord(word='a', entity_label='O'), OneWord(word='sample', entity_label='O'), OneWord(word='contains', entity_label='O'), OneWord(word='enterovirus', entity_label='O'), OneWord(word=',', entity_label='O'), OneWord(word='and', entity_label='O'), OneWord(word='if', entity_label='O'), OneWord(word='it', entity_label='O'), OneWord(word='is', entity_label='O'), OneWord(word='the', entity_label='O'), OneWord(word='deadly', entity_label='O'), OneWord(word='strain', entity_label='O'), OneWord(word='Entero', entity_label='O'), OneWord(word='71', entity_label='O'), OneWord(word='.', entity_label='O')]\n",
            "9751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZHhXzVTQA_P",
        "colab": {}
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for line in f:\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for OneWord in line:\n",
        "            #print(OneWord)\n",
        "            sent_text.append(OneWord.word)\n",
        "            sent_tags.append(OneWord.entity_label)\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict)\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "train_data = list(reshape_sent2dicts(sentences_train[:30000]))\n",
        "\n",
        "dev_data = list(reshape_sent2dicts(sentences_dev))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpYBQMbcQGBi",
        "outputId": "9db5f59b-fbff-406a-9395-56f531c9faf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "random.seed(123)\n",
        "random.shuffle(train_data)\n",
        "print(type(train_data))\n",
        "print(type(train_data[0]))\n",
        "\n",
        "train_texts=[i[\"text\"] for i in train_data]\n",
        "train_labels=[i[\"tags\"] for i in train_data]\n",
        "\n",
        "print(type(train_texts))\n",
        "print(type(train_texts[0]))\n",
        "\n",
        "print('Text: ', train_texts[0])\n",
        "print('Label: ', train_labels[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'dict'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "Text:  ['Sharon', 'Repudiates', 'the', 'Road', 'Map', '.']\n",
            "Label:  ['O', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQQRw0YO-Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## same for validation/dev data\n",
        "dev_texts=[i[\"text\"] for i in dev_data]\n",
        "dev_labels=[i[\"tags\"] for i in dev_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICn08fOgbyXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3bb202f3-2b13-4986-cd7f-e1cdc9183e89"
      },
      "source": [
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-08 17:32:20--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  5.65MB/s    in 48s     \n",
            "\n",
            "2020-05-08 17:33:08 (13.7 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFv2qclTbyXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7aaa2eda-1927-4b53-d8f0-5b9eb8da41af"
      },
      "source": [
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_C5ii8byXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "96f20ed2-8bf3-4245-cecf-7ef3a1f17881"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdgjgOlbyXx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0ec63394-3b51-4b37-8727-66f02ff2df4b"
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGaojUBhbyX2",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Ox5_ObyX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "bc644435-f089-4c79-b3ad-8bfea470c49d"
      },
      "source": [
        "#Labels\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in train_labels: #loops over sentences \n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 4,\n",
            " 'B-DATE': 31,\n",
            " 'B-EVENT': 24,\n",
            " 'B-FAC': 13,\n",
            " 'B-GPE': 2,\n",
            " 'B-LANGUAGE': 0,\n",
            " 'B-LAW': 5,\n",
            " 'B-LOC': 16,\n",
            " 'B-MONEY': 8,\n",
            " 'B-NORP': 36,\n",
            " 'B-ORDINAL': 7,\n",
            " 'B-ORG': 22,\n",
            " 'B-PERCENT': 15,\n",
            " 'B-PERSON': 3,\n",
            " 'B-PRODUCT': 10,\n",
            " 'B-QUANTITY': 35,\n",
            " 'B-TIME': 6,\n",
            " 'B-WORK_OF_ART': 26,\n",
            " 'I-CARDINAL': 27,\n",
            " 'I-DATE': 14,\n",
            " 'I-EVENT': 17,\n",
            " 'I-FAC': 21,\n",
            " 'I-GPE': 28,\n",
            " 'I-LANGUAGE': 30,\n",
            " 'I-LAW': 34,\n",
            " 'I-LOC': 33,\n",
            " 'I-MONEY': 23,\n",
            " 'I-NORP': 9,\n",
            " 'I-ORDINAL': 20,\n",
            " 'I-ORG': 32,\n",
            " 'I-PERCENT': 11,\n",
            " 'I-PERSON': 1,\n",
            " 'I-PRODUCT': 25,\n",
            " 'I-QUANTITY': 29,\n",
            " 'I-TIME': 12,\n",
            " 'I-WORK_OF_ART': 19,\n",
            " 'O': 18}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k8DshceEaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "791c5c85-f844-4460-f971-5d9340be1a77"
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in train_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
        "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "pprint(vectorized_labels[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18, 18, 18, 18, 18, 18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtqLdCMPf3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8854df5f-6046-48af-d73a-304aa1221453"
      },
      "source": [
        "## vectorization of the texts\n",
        "def text_vectorizer(vocab, train_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(train_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
        "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "\n",
        "pprint(train_texts[0])\n",
        "pprint(vectorized_data[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sharon', 'Repudiates', 'the', 'Road', 'Map', '.']\n",
            "[8346, 1, 3, 1685, 8936, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6FH5F1QGrq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "724f9558-4b12-4186-d0a6-ecc4bb84044b"
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max(lengths))\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max(lengths)), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])\n",
        "\n",
        "# Same stuff for the validation data\n",
        "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max(lengths))\n",
        "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post',maxlen=max(lengths)), -1)\n",
        "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
        "validation_weights[validation_weights > 0] = 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Old shape: (30000,)\n",
            "New shape: (30000, 561)\n",
            "First example:\n",
            "[8346    1    3 1685 8936    4    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n",
            "Padded labels shape: (30000, 561, 1)\n",
            "{'B-CARDINAL': 4,\n",
            " 'B-DATE': 31,\n",
            " 'B-EVENT': 24,\n",
            " 'B-FAC': 13,\n",
            " 'B-GPE': 2,\n",
            " 'B-LANGUAGE': 0,\n",
            " 'B-LAW': 5,\n",
            " 'B-LOC': 16,\n",
            " 'B-MONEY': 8,\n",
            " 'B-NORP': 36,\n",
            " 'B-ORDINAL': 7,\n",
            " 'B-ORG': 22,\n",
            " 'B-PERCENT': 15,\n",
            " 'B-PERSON': 3,\n",
            " 'B-PRODUCT': 10,\n",
            " 'B-QUANTITY': 35,\n",
            " 'B-TIME': 6,\n",
            " 'B-WORK_OF_ART': 26,\n",
            " 'I-CARDINAL': 27,\n",
            " 'I-DATE': 14,\n",
            " 'I-EVENT': 17,\n",
            " 'I-FAC': 21,\n",
            " 'I-GPE': 28,\n",
            " 'I-LANGUAGE': 30,\n",
            " 'I-LAW': 34,\n",
            " 'I-LOC': 33,\n",
            " 'I-MONEY': 23,\n",
            " 'I-NORP': 9,\n",
            " 'I-ORDINAL': 20,\n",
            " 'I-ORG': 32,\n",
            " 'I-PERCENT': 11,\n",
            " 'I-PERSON': 1,\n",
            " 'I-PRODUCT': 25,\n",
            " 'I-QUANTITY': 29,\n",
            " 'I-TIME': 12,\n",
            " 'I-WORK_OF_ART': 19,\n",
            " 'O': 18}\n",
            "First example labels:\n",
            "array([[18],\n",
            "       [18],\n",
            "       [18],\n",
            "       [18],\n",
            "       [18],\n",
            "       [18],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0],\n",
            "       [ 0]], dtype=int32)\n",
            "First weight vector:\n",
            "[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAOVAbBTRAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function\n",
        "import keras\n",
        "\n",
        "def _convert_to_entities(input_sequence):\n",
        "    \"\"\"\n",
        "    Reads a sequence of tags and converts them into a set of entities.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    previous_tag = label_map['O']\n",
        "    for i, tag in enumerate(input_sequence):\n",
        "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "            current_entity.append((tag, i))\n",
        "        elif tag == label_map['O']: # Entity has ended\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "        elif tag == previous_tag: # Current entity continues\n",
        "            current_entity.append((tag, i))\n",
        "        previous_tag = tag\n",
        "    \n",
        "    # Add the last entity to our entity list if the sentences ends with an entity\n",
        "    if len(current_entity) > 0:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    entity_offsets = set()\n",
        "    \n",
        "    for e in entities:\n",
        "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
        "    return entity_offsets\n",
        "\n",
        "def _entity_level_PRF(predictions, gold, lengths):\n",
        "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
        "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
        "    \n",
        "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
        "    pred_count = sum([len(e) for e in pred_entities])\n",
        "    \n",
        "    try:\n",
        "        precision = tp / pred_count # tp / (tp+np)\n",
        "        recall = tp / sum([len(e) for e in gold_entities])\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    except Exception as e:\n",
        "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
        "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
        "    return precision, recall, fscore             \n",
        "\n",
        "def evaluate(predictions, gold, lengths):\n",
        "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
        "    return precision, recall, fscore\n",
        "\n",
        "class EvaluateEntities(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.precision = []\n",
        "        self.recall = []\n",
        "        self.fscore = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
        "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
        "        self.precision.append(evaluation_parameters[0])\n",
        "        self.recall.append(evaluation_parameters[1])\n",
        "        self.fscore.append(evaluation_parameters[2])\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ydCexfTg5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oUO3GLfTrl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "e0f6df8c-bb12-414d-aadc-feb26fca9693"
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 561, 300)          15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 561, 100)          30100     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 561, 37)           3737      \n",
            "=================================================================\n",
            "Total params: 15,034,437\n",
            "Trainable params: 33,837\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIPQrLXUVrWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "7610ed55-e9de-4792-8f68-42f431a749c1"
      },
      "source": [
        "# train the model\n",
        "optimizer=Adam(lr=0.001) # define the learning rate\n",
        "model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 4s - loss: 0.0256\n",
            "\n",
            "Precision/Recall/F-score: 0.4979462285287528 / 0.0964103676390847 / 0.1615433538265847\n",
            "Epoch 2/10\n",
            " - 2s - loss: 0.0090\n",
            "\n",
            "Precision/Recall/F-score: 0.46164978292329956 / 0.2191013266818494 / 0.2971661110021573\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.0080\n",
            "\n",
            "Precision/Recall/F-score: 0.5005054930241962 / 0.2684452156309872 / 0.34945882352941177\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.0075\n",
            "\n",
            "Precision/Recall/F-score: 0.4998660056277636 / 0.26971044355276 / 0.35037216182582354\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.0073\n",
            "\n",
            "Precision/Recall/F-score: 0.49360727689078304 / 0.28051910494161875 / 0.35773557071731515\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.0071\n",
            "\n",
            "Precision/Recall/F-score: 0.4888363851151801 / 0.2991721794454687 / 0.3711793330791828\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.0070\n",
            "\n",
            "Precision/Recall/F-score: 0.48504715896020245 / 0.3048837797780429 / 0.3744201016625602\n",
            "Epoch 8/10\n",
            " - 2s - loss: 0.0069\n",
            "\n",
            "Precision/Recall/F-score: 0.48588551585078976 / 0.3135957777536782 / 0.3811762638135202\n",
            "Epoch 9/10\n",
            " - 2s - loss: 0.0069\n",
            "\n",
            "Precision/Recall/F-score: 0.49505471996254463 / 0.30578751400788057 / 0.3780558659217877\n",
            "Epoch 10/10\n",
            " - 2s - loss: 0.0068\n",
            "\n",
            "Precision/Recall/F-score: 0.48802776171197226 / 0.3050283772548169 / 0.3754143216248081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bah0URSVV5GP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "e4bac5bd-0cba-4005-8a16-ac7757cf714f"
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    print(\"History:\", fscores)\n",
        "    print(\"Highest f-score:\", max(fscores))\n",
        "    plt.plot(fscores)\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "History: [0.1615433538265847, 0.2971661110021573, 0.34945882352941177, 0.35037216182582354, 0.35773557071731515, 0.3711793330791828, 0.3744201016625602, 0.3811762638135202, 0.3780558659217877, 0.3754143216248081]\n",
            "Highest f-score: 0.3811762638135202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwMwwo9-MPv4"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9pXRewbyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#expanding to RNN model with context\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "rnn_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
        "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
        "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP-kwoNXUMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "75c38e7c-edd5-4fdc-bef9-50c83f9644d3"
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 561)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 561, 300)          15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 561, 100)          30100     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 561, 37)           3737      \n",
            "=================================================================\n",
            "Total params: 15,034,437\n",
            "Trainable params: 33,837\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIal9meVXnN_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "cc769353-9dc3-43be-f5ab-a5f785e6d9cd"
      },
      "source": [
        "\n",
        "optimizer=Adam(lr=0.01) # define the learning rate\n",
        "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 213s - loss: 0.1146\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Epoch 2/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f4a55247d260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrnn_hist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorized_labels_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevaluation_function\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKNs4t8X3Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCo0xF5kMMbH"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgSYNcerMI9R"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRDxKgLSL_uf"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlG6ZWkIL-HY"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}