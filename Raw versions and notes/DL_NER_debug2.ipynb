{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NER_debug.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTBsYI1tLeVk"
      },
      "source": [
        "# Deep Learning NER task\n",
        "\n",
        "Tatjana Cucic and Sanna Volanen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2GevEzfPP2",
        "colab_type": "text"
      },
      "source": [
        "https://spacy.io/api/annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O5MwAmUALZ4V"
      },
      "source": [
        "# Milestones\n",
        "\n",
        "## 1.1 Predicting word labels independently\n",
        "\n",
        "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
        "* Evaluate the results on token level and entity level. \n",
        "* Report your results with different network hyperparameters. \n",
        "* Also discuss whether the token level accuracy is a reasonable metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Q3HiGQgMU5L",
        "outputId": "5e25713d-6e54-4a34-8a2b-cf91140c58ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Training data: Used for training the model\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
        "\n",
        "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
        "\n",
        "# Test data: Never touched during training / model development, used for evaluating the final model\n",
        "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
        "\n",
        "#saved model\n",
        "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘train.tsv’ already there; not retrieving.\n",
            "\n",
            "File ‘dev.tsv’ already there; not retrieving.\n",
            "\n",
            "File ‘test.tsv’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZhc7b6VFWYX",
        "colab_type": "code",
        "outputId": "a27278f4-a91a-4aef-b29d-8f21b7cd79e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys \n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOOHEYpiMzFp",
        "outputId": "a86d2e56-e4e1-49fb-92bc-2c697d759f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
        "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
        "\n",
        "def read_ontonotes(tsv_file): # \n",
        "    current_sent = [] # list of (word,label) lists\n",
        "    with open(tsv_file) as f:\n",
        "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
        "        for line in tsvreader:\n",
        "            #print(line)\n",
        "            if not line:\n",
        "                if current_sent:\n",
        "                    yield current_sent\n",
        "                    current_sent=[]\n",
        "                continue\n",
        "            current_sent.append(line[0]) \n",
        "        else:\n",
        "            if current_sent:\n",
        "                yield current_sent\n",
        "\n",
        "full_train_data = list(read_ontonotes('train.tsv'))\n",
        "size_tr = int(len(full_train_data)/2)\n",
        "#print(size_tr)\n",
        "##slice train\n",
        "train_data_cut = full_train_data[:size_tr]\n",
        "print(train_data_cut[:30])\n",
        "\n",
        "#print()\n",
        "full_dev_data = list(read_ontonotes('dev.tsv'))\n",
        "size_dv = int(len(full_dev_data)/2)\n",
        "#print(size_dv)\n",
        "#slice dev\n",
        "dev_data_sample = full_dev_data[:size_dv]\n",
        "#print(dev_data_sample[:5])\n",
        "#print()\n",
        "full_test_data = list(read_ontonotes('test.tsv'))\n",
        "size_ts = int(len(full_test_data)/2)\n",
        "#print(size_ts)\n",
        "test_data_sample = full_test_data[:size_ts]\n",
        "\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Big\\tO', 'Managers\\tO', 'on\\tO', 'Campus\\tO'], ['In\\tO', 'recent\\tB-DATE', 'years\\tI-DATE', ',\\tO', 'advanced\\tO', 'education\\tO', 'for\\tO', 'professionals\\tO', 'has\\tO', 'become\\tO', 'a\\tO', 'hot\\tO', 'topic\\tO', 'in\\tO', 'the\\tO', 'business\\tO', 'community\\tO', '.\\tO'], ['With\\tO', 'this\\tO', 'trend\\tO', ',\\tO', 'suddenly\\tO', 'the\\tO', 'mature\\tO', 'faces\\tO', 'of\\tO', 'managers\\tO', 'boasting\\tO', 'an\\tO', 'average\\tO', 'of\\tO', 'over\\tO', 'ten\\tB-DATE', 'years\\tI-DATE', 'of\\tO', 'professional\\tO', 'experience\\tO', 'have\\tO', 'flooded\\tO', 'in\\tO', 'among\\tO', 'the\\tO', 'young\\tO', 'people\\tO', 'populating\\tO', 'university\\tO', 'campuses\\tO', '.\\tO'], ['In\\tO', 'order\\tO', 'to\\tO', 'attract\\tO', 'this\\tO', 'group\\tO', 'of\\tO', 'seasoned\\tO', 'adults\\tO', 'pulling\\tO', 'in\\tO', 'over\\tO', 'NT$\\tB-MONEY', '1\\tI-MONEY', 'million\\tI-MONEY', 'a\\tO', 'year\\tO', 'back\\tO', 'to\\tO', 'the\\tO', 'ivory\\tO', 'tower\\tO', ',\\tO', 'universities\\tO', 'have\\tO', 'begun\\tO', 'to\\tO', 'establish\\tO', 'executive\\tO', 'MBA\\tB-WORK_OF_ART', '(\\tO', 'EMBA\\tB-WORK_OF_ART', ')\\tO', 'programs\\tO', '.\\tO'], ['In\\tO', 'response\\tO', ',\\tO', 'each\\tO', 'year\\tO', 'over\\tO', '1000\\tB-CARDINAL', 'mature\\tO', 'professionals\\tO', 'looking\\tO', 'to\\tO', 'recharge\\tO', 'their\\tO', 'minds\\tO', 'and\\tO', 'retool\\tO', 'their\\tO', 'know\\tO', '-\\tO', 'how\\tO', 'compete\\tO', 'for\\tO', 'a\\tO', 'precious\\tO', 'few\\tO', 'openings\\tO', 'in\\tO', 'executive\\tO', 'degree\\tO', 'programs\\tO', 'at\\tO', 'top\\tO', 'institutions\\tO', 'such\\tO', 'as\\tO', 'National\\tB-ORG', 'Taiwan\\tI-ORG', 'University\\tI-ORG', '(\\tO', 'NTU\\tB-ORG', ')\\tO', 'and\\tO', 'National\\tB-ORG', 'Chengchi\\tI-ORG', 'University\\tI-ORG', '.\\tO'], ['What\\tO', 'brings\\tO', 'these\\tO', 'accomplished\\tO', 'business\\tO', 'professionals\\tO', 'back\\tO', 'to\\tO', 'the\\tO', 'classroom\\tO', '?\\tO'], ['What\\tO', 'knowledge\\tO', 'do\\tO', 'they\\tO', 'seek\\tO', 'that\\tO', 'can\\tO', 'not\\tO', 'be\\tO', 'found\\tO', 'in\\tO', 'society\\tO', 'at\\tO', 'large\\tO', '?\\tO'], ['And\\tO', 'what\\tO', 'effect\\tO', 'does\\tO', 'their\\tO', 'return\\tO', 'have\\tO', 'on\\tO', 'campus\\tO', '?\\tO'], ['Morris\\tB-PERSON', 'Chang\\tI-PERSON', ',\\tO', 'chairman\\tO', 'of\\tO', 'TSMC\\tB-ORG', 'and\\tO', 'a\\tO', 'leading\\tO', 'light\\tO', 'of\\tO', 'Taiwan\\tB-GPE', \"'s\\tO\", 'high\\tO', '-\\tO', 'tech\\tO', 'industry\\tO', ',\\tO', 'is\\tO', 'the\\tO', 'hottest\\tO', 'university\\tO', 'lecturer\\tO', 'around\\tO', '.\\tO'], ['Two\\tB-DATE', 'years\\tI-DATE', 'ago\\tI-DATE', ',\\tO', 'National\\tB-ORG', 'Chiaotung\\tI-ORG', 'University\\tI-ORG', 'invited\\tO', 'Chang\\tB-PERSON', ',\\tO', 'one\\tB-CARDINAL', 'of\\tO', 'the\\tO', 'Taiwan\\tB-GPE', 'IT\\tO', 'sector\\tO', \"'s\\tO\", 'most\\tO', 'prominent\\tO', 'success\\tO', 'stories\\tO', ',\\tO', 'to\\tO', 'lecture\\tO', 'to\\tO', 'students\\tO', 'in\\tO', 'its\\tO', 'MBA\\tB-WORK_OF_ART', 'program\\tO', 'for\\tO', 'top\\tO', 'managers\\tO', ',\\tO', 'single\\tO', '-\\tO', 'handedly\\tO', 'raising\\tO', 'awareness\\tO', 'of\\tO', 'executive\\tO', 'MBA\\tB-WORK_OF_ART', '(\\tO', 'EMBA\\tB-WORK_OF_ART', ')\\tO', 'programs\\tO', '.\\tO'], ['This\\tB-DATE', 'year\\tI-DATE', ',\\tO', 'Acer\\tB-ORG', 'Group\\tI-ORG', 'chairman\\tO', 'Stan\\tB-PERSON', 'Shih\\tI-PERSON', 'will\\tO', 'be\\tO', 'a\\tO', 'guest\\tO', 'lecturer\\tO', 'at\\tO', 'the\\tO', 'university\\tO', '.\\tO'], ['The\\tO', 'crush\\tO', 'of\\tO', 'business\\tO', 'professionals\\tO', 'fighting\\tO', 'for\\tO', 'a\\tO', 'spot\\tO', 'in\\tO', 'the\\tO', 'MBA\\tB-WORK_OF_ART', 'program\\tO', 'at\\tO', 'Taiwan\\tB-GPE', \"'s\\tO\", 'most\\tO', 'illustrious\\tO', 'institution\\tO', 'of\\tO', 'higher\\tO', 'education\\tO', '-\\tB-ORG', 'National\\tI-ORG', 'Taiwan\\tI-ORG', 'University\\tI-ORG', '-\\tI-ORG', 'ensured\\tO', 'that\\tO', 'the\\tO', 'acceptance\\tO', 'rate\\tO', 'for\\tO', 'the\\tO', 'executive\\tO', 'graduate\\tO', 'school\\tO', 'was\\tO', 'even\\tO', 'lower\\tO', 'than\\tO', 'that\\tO', 'for\\tO', 'undergraduates\\tO', 'taking\\tO', 'the\\tO', 'national\\tO', 'university\\tO', 'entrance\\tO', 'examinations\\tO', '.\\tO'], ['Various\\tO', 'indications\\tO', 'demonstrate\\tO', 'that\\tO', 'the\\tO', '\\tO\\nback\\tO\\nflow\\tO\\neducation\\tO\\n\\tO', 'promoted\\tO', 'by\\tO', 'the\\tB-ORG', 'Ministry\\tI-ORG', 'of\\tI-ORG', 'Education\\tI-ORG', 'has\\tO', 'become\\tO', 'the\\tO', 'hottest\\tO', 'investment\\tO', 'undertaking\\tO', 'among\\tO', 'Taiwan\\tB-GPE', \"'s\\tO\", 'universities\\tO', '.\\tO'], ['Three\\tB-DATE', 'years\\tI-DATE', 'ago\\tI-DATE', ',\\tO', 'in\\tO', 'the\\tO', 'effort\\tO', 'to\\tO', 'establish\\tO', 'a\\tO', 'greater\\tO', 'diversity\\tO', 'of\\tO', 'higher\\tO', 'educational\\tO', 'avenues\\tO', ',\\tO', 'the\\tB-ORG', 'Ministry\\tI-ORG', 'of\\tI-ORG', 'Education\\tI-ORG', '(\\tO', 'MOE\\tB-ORG', ')\\tO', 'greatly\\tO', 'increased\\tO', 'the\\tO', 'number\\tO', 'of\\tO', 'MA\\tB-WORK_OF_ART', 'degrees\\tO', 'open\\tO', 'to\\tO', 'professionals\\tO', 'in\\tO', 'the\\tO', 'workforce\\tO', ',\\tO', 'steadily\\tO', 'reducing\\tO', 'the\\tO', 'proportion\\tO', 'of\\tO', 'regular\\tO', 'students\\tO', 'to\\tO', 'professionals\\tO', 'from\\tO', 'the\\tO', 'previous\\tO', '9\\tB-CARDINAL', '-\\tO', 'to\\tO', '-\\tO', '1\\tB-CARDINAL', 'ratio\\tO', '.\\tO'], ['In\\tO', 'addition\\tO', ',\\tO', 'the\\tO', 'MOE\\tB-ORG', 'allowed\\tO', 'universities\\tO', 'to\\tO', 'determine\\tO', 'entrance\\tO', 'examination\\tO', 'topics\\tO', ',\\tO', 'testing\\tO', 'dates\\tO', 'and\\tO', 'times\\tO', ',\\tO', 'and\\tO', 'fee\\tO', 'structures\\tO', 'individually\\tO', 'to\\tO', 'help\\tO', 'them\\tO', 'find\\tO', 'and\\tO', 'obtain\\tO', 'the\\tO', 'best\\tO', 'students\\tO', '.\\tO'], ['Always\\tO', 'in\\tO', 'search\\tO', 'of\\tO', 'revenue\\tO', 'sources\\tO', ',\\tO', 'universities\\tO', 'naturally\\tO', 'fell\\tO', 'right\\tO', 'into\\tO', 'line\\tO', '.\\tO'], ['Twelve\\tB-CARDINAL', 'institutions\\tO', 'opened\\tO', 'programs\\tO', 'in\\tO', '1999\\tB-DATE', ',\\tO', 'with\\tO', 'the\\tO', 'number\\tO', 'of\\tO', 'executive\\tO', 'degree\\tO', 'programs\\tO', 'growing\\tO', 'to\\tO', '34\\tB-CARDINAL', 'as\\tO', 'of\\tO', 'this\\tB-DATE', 'year\\tI-DATE', '.\\tO'], ['And\\tO', 'as\\tO', 'the\\tO', 'supply\\tO', 'has\\tO', 'grown\\tO', ',\\tO', 'professionals\\tO', 'looking\\tO', 'to\\tO', 'recharge\\tO', 'their\\tO', 'intellectual\\tO', 'batteries\\tO', 'in\\tO', 'disciplines\\tO', 'ranging\\tO', 'from\\tO', 'Chinese\\tB-NORP', ',\\tO', 'foreign\\tO', 'languages\\tO', ',\\tO', 'and\\tO', 'physics\\tO', 'to\\tO', 'information\\tO', 'technology\\tO', 'and\\tO', 'management\\tO', 'now\\tO', 'have\\tO', 'a\\tO', 'greater\\tO', 'selection\\tO', 'from\\tO', 'which\\tO', 'to\\tO', 'choose\\tO', '.\\tO'], ['Management\\tO', 'white\\tO', '-\\tO', 'hot\\tO'], ['A\\tO', 'wide\\tO', 'range\\tO', 'of\\tO', 'programs\\tO', 'is\\tO', 'available\\tO', ',\\tO', 'but\\tO', 'popularity\\tO', 'varies\\tO', 'greatly\\tO', '.\\tO'], ['Last\\tB-DATE', 'year\\tI-DATE', ',\\tO', 'National\\tB-ORG', 'Taiwan\\tI-ORG', 'University\\tI-ORG', 'of\\tI-ORG', 'Science\\tI-ORG', 'and\\tI-ORG', 'Technology\\tI-ORG', '(\\tO', 'NTUST\\tB-ORG', ')\\tO', 'opened\\tO', 'the\\tO', 'doors\\tO', 'of\\tO', 'five\\tB-CARDINAL', 'of\\tO', 'its\\tO', 'graduate\\tO', 'schools\\tO', 'to\\tO', 'professionals\\tO', 'in\\tO', 'the\\tO', 'workforce\\tO', '.\\tO'], ['Among\\tO', 'them\\tO', ',\\tO', 'the\\tB-ORG', 'Institute\\tI-ORG', 'of\\tI-ORG', 'Fiber\\tI-ORG', 'planned\\tO', 'to\\tO', 'accept\\tO', '10\\tB-CARDINAL', 'students\\tO', ',\\tO', 'but\\tO', 'only\\tO', '20\\tB-CARDINAL', 'applied\\tO', ',\\tO', 'resulting\\tO', 'in\\tO', 'a\\tO', '50\\tB-PERCENT', '%\\tI-PERCENT', 'acceptance\\tO', 'rate\\tO', ';\\tO', 'meanwhile\\tO', ',\\tO', 'the\\tB-ORG', 'Institute\\tI-ORG', 'of\\tI-ORG', 'Mechanical\\tI-ORG', 'Engineering\\tI-ORG', 'had\\tO', 'only\\tO', '30\\tB-CARDINAL', 'applicants\\tO', 'for\\tO', '10\\tB-CARDINAL', 'spots\\tO', '.\\tO'], ['Similarly\\tO', ',\\tO', 'nine\\tB-CARDINAL', 'graduate\\tO', 'institutes\\tO', 'at\\tO', 'NTU\\tB-ORG', ',\\tO', 'including\\tO', 'mechanical\\tO', 'engineering\\tO', ',\\tO', 'Three\\tB-CARDINAL', 'Principles\\tO', 'of\\tO', 'the\\tO', 'People\\tO', ',\\tO', 'and\\tO', 'biology\\tO', ',\\tO', 'opened\\tO', 'degree\\tO', 'programs\\tO', 'to\\tO', 'professionals\\tO', 'last\\tB-DATE', 'year\\tI-DATE', '.\\tO'], ['Expecting\\tO', 'to\\tO', 'accept\\tO', '167\\tB-CARDINAL', 'students\\tO', ',\\tO', 'they\\tO', 'attracted\\tO', 'a\\tO', 'mere\\tO', '330\\tB-CARDINAL', '-\\tO', 'combined\\tO', 'applicants\\tO', '.\\tO'], ['Compared\\tO', 'to\\tO', 'the\\tO', 'lukewarm\\tO', 'popularity\\tO', 'of\\tO', 'other\\tO', 'graduate\\tO', 'programs\\tO', ',\\tO', 'management\\tO', 'institutes\\tO', 'are\\tO', 'white\\tO', 'hot\\tO', '.\\tO'], ['The\\tO', 'NTUST\\tB-ORG', 'College\\tB-ORG', 'of\\tI-ORG', 'Management\\tI-ORG', 'attracted\\tO', 'nearly\\tO', '500\\tB-CARDINAL', 'applicants\\tO', 'with\\tO', 'over\\tO', '10\\tB-DATE', 'years\\tI-DATE', 'of\\tO', 'work\\tO', 'experience\\tO', 'for\\tO', '20\\tB-CARDINAL', 'spots\\tO', 'in\\tO', 'its\\tO', 'executive\\tO', 'management\\tO', 'program\\tO', 'beginning\\tO', 'in\\tO', '1999\\tB-DATE', '.\\tO'], ['NTU\\tB-ORG', \"'s\\tO\", 'College\\tB-ORG', 'of\\tI-ORG', 'Management\\tI-ORG', 'opened\\tO', 'its\\tO', 'doors\\tO', 'to\\tO', '44\\tB-CARDINAL', 'professionals\\tO', 'on\\tO', 'a\\tO', 'trial\\tO', 'basis\\tO', 'in\\tO', '1996\\tB-DATE', ',\\tO', 'attracting\\tO', 'a\\tO', 'phenomenal\\tO', '990\\tB-CARDINAL', 'applicants\\tO', '.\\tO'], ['When\\tO', '95\\tB-CARDINAL', 'spots\\tO', 'were\\tO', 'offered\\tO', 'the\\tB-DATE', 'following\\tI-DATE', 'year\\tI-DATE', ',\\tO', 'the\\tO', 'number\\tO', 'of\\tO', 'applicants\\tO', 'rose\\tO', 'again\\tO', 'to\\tO', '1400\\tB-CARDINAL', '.\\tO'], ['Currently\\tO', ',\\tO', 'around\\tB-CARDINAL', 'a\\tI-CARDINAL', 'dozen\\tI-CARDINAL', 'public\\tO', 'and\\tO', 'private\\tO', 'universities\\tO', 'in\\tO', 'Taiwan\\tB-GPE', ',\\tO', 'including\\tO', 'NTU\\tB-ORG', ',\\tO', 'National\\tB-ORG', 'Chiaotung\\tI-ORG', 'University\\tI-ORG', ',\\tO', 'National\\tB-ORG', 'Sun\\tI-ORG', 'Yat\\tI-ORG', '-\\tI-ORG', 'sen\\tI-ORG', 'University\\tI-ORG', ',\\tO', 'Da\\tB-ORG', '-\\tI-ORG', 'Yeh\\tI-ORG', 'Institute\\tI-ORG', 'of\\tI-ORG', 'Technology\\tI-ORG', ',\\tO', 'and\\tO', 'Yuan\\tB-ORG', '-\\tI-ORG', 'Ze\\tI-ORG', 'Institute\\tI-ORG', 'of\\tI-ORG', 'Technology\\tI-ORG', ',\\tO', 'offer\\tO', 'advanced\\tO', 'degree\\tO', 'programs\\tO', 'for\\tO', 'executive\\tO', 'management\\tO', 'personnel\\tO', '.\\tO'], ['What\\tO', 'is\\tO', 'it\\tO', 'that\\tO', 'makes\\tO', 'graduate\\tO', 'programs\\tO', 'in\\tO', 'management\\tO', 'so\\tO', 'popular\\tO', 'among\\tO', 'the\\tO', 'working\\tO', 'population\\tO', '?\\tO']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKt7Ef3h0a-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q32R9o_mJZAt",
        "colab_type": "code",
        "outputId": "9829f4b8-02c4-4f58-c7e9-bcf37bf2b03f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "import re\n",
        "from pprint import pprint\n",
        "#regex for empty space chars, \\t \\n\n",
        "#tab = re.compile('[\\t]')\n",
        "#line = re.compile('[\\n')\n",
        "\n",
        "def clean(raw_data): ## input list is list of lists of strings \n",
        "    clean_data =[]  #list of lists that have one clean sentence per list\n",
        "    for sent in raw_data:\n",
        "        clean_sent = []\n",
        "        for item in sent: # item is string\n",
        "            #print('---')\n",
        "            #print(\"item type: \", type(item))\n",
        "            #print(\"item\", item)\n",
        "            one_sentence = []\n",
        "            item_list = item.split(\"\\n\") # if new lines present\n",
        "            if len(item_list)== 1: ## item only has one token and tag separated with \\t\n",
        "                #print(\"item as list\", item_list)\n",
        "                item = item.split(\"\\t\")\n",
        "                one_sentence.append(item)\n",
        "            elif len(item_list) > 1:   ## item has more than one token and tag pair separated with \\t and also \\n NOTE! these turned out to be quotes\n",
        "                print(\"item as list\", item_list)\n",
        "                for subitem in item_list:\n",
        "                    subitem_list = subitem.split(\"\\n\") ## if even the sublist items have items with \\n\n",
        "                    for subsubitem in subitem_list:\n",
        "                        sub = subsubitem.split(\"\\t\")\n",
        "                        #print(\"Splitted subitem: \", sub)\n",
        "                        if sub[0] ==\"\" : # replacing empty token with missing quote marks\n",
        "                            #print(sub[0])\n",
        "                            sub[0] = '\\\"'\n",
        "                            #print(sub)\n",
        "                    else:\n",
        "                        print(\"Subitem type:\", type(subitem))\n",
        "                    sublist.append(sub )\n",
        "                for item in sublist:\n",
        "                    clean_sent.append(item)\n",
        "        clean_data.append(clean_sent)        \n",
        "    return clean_data\n",
        "\n",
        "#train_data_sample = train_data_cut\n",
        "train_data_clean = clean(train_data_cut[:50])\n",
        "#print(len(train_data_clean))\n",
        "#for item in train_data_clean[:3]:\n",
        "    #print(item)\n",
        "item_lengths = []\n",
        "max_text = 0\n",
        "for item in train_data_clean:\n",
        "    #print(item)\n",
        "    item_lengths.append(len(item))\n",
        "    if len(item) > max_text:\n",
        "        max_text = len(item)\n",
        "        ind = train_data_clean.index(item)\n",
        "print(\"Longest sentence:\", max_text, \"index: \",ind)\n",
        "\n",
        "item_lengths_sorted = sorted(item_lengths, reverse=True)\n",
        "max = item_lengths_sorted[0]\n",
        "print(max)\n",
        "#pprint(train_data_clean[21367])\n",
        "pprint(item_lengths_sorted[:100])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item as list ['\\tO', 'back\\tO', 'flow\\tO', 'education\\tO', '\\tO']\n",
            "Subitem type: <class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-eec75a48796e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#train_data_sample = train_data_cut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_data_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_cut\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m#print(len(train_data_clean))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#for item in train_data_clean[:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-121-eec75a48796e>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(raw_data)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Subitem type:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msublist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mclean_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sublist' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk8Brh3sd6dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('------------------------------------------')\n",
        "dev_data_clean = clean(dev_data_sample)\n",
        "print(len(dev_data_clean))\n",
        "for item in dev_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')\n",
        "test_data_clean = clean(test_data_sample)\n",
        "print(len(test_data_clean))\n",
        "for item in test_data_clean[:3]:\n",
        "    print(item)\n",
        "print('------------------------------------------')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZHhXzVTQA_P",
        "outputId": "adf0b7a5-9f9e-4bfa-b7c1-6dd712f08149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# shape into dicts per sentence\n",
        "\n",
        "def reshape_sent2dicts(f):\n",
        "    data_dict = []\n",
        "    for item in f: # list of lists (tokens)\n",
        "        #print(item)\n",
        "        sent_text= [] \n",
        "        sent_tags = []\n",
        "        for token in item:\n",
        "            if len(token) ==2:\n",
        "                sent_text.append(token[0])\n",
        "                sent_tags.append(token[1])\n",
        "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
        "        #print(sent_dict['text'])\n",
        "        #print(sent_dict['tags'])\n",
        "        data_dict.append(sent_dict)\n",
        "    return data_dict\n",
        "\n",
        "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))\n",
        "samp = train_data_sent[:2]\n",
        "print(samp)\n",
        "print()\n",
        "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
        "samp2 = dev_data_sent[:3]\n",
        "print(samp2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'text': ['Big', 'Managers', 'on', 'Campus'], 'tags': ['O', 'O', 'O', 'O']}, {'text': ['In', 'recent', 'years', ',', 'advanced', 'education', 'for', 'professionals', 'has', 'become', 'a', 'hot', 'topic', 'in', 'the', 'business', 'community'], 'tags': ['O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}]\n",
            "\n",
            "[{'text': ['President', 'Chen', 'Travels', 'Abroad'], 'tags': ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']}, {'text': ['(', 'Chang', 'Chiung', '-', 'fang', '/', 'tr.', 'by', 'David', 'Mayer', ')'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O']}, {'text': ['President', 'Chen', 'Shui', '-', 'bian', 'visited', 'the', 'Nicaraguan', 'National', 'Assembly', 'on', 'August', '17', ',', 'where', 'he', 'received', 'a', 'medal', 'from', 'the', 'president', 'of', 'the', 'assembly', ',', 'Ivan', 'Escobar', 'Fornos'], 'tags': ['O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'I-FAC', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOZ42ShXJpdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4557e489-b441-4e94-f712-680c7a97e6ad"
      },
      "source": [
        "item_lengths = []\n",
        "max_text = 0\n",
        "\n",
        "for item in train_data_sent:\n",
        "    item_lengths.append(len(item[\"text\"]))\n",
        "    if len(item[\"text\"]) > max_text:\n",
        "        max_text = len(item[\"text\"])\n",
        "print(\"Longest sentence:\", max_text)\n",
        "\n",
        "item_lengths_sorted = sorted(item_lengths, reverse=True)\n",
        "max = item_lengths_sorted[0]\n",
        "print(max)\n",
        "for i in range(100):\n",
        "    print(item_lengths_sorted[i])\n",
        "for item in train_data_sent:\n",
        "    if len(item[\"text\"]) == 17040:\n",
        "        ind = train_data_sent.index(item)\n",
        "print(ind)\n",
        "\n",
        "max_text = 0\n",
        "for item in train_data_sent:\n",
        "    if len(item[\"text\"]) > max_text:\n",
        "        max_text = len(item[\"text\"])\n",
        "print(\"Longest sentence:\", max_text)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest sentence: 17040\n",
            "17040\n",
            "17040\n",
            "15708\n",
            "11463\n",
            "9411\n",
            "6619\n",
            "6433\n",
            "6053\n",
            "4500\n",
            "4450\n",
            "4277\n",
            "4274\n",
            "3937\n",
            "3851\n",
            "3726\n",
            "3689\n",
            "3670\n",
            "3352\n",
            "3346\n",
            "3012\n",
            "3006\n",
            "2995\n",
            "2754\n",
            "2741\n",
            "2412\n",
            "2393\n",
            "2352\n",
            "2174\n",
            "2151\n",
            "2118\n",
            "1992\n",
            "1942\n",
            "1833\n",
            "1669\n",
            "1619\n",
            "1590\n",
            "1560\n",
            "1537\n",
            "1525\n",
            "1456\n",
            "1358\n",
            "1305\n",
            "1240\n",
            "1232\n",
            "1215\n",
            "1196\n",
            "1185\n",
            "1118\n",
            "1116\n",
            "1091\n",
            "989\n",
            "981\n",
            "941\n",
            "875\n",
            "853\n",
            "834\n",
            "785\n",
            "782\n",
            "759\n",
            "757\n",
            "750\n",
            "741\n",
            "726\n",
            "708\n",
            "704\n",
            "695\n",
            "680\n",
            "660\n",
            "642\n",
            "621\n",
            "619\n",
            "597\n",
            "592\n",
            "547\n",
            "538\n",
            "535\n",
            "530\n",
            "518\n",
            "481\n",
            "442\n",
            "439\n",
            "425\n",
            "410\n",
            "407\n",
            "404\n",
            "394\n",
            "387\n",
            "374\n",
            "361\n",
            "336\n",
            "329\n",
            "323\n",
            "322\n",
            "313\n",
            "305\n",
            "300\n",
            "298\n",
            "297\n",
            "292\n",
            "291\n",
            "273\n",
            "21367\n",
            "Longest sentence: 17040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VpYBQMbcQGBi",
        "outputId": "86da74be-0fcd-4c6e-dcb3-403c1bda6d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "import random\n",
        "import numpy\n",
        "\n",
        "#random.seed(123)\n",
        "#random.shuffle(train_data_sent)\n",
        "#max_sent = [max(len(i[\"text\"])) for i in train_data_sent]\n",
        "#print(max_sent)\n",
        "print(type(train_data_sent))\n",
        "print(train_data_sent[0]) ##one dict\n",
        "print()\n",
        "print(train_data_sent[0][\"text\"])\n",
        "print()\n",
        "print(train_data_sent[0][\"tags\"])\n",
        "print('------------')\n",
        "\n",
        "def typed_listing(data, key):\n",
        "    listed = []\n",
        "    max_length = 0\n",
        "    for item in data: # dictionary {text:\"\", tags:\"\"}\n",
        "        #print('Item: ', item)\n",
        "        #print('Key: ', key, ' content: ', item[key], 'length: ',len(item[key]))\n",
        "        if len(item[key]) > max_length:\n",
        "            max = len(item[key])\n",
        "        listed.append(item[key])\n",
        "    return listed, max_length\n",
        "\n",
        "listed_texts= typed_listing(train_data_sent, \"text\")\n",
        "train_texts = listed_texts[0]\n",
        "train_txt_max = listed_texts[1]\n",
        "listed_labels = typed_listing(train_data_sent, \"tags\")\n",
        "train_labels= listed_labels[0]\n",
        "train_lbl_max = listed_labels[1]\n",
        "print(train_txt_max)\n",
        "print(train_texts[0])\n",
        "print(train_labels[0])\n",
        "\n",
        "\n",
        "print('-----------------------------')\n",
        "print(len(train_texts))\n",
        "print('-----------------------')\n",
        "print('Text: ', train_texts[0])\n",
        "print(' Texts length: ',len(train_texts))\n",
        "print('Label: ', train_labels[0])\n",
        "print(' Labels length: ',len(train_labels))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b1af84c83387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#random.seed(123)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#random.shuffle(train_data_sent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmax_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-b1af84c83387>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#random.seed(123)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#random.shuffle(train_data_sent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmax_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUptkZ6SLe3t",
        "colab_type": "code",
        "outputId": "d93f413a-0e52-4197-abbd-18bad8a1316e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest sentence: 17040\n",
            "Longest labels: 17040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQQRw0YO-Ng",
        "colab_type": "code",
        "outputId": "e64965a8-0396-4e06-9713-795da2e66a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "## same for validation/dev data\n",
        "listed_texts= typed_listing(dev_data_sent, \"text\")\n",
        "dev_texts = listed_texts[0]\n",
        "dev_txt_max = listed_texts[1]\n",
        "listed_labels = typed_listing(dev_data_sent, \"tags\")\n",
        "dev_labels= listed_labels[0]\n",
        "dev_lbl_max = listed_labels[1]\n",
        "print('Text: ', dev_texts[0])\n",
        "print(' Texts length: ',len(dev_texts))\n",
        "print('Label: ', dev_labels[0])\n",
        "print(' Labels length: ',len(dev_labels))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  ['President', 'Chen', 'Travels', 'Abroad']\n",
            " Texts length:  5806\n",
            "Label:  ['B-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART', 'I-WORK_OF_ART']\n",
            " Labels length:  5806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICn08fOgbyXl",
        "colab_type": "code",
        "outputId": "b17387bd-598e-4c94-ca3c-babe5d62d3c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Load pretrained embeddings\n",
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-13 17:43:34--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  11.1MB/s    in 61s     \n",
            "\n",
            "2020-05-13 17:44:37 (10.6 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFv2qclTbyXp",
        "colab_type": "code",
        "outputId": "5755c7c4-5535-4a9b-83a7-d783dc68ae84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Give -n argument so that a possible existing file isn't overwritten \n",
        "!unzip -n wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_C5ii8byXt",
        "colab_type": "code",
        "outputId": "d131df30-feb1-4c97-bfff-ee03bdc74aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
        "\n",
        "\n",
        "# sort based on the index to make sure they are in the correct order\n",
        "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
        "print(\"Words from embedding model:\", len(words))\n",
        "print(\"First 50 words:\", words[:50])\n",
        "\n",
        "# Normalize the vectors to unit length\n",
        "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
        "vector_model.init_sims(replace=True)\n",
        "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Words from embedding model: 50000\n",
            "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
            "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
            " -0.0063]\n",
            "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
            "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkdgjgOlbyXx",
        "colab_type": "code",
        "outputId": "db23b1bd-dd3f-4359-e311-fea50781dc4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Build vocabulary mappings\n",
        "\n",
        "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
        "# Also reserve an index for out-of-vocabulary items.\n",
        "vocabulary={\n",
        "    \"<PAD>\": 0,\n",
        "    \"<OOV>\": 1\n",
        "}\n",
        "\n",
        "for word in words: # These are words from the word2vec model\n",
        "    vocabulary.setdefault(word, len(vocabulary))\n",
        "\n",
        "print(\"Words in vocabulary:\",len(vocabulary))\n",
        "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
        "\n",
        "\n",
        "# Embedding matrix\n",
        "def load_pretrained_embeddings(vocab, embedding_model):\n",
        "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
        "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
        "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
        "    found=0\n",
        "    for word,idx in vocab.items():\n",
        "        if word in embedding_model.vocab:\n",
        "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
        "            found+=1\n",
        "            \n",
        "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
        "    return pretrained_embeddings\n",
        "\n",
        "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words in vocabulary: 50002\n",
            "Found pretrained vectors for 50000 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGaojUBhbyX2",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Ox5_ObyX3",
        "colab_type": "code",
        "outputId": "6e35e02b-e892-429c-fe62-1bf54b250334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#Labels\n",
        "\n",
        "\n",
        "not_letter = re.compile(r'[^a-zA-Z]')\n",
        "# Label mappings\n",
        "# 1) gather a set of unique labels\n",
        "label_set = set()\n",
        "for sentence_labels in train_labels: #loops over sentences \n",
        "    #print(sentence_labels)\n",
        "    for label in sentence_labels: #loops over labels in one sentence\n",
        "       # match = not_letter.match(label)\n",
        "        #if match or label== 'O':\n",
        "        #    break\n",
        "        #else:    \n",
        "        label_set.add(label)\n",
        "\n",
        "# 2) index these\n",
        "label_map = {}\n",
        "for index, label in enumerate(label_set):\n",
        "    label_map[label]=index\n",
        "    \n",
        "pprint(label_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'B-CARDINAL': 0,\n",
            " 'B-DATE': 31,\n",
            " 'B-EVENT': 11,\n",
            " 'B-FAC': 33,\n",
            " 'B-GPE': 6,\n",
            " 'B-LANGUAGE': 25,\n",
            " 'B-LAW': 20,\n",
            " 'B-LOC': 29,\n",
            " 'B-MONEY': 19,\n",
            " 'B-NORP': 23,\n",
            " 'B-ORDINAL': 3,\n",
            " 'B-ORG': 24,\n",
            " 'B-PERCENT': 36,\n",
            " 'B-PERSON': 30,\n",
            " 'B-PRODUCT': 13,\n",
            " 'B-QUANTITY': 35,\n",
            " 'B-TIME': 22,\n",
            " 'B-WORK_OF_ART': 4,\n",
            " 'I-CARDINAL': 1,\n",
            " 'I-DATE': 18,\n",
            " 'I-EVENT': 8,\n",
            " 'I-FAC': 26,\n",
            " 'I-GPE': 5,\n",
            " 'I-LANGUAGE': 15,\n",
            " 'I-LAW': 34,\n",
            " 'I-LOC': 10,\n",
            " 'I-MONEY': 9,\n",
            " 'I-NORP': 16,\n",
            " 'I-ORDINAL': 28,\n",
            " 'I-ORG': 2,\n",
            " 'I-PERCENT': 7,\n",
            " 'I-PERSON': 17,\n",
            " 'I-PRODUCT': 32,\n",
            " 'I-QUANTITY': 12,\n",
            " 'I-TIME': 14,\n",
            " 'I-WORK_OF_ART': 27,\n",
            " 'O': 21}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k8DshceEaI",
        "colab_type": "code",
        "outputId": "168f9e68-4877-4a52-c843-3ac76738605d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vectorize the labels\n",
        "def label_vectorizer(train_labels,label_map):\n",
        "    vectorized_labels = []\n",
        "    for label in train_labels:\n",
        "        vectorized_example_label = []\n",
        "        for token in label:\n",
        "            if token in label_map:\n",
        "                vectorized_example_label.append(label_map[token])\n",
        "        vectorized_labels.append(vectorized_example_label)\n",
        "    vectorized_labels = numpy.array(vectorized_labels)\n",
        "    return vectorized_labels\n",
        "        \n",
        "\n",
        "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
        "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
        "\n",
        "print(vectorized_labels[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUtqLdCMPf3X",
        "colab_type": "code",
        "outputId": "7e63d28e-8c44-44bc-bc1a-4687ffe0c9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "## vectorization of the texts\n",
        "def text_vectorizer(vocab, train_texts):\n",
        "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
        "    sentence_lengths = [] # Number of tokens in each sentence\n",
        "    \n",
        "    for i, one_example in enumerate(train_texts):\n",
        "        vectorized_example = []\n",
        "        for word in one_example:\n",
        "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
        "\n",
        "        vectorized_data.append(vectorized_example)     \n",
        "        sentence_lengths.append(len(one_example))\n",
        "        \n",
        "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
        "    \n",
        "    return vectorized_data, sentence_lengths\n",
        "\n",
        "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
        "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
        "\n",
        "print(train_texts[0])\n",
        "print(vectorized_data[0])\n",
        "pprint(type(lengths))\n",
        "#max = lengths.index(17040)\n",
        "#print(max)\n",
        "#pprint(train_texts[11103])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['wrote', ':', 'I', \"'ll\", 'tell', 'a', 'tale', 'so', 'moving', 'that', 'it', \"'s\", 'sure', 'to', 'make', 'you', 'snivel', ',', 'A', 'turkey', 'learned', 'to', 'peck', 'the', 'keys', 'and', 'post', 'a', 'pile', 'of', 'drivel', ',', 'How', 'long', 'have', 'you', 'known', 'you', 'are', 'a', 'Turkey', '?']\n",
            "[789, 11, 30, 1796, 1367, 9, 7233, 59, 1238, 13, 21, 26, 584, 7, 140, 32, 1, 2, 106, 12094, 2533, 7, 1, 3, 5824, 5, 699, 9, 11297, 6, 21599, 2, 979, 389, 38, 32, 456, 32, 34, 9, 1959, 39]\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6FH5F1QGrq",
        "colab_type": "code",
        "outputId": "3e1d6d3e-1a7f-4e62-b386-2d11081dcf61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "# padding for tensor\n",
        "import tensorflow as tf\n",
        "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "#set_session(tf.Session(config=config))\n",
        "### ---end of weird stuff\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Old shape:\", vectorized_data.shape)\n",
        "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max(lengths))\n",
        "print(\"New shape:\", vectorized_data_padded.shape)\n",
        "print(\"First example:\")\n",
        "print( vectorized_data_padded[0])\n",
        "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
        "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max(lengths)), -1)\n",
        "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
        "pprint(label_map)\n",
        "print(\"First example labels:\")\n",
        "pprint(vectorized_labels_padded[0])\n",
        "\n",
        "weights = numpy.copy(vectorized_data_padded)\n",
        "weights[weights > 0] = 1\n",
        "print(\"First weight vector:\")\n",
        "print( weights[0])\n",
        "\n",
        "# Same stuff for the validation data\n",
        "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max(lengths))\n",
        "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max(lengths)), -1)\n",
        "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
        "validation_weights[validation_weights > 0] = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old shape: (30000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-09cde135f285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Old shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mvectorized_data_padded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"New shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_data_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First example:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhAOVAbBTRAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation function\n",
        "import keras\n",
        "\n",
        "def _convert_to_entities(input_sequence):\n",
        "    \"\"\"\n",
        "    Reads a sequence of tags and converts them into a set of entities.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "    current_entity = []\n",
        "    previous_tag = label_map['O']\n",
        "    for i, tag in enumerate(input_sequence):\n",
        "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "            current_entity.append((tag, i))\n",
        "        elif tag == label_map['O']: # Entity has ended\n",
        "            if len(current_entity) > 0:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = []\n",
        "        elif tag == previous_tag: # Current entity continues\n",
        "            current_entity.append((tag, i))\n",
        "        previous_tag = tag\n",
        "    \n",
        "    # Add the last entity to our entity list if the sentences ends with an entity\n",
        "    if len(current_entity) > 0:\n",
        "        entities.append(current_entity)\n",
        "    \n",
        "    entity_offsets = set()\n",
        "    \n",
        "    for e in entities:\n",
        "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
        "    return entity_offsets\n",
        "\n",
        "def _entity_level_PRF(predictions, gold, lengths):\n",
        "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
        "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
        "    \n",
        "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
        "    pred_count = sum([len(e) for e in pred_entities])\n",
        "    \n",
        "    try:\n",
        "        precision = tp / pred_count # tp / (tp+np)\n",
        "        recall = tp / sum([len(e) for e in gold_entities])\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    except Exception as e:\n",
        "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
        "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
        "    return precision, recall, fscore             \n",
        "\n",
        "def evaluate(predictions, gold, lengths):\n",
        "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
        "    return precision, recall, fscore\n",
        "\n",
        "class EvaluateEntities(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.precision = []\n",
        "        self.recall = []\n",
        "        self.fscore = []\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
        "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
        "        self.precision.append(evaluation_parameters[0])\n",
        "        self.recall.append(evaluation_parameters[1])\n",
        "        self.fscore.append(evaluation_parameters[2])\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VdAreQm52JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_EL = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVS-GKkQUpue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model 3 KEEP!\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
        "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "hidden_size = 50\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
        "    hidden = TimeDistributed(Dense(hidden_size, activation=\"sigmoid\"))(embeddings) # We change this activation function\n",
        "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "model3 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj3x13FELJLt",
        "colab_type": "code",
        "outputId": "6d115693-97fa-424e-b0df-d4ec7ee21c24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print(model3.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 17040)             0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 17040, 300)        15000600  \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 17040, 50)         15050     \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 17040, 37)         1887      \n",
            "=================================================================\n",
            "Total params: 15,017,537\n",
            "Trainable params: 16,937\n",
            "Non-trainable params: 15,000,600\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuWSrGPVUw9Z",
        "colab_type": "code",
        "outputId": "63cb737d-ec18-4b84-e1b8-0d877d8c6ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "# train the model 3 KEEP!!\n",
        "optimizer=Adadelta(lr=0.01) # define the learning rate\n",
        "model3.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "vanilla_hist=model3.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 101s - loss: 0.0045\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Epoch 2/10\n",
            " - 100s - loss: 0.0045\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Epoch 3/10\n",
            " - 100s - loss: 0.0044\n",
            "\n",
            "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
            "Epoch 4/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-127c3ccc2cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvanilla_hist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectorized_labels_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevaluation_function\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecFC_-OhC1lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the f scores\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(fscores):\n",
        "    print(\"History:\", fscores)\n",
        "    print(\"Highest f-score:\", max(fscores))\n",
        "    plt.plot(fscores)\n",
        "    plt.legend(loc='lower center', borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cwMwwo9-MPv4"
      },
      "source": [
        "## 1.2 Expand context\n",
        "\n",
        "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9pXRewbyX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#expanding to RNN model with context\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "example_count, sequence_len = vectorized_data_padded.shape\n",
        "class_count = len(label_set)\n",
        "rnn_size = 100\n",
        "\n",
        "vector_size= pretrained.shape[1]\n",
        "\n",
        "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
        "    inp=Input(shape=(sequence_len,))\n",
        "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
        "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
        "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
        "    return Model(inputs=[inp], outputs=[outp])\n",
        "\n",
        "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP-kwoNXUMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIal9meVXnN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer=Adam(lr=0.01) # define the learning rate\n",
        "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
        "\n",
        "evaluation_function=EvaluateEntities()\n",
        "\n",
        "# train\n",
        "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRKNs4t8X3Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plot_history(evaluation_function.fscore)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCo0xF5kMMbH"
      },
      "source": [
        "## 2.1 Use deep contextual representations\n",
        "\n",
        "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sgSYNcerMI9R"
      },
      "source": [
        "## 2.2 Error analysis\n",
        "\n",
        "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRDxKgLSL_uf"
      },
      "source": [
        "## 3.1 Predictions on unannotated text\n",
        "\n",
        "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlG6ZWkIL-HY"
      },
      "source": [
        "## 3.2 Statistically analyze the results\n",
        "\n",
        "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
      ]
    }
  ]
}