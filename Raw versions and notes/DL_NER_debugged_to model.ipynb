{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTBsYI1tLeVk"
   },
   "source": [
    "# Deep Learning NER task\n",
    "\n",
    "Tatjana Cucic and Sanna Volanen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9T2GevEzfPP2"
   },
   "source": [
    "https://spacy.io/api/annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5MwAmUALZ4V"
   },
   "source": [
    "# Milestones\n",
    "\n",
    "## 1.1 Predicting word labels independently\n",
    "\n",
    "* The first part is to train a classifier which assigns a label for each given input word independently. \n",
    "* Evaluate the results on token level and entity level. \n",
    "* Report your results with different network hyperparameters. \n",
    "* Also discuss whether the token level accuracy is a reasonable metric.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "0Q3HiGQgMU5L",
    "outputId": "6aa4cfdc-f214-455a-d41a-0dde30e4ed71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Training data: Used for training the model\n",
    "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/train.tsv\n",
    "\n",
    "# Development/ validation data: Used for testing different model parameters, for example level of regularization needed\n",
    "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/dev.tsv\n",
    "\n",
    "# Test data: Never touched during training / model development, used for evaluating the final model\n",
    "!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/data/test.tsv\n",
    "\n",
    "#saved model\n",
    "#!wget -nc https://raw.githubusercontent.com/sainvo/DeepLearning_NER/master/saved_models/Adamax90.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FZhc7b6VFWYX",
    "outputId": "862727bf-32a4-432a-a348-69baba3fcef2"
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4be31588033b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfield_size_limit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOOHEYpiMzFp"
   },
   "outputs": [],
   "source": [
    "#read tsv data to list of lists of lists: a list of sentences that contain lists of tokens that are lists of unsplit \\t lines from the tsv, such as ['attract\\tO']\n",
    "token = {\"word\":\"\",\"entity_label\":\"\"}\n",
    "\n",
    "def read_ontonotes(tsv_file): # \n",
    "    current_sent = [] # list of (word,label) lists\n",
    "    with open(tsv_file) as f:\n",
    "        tsvreader = csv.reader(f, delimiter= '\\n')\n",
    "        for line in tsvreader:\n",
    "            #print(line)\n",
    "            if not line:\n",
    "                if current_sent:\n",
    "                    yield current_sent\n",
    "                    current_sent=[]\n",
    "                continue\n",
    "            current_sent.append(line[0]) \n",
    "        else:\n",
    "            if current_sent:\n",
    "                yield current_sent\n",
    "\n",
    "train_data_full = list(read_ontonotes('train.tsv'))\n",
    "dev_data_full = list(read_ontonotes('dev.tsv'))\n",
    "test_data_full = list(read_ontonotes('test.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "q32R9o_mJZAt",
    "outputId": "456f0e4d-bd31-4f39-e61d-63c2cbf65d29"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "#regex for empty space chars, \\t \\n\n",
    "#tab = re.compile('[\\t]')\n",
    "#line = re.compile('[\\n]')\n",
    "punct = re.compile('[.?!:;]')\n",
    "\n",
    "def splitter(sent):\n",
    "    #print('----------------------------------------')\n",
    "    #print(\"one sentence in raw data:\", sent)\n",
    "    split_list = []\n",
    "    # loop over tokens items inside sentence, supposedly item= token+ \\t +tag\n",
    "    for item in sent: \n",
    "        #print(\"Item in sentence: \", item)\n",
    "        if item != None:\n",
    "            match1 = item.count('\\n')\n",
    "            #print(match1)\n",
    "            match2 = item.count('\\t')\n",
    "            #print(match2)\n",
    "            if match1 ==0: # no new lines nested\n",
    "                if match2 == 1: #just one tab inside token\n",
    "                    item_pair = item.split('\\t')\n",
    "                if item_pair[0] =='': # replacing empty string with missing quote marks\n",
    "                    item_pair[0] = '\\\"'\n",
    "                split_list.append(item_pair) \n",
    "            else:\n",
    "                subitems_list = item.split('\\n') ## check if token has \\n -> bundled, quotes\n",
    "                if len(subitems_list) > 1:  ## item string has more than one sentence nested in it\n",
    "                    #print(\"Found nested sentences: \", subitems_list)\n",
    "                    #print(\"subseq start\")\n",
    "                    for j in range(len(subitems_list)):  \n",
    "                        token = subitems_list[j]  \n",
    "                        #print(token)\n",
    "                        subtoken_listed_again = token.split('\\n') \n",
    "                        for token in subtoken_listed_again:\n",
    "                            match1=token.count('\\n')\n",
    "                            match2=token.count('\\t')\n",
    "                            if  match1 == 0: # no new lines nested\n",
    "                               if  match2 == 1: #just one tab inside token\n",
    "                                    token = token.split('\\t')\n",
    "                            if token =='': # replacing empty string with missing quote marks\n",
    "                                token = '\\\"'\n",
    "                            if token == '.':\n",
    "                                split_list.append(token)\n",
    "                                continue\n",
    "                                split_list=[]\n",
    "                            else:\n",
    "                                split_list.append(token)\n",
    "                    #print(\"subseq end\")\n",
    "    for item in split_list:\n",
    "        #print(\"Item in split list: \",item)\n",
    "        if type(item) != list:\n",
    "            split_list.remove(item)\n",
    "        if item[0] =='': # replacing empty string with missing quote marks\n",
    "            item[0] = '\\\"'\n",
    "    #print(\"Resplitted sentence :\", split_list)\n",
    "    return split_list\n",
    "\n",
    "def clean(raw_data): ## input list is list of lists of strings \n",
    "    clean_data =[]  #list of lists that have one clean sentence per list\n",
    "    for sent in raw_data: # split by [] lines, supposedly a sentence line\n",
    "        one_sentence = [] #collects the new sentence if there has been need to resplit items\n",
    "        splitted= splitter(sent)\n",
    "        for item in splitted:\n",
    "            #print(item)\n",
    "            matchi = re.match(punct, item[0])\n",
    "            if matchi:\n",
    "                #print(\"collected sentence\")\n",
    "                one_sentence.append(item)\n",
    "                clean_data.append(one_sentence)\n",
    "                one_sentence=[]\n",
    "                break\n",
    "            else:\n",
    "                one_sentence.append(item)\n",
    "\n",
    "    return clean_data\n",
    "\n",
    "train_data_clean = clean(train_data_full)\n",
    "print(len(train_data_clean))\n",
    "for item in train_data_clean[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "jQrs8MSroPfa",
    "outputId": "6fa023af-ea39-4540-ac3e-083a85e6f1d5"
   },
   "outputs": [],
   "source": [
    "# final check on the sentences\n",
    "item_lengths = []\n",
    "max_text = 0\n",
    "for item in train_data_clean:\n",
    "    item_lengths.append(len(item))\n",
    "    if len(item) > max_text:\n",
    "        max_text = len(item)\n",
    "        ind = train_data_clean.index(item)\n",
    "print(\"Longest sentence:\", max_text, \"index: \",ind)\n",
    "\n",
    "lengths_sorted = sorted(item_lengths, reverse=True)\n",
    "max = item_lengths.index(max_text)\n",
    "#print(items_sorted[0])\n",
    "#pprint(train_data_clean[max])\n",
    "print(lengths_sorted[:300]) # longest sentences\n",
    "# checking long items\n",
    "#for item in train_data_clean:\n",
    "    #if len(item) == 123:\n",
    "        #pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "tk8Brh3sd6dl",
    "outputId": "21341a1a-f969-4fca-dda8-046c2d75306c"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('------------------------------------------')\n",
    "dev_data_clean = clean(dev_data_full)\n",
    "print(len(dev_data_clean))\n",
    "for item in dev_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')\n",
    "test_data_clean = clean(test_data_full)\n",
    "print(len(test_data_clean))\n",
    "for item in test_data_clean[:3]:\n",
    "    print(item)\n",
    "print('------------------------------------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "cZHhXzVTQA_P",
    "outputId": "aa4715b9-a966-490a-d09e-6ff8c947ba92"
   },
   "outputs": [],
   "source": [
    "# shape into dicts per sentence\n",
    "\n",
    "def reshape_sent2dicts(f):\n",
    "    data_dict = []\n",
    "    for item in f: # list of lists (tokens)\n",
    "        #print(item)\n",
    "        sent_text= [] \n",
    "        sent_tags = []\n",
    "        for token in item:\n",
    "            if len(token) ==2:\n",
    "                sent_text.append(token[0])\n",
    "                sent_tags.append(token[1])\n",
    "        sent_dict = {'text':sent_text,'tags':sent_tags }\n",
    "        #print(sent_dict['text'])\n",
    "        #print(sent_dict['tags'])\n",
    "        data_dict.append(sent_dict)\n",
    "    return data_dict\n",
    "\n",
    "train_data_sent = list(reshape_sent2dicts(train_data_clean[:30000]))\n",
    "samp = train_data_sent[:2]\n",
    "print(samp)\n",
    "print()\n",
    "dev_data_sent = list(reshape_sent2dicts(dev_data_clean))\n",
    "samp2 = dev_data_sent[:3]\n",
    "print(samp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "VpYBQMbcQGBi",
    "outputId": "c5664322-a7b5-4b7f-f7ce-7e30f55041d1"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "random.seed(124)\n",
    "random.shuffle(train_data_sent)\n",
    "#max_sent = [max(len(i[\"text\"])) for i in train_data_sent]\n",
    "#print(max_sent)\n",
    "print(type(train_data_sent))\n",
    "print(train_data_sent[0]) ##one dict\n",
    "print()\n",
    "print(train_data_sent[0][\"text\"])\n",
    "print()\n",
    "print(train_data_sent[0][\"tags\"])\n",
    "print('------------')\n",
    "\n",
    "def typed_listing(data, key):\n",
    "    listed = []\n",
    "    max_length = 0\n",
    "    for item in data: # dictionary {text:\"\", tags:\"\"}\n",
    "        #print('Item: ', item)\n",
    "        #print('Key: ', key, ' content: ', item[key], 'length: ',len(item[key]))\n",
    "        if len(item[key]) > max_length:\n",
    "            max = len(item[key])\n",
    "        listed.append(item[key])\n",
    "    return listed, max_length\n",
    "\n",
    "listed_texts= typed_listing(train_data_sent, \"text\")\n",
    "train_texts = listed_texts[0]\n",
    "train_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(train_data_sent, \"tags\")\n",
    "train_labels= listed_labels[0]\n",
    "train_lbl_max = listed_labels[1]\n",
    "print(train_txt_max)\n",
    "print(train_texts[0])\n",
    "print(train_labels[0])\n",
    "\n",
    "\n",
    "print('-----------------------------')\n",
    "print(len(train_texts))\n",
    "print('-----------------------')\n",
    "print('Text: ', train_texts[0])\n",
    "print(' Texts length: ',len(train_texts))\n",
    "print('Label: ', train_labels[0])\n",
    "print(' Labels length: ',len(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "DNQQRw0YO-Ng",
    "outputId": "8a30c1c9-b13e-4de6-84b0-b4839126e6e3"
   },
   "outputs": [],
   "source": [
    "## same for validation/dev data\n",
    "listed_texts= typed_listing(dev_data_sent, \"text\")\n",
    "dev_texts = listed_texts[0]\n",
    "dev_txt_max = listed_texts[1]\n",
    "listed_labels = typed_listing(dev_data_sent, \"tags\")\n",
    "dev_labels= listed_labels[0]\n",
    "dev_lbl_max = listed_labels[1]\n",
    "print('Text: ', dev_texts[0])\n",
    "print(' Texts length: ',len(dev_texts))\n",
    "print('Label: ', dev_labels[0])\n",
    "print(' Labels length: ',len(dev_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ICn08fOgbyXl",
    "outputId": "a44dc3a3-aa42-42b2-9651-24d10bbd13ce"
   },
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bFv2qclTbyXp",
    "outputId": "3a495d72-ee6b-4408-c59e-d7a8953cbd50"
   },
   "outputs": [],
   "source": [
    "# Give -n argument so that a possible existing file isn't overwritten \n",
    "!unzip -n wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "kx_C5ii8byXt",
    "outputId": "24b00d9a-34ab-4d56-cfc1-13f0b6c703dd"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
    "print(\"Words from embedding model:\", len(words))\n",
    "print(\"First 50 words:\", words[:50])\n",
    "\n",
    "# Normalize the vectors to unit length\n",
    "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NkdgjgOlbyXx",
    "outputId": "f4529fc8-e6e1-4b06-bfdc-101155f04f54"
   },
   "outputs": [],
   "source": [
    "# Build vocabulary mappings\n",
    "\n",
    "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
    "# Also reserve an index for out-of-vocabulary items.\n",
    "vocabulary={\n",
    "    \"<PAD>\": 0,\n",
    "    \"<OOV>\": 1\n",
    "}\n",
    "\n",
    "for word in words: # These are words from the word2vec model\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
    "\n",
    "\n",
    "# Embedding matrix\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGaojUBhbyX2"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "_M9Ox5_ObyX3",
    "outputId": "ff631650-a6c0-4115-b43f-dc9d1509d9b9"
   },
   "outputs": [],
   "source": [
    "#Labels\n",
    "\n",
    "\n",
    "not_letter = re.compile(r'[^a-zA-Z]')\n",
    "# Label mappings\n",
    "# 1) gather a set of unique labels\n",
    "label_set = set()\n",
    "for sentence_labels in train_labels: #loops over sentences \n",
    "    #print(sentence_labels)\n",
    "    for label in sentence_labels: #loops over labels in one sentence\n",
    "       # match = not_letter.match(label)\n",
    "        #if match or label== 'O':\n",
    "        #    break\n",
    "        #else:    \n",
    "        label_set.add(label)\n",
    "\n",
    "# 2) index these\n",
    "label_map = {}\n",
    "for index, label in enumerate(label_set):\n",
    "    label_map[label]=index\n",
    "    \n",
    "pprint(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r8k8DshceEaI",
    "outputId": "43247506-c5ef-4a0e-fe6b-ef2ef10344a3"
   },
   "outputs": [],
   "source": [
    "# vectorize the labels\n",
    "def label_vectorizer(train_labels,label_map):\n",
    "    vectorized_labels = []\n",
    "    for label in train_labels:\n",
    "        vectorized_example_label = []\n",
    "        for token in label:\n",
    "            if token in label_map:\n",
    "                vectorized_example_label.append(label_map[token])\n",
    "        vectorized_labels.append(vectorized_example_label)\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    return vectorized_labels\n",
    "        \n",
    "\n",
    "vectorized_labels = label_vectorizer(train_labels,label_map)\n",
    "validation_vectorized_labels = label_vectorizer(dev_labels,label_map)\n",
    "\n",
    "print(vectorized_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "YUtqLdCMPf3X",
    "outputId": "7b52b22b-21ba-4a0c-bb96-eac08038d1ff"
   },
   "outputs": [],
   "source": [
    "## vectorization of the texts\n",
    "def text_vectorizer(vocab, train_texts):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(train_texts):\n",
    "        vectorized_example = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "\n",
    "        vectorized_data.append(vectorized_example)     \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
    "    \n",
    "    return vectorized_data, sentence_lengths\n",
    "\n",
    "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
    "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, dev_texts)\n",
    "\n",
    "print(train_texts[0])\n",
    "print(vectorized_data[0])\n",
    "print(type(lengths))\n",
    "print(type(lengths[0]))\n",
    "\n",
    "#max = lengths.index(17040)\n",
    "#print(max)\n",
    "#pprint(train_texts[11103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9e6FH5F1QGrq",
    "outputId": "473a1083-4f61-49b2-e414-129f6623610e"
   },
   "outputs": [],
   "source": [
    "# padding for tensor\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "### Only needed for me, not to block the whole GPU, you don't need this stuff\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "arr_lengths = numpy.array(lengths)\n",
    "max_len = numpy.max(arr_lengths)\n",
    "print(max_len)\n",
    "\n",
    "\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='pre', maxlen=max_len)\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\")\n",
    "print( vectorized_data_padded[0])\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='pre', maxlen=max_len), -1)\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "pprint(label_map)\n",
    "print(\"First example labels:\")\n",
    "pprint(vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\")\n",
    "print( weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='pre', maxlen=max_len)\n",
    "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='pre',maxlen=max_len), -1)\n",
    "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhAOVAbBTRAv"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "import keras\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    return entity_offsets\n",
    "\n",
    "def _entity_level_PRF(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    \n",
    "    try:\n",
    "        precision = tp / pred_count # tp / (tp+np)\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "    return precision, recall, fscore             \n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.fscore = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
    "        self.precision.append(evaluation_parameters[0])\n",
    "        self.recall.append(evaluation_parameters[1])\n",
    "        self.fscore.append(evaluation_parameters[2])\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVS-GKkQUpue"
   },
   "outputs": [],
   "source": [
    "# model 3 KEEP!\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, Adamax, Adadelta, Adagrad, Nadam \n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 100\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "model3 = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "gj3x13FELJLt",
    "outputId": "e8309096-8b92-4a94-e933-1ca67f0c3bf3"
   },
   "outputs": [],
   "source": [
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "QuWSrGPVUw9Z",
    "outputId": "4731be8c-199b-49a1-857f-7d70b38b549d"
   },
   "outputs": [],
   "source": [
    "# train the model 3 KEEP!!\n",
    "optimizer=Adamax(lr=0.05) # define the learning rate\n",
    "model3.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "vanilla_hist=model3.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "ecFC_-OhC1lN",
    "outputId": "c3c29646-4b4e-4017-f084-cf6b99265c26"
   },
   "outputs": [],
   "source": [
    "# plot the f scores\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(fscores):\n",
    "    print(\"History:\", fscores)\n",
    "    print(\"Highest f-score:\", max(fscores))\n",
    "    plt.plot(fscores)\n",
    "    plt.legend(loc='lower center', borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VdAreQm52JW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_EL = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cwMwwo9-MPv4"
   },
   "source": [
    "## 1.2 Expand context\n",
    "\n",
    "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lw9pXRewbyX6"
   },
   "outputs": [],
   "source": [
    "#expanding to RNN model with context\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "rnn_size = 100\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
    "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPP-kwoNXUMb"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIal9meVXnN_"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer=Adam(lr=0.01) # define the learning rate\n",
    "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRKNs4t8X3Ed"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sCo0xF5kMMbH"
   },
   "source": [
    "## 2.1 Use deep contextual representations\n",
    "\n",
    "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgSYNcerMI9R"
   },
   "source": [
    "## 2.2 Error analysis\n",
    "\n",
    "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRDxKgLSL_uf"
   },
   "source": [
    "## 3.1 Predictions on unannotated text\n",
    "\n",
    "Use the three models selected in milestone 2.2 to do predictions on the sampled wikipedia text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlG6ZWkIL-HY"
   },
   "source": [
    "## 3.2 Statistically analyze the results\n",
    "\n",
    "Statistically analyze (i.e. count the number of instances) and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DL_NER_debug2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
